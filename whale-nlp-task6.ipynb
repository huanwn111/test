{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task06 bert pre-train预训练， bert funtunning使用预训练模型做分类任务\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pre-train预训练是换上自己的数据做一个预训练模型，一般不是特殊领域不需要，直接下载训练好的预训练模型使用即可。且耗资大，此处只做演示说明。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#下载bert源码\n",
    "# pre-train涉及三个模块\n",
    "# 1.tokenization.py\n",
    "# 对原始句子内容的分词、解析，分为BasicTokenizer和WordpieceTokenizer两个，不只是在预训练中，在fine-tune和推断过程同样要用到它\n",
    "# BasicTokenizer的主要是进行unicode转换、标点符号分割、小写转换、中文字符分割、去除重音符号等操作，最后返回的是关于词的数组（中文是字的数组）\n",
    "# WordpieceTokenizer的目的是将合成词分解成类似词根一样的词片\n",
    "# FullTokenizer的作用，是对一个文本段进行以上两种解析，最后返回词（字）的数组，同时还提供token到id的索引以及id到token的索引。这里的token可以理解为文本段处理过后的最小单元。\n",
    "\n",
    "# 2.create_pretraining_data.py\n",
    "# 将原始语料转换成适合模型预训练的输入数据\n",
    "# 3.run_pretraining.py\n",
    "# 是预训练的执行代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 一、tokenization.py\n",
    "#以下三个函数，无需改，知道用途，能在main入口中按需调用\n",
    "# class BasicTokenizer(object):\n",
    "# class WordpieceTokenizer(object):\n",
    "# class FullTokenizer(object):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 二、create_pretraining_data.py\n",
    "# 1、配置，【需改】\n",
    "flags.DEFINE_string(\"input_file\", None,\n",
    "                    \"输入的源语料文件\")\n",
    "flags.DEFINE_string(\n",
    "    \"output_file\", None,\n",
    "    \"处理过的语料文件地址\")\n",
    "flags.DEFINE_string(\"vocab_file\", None,\n",
    "                    \"The vocabulary file that the BERT model was trained on.\")\n",
    "flags.DEFINE_bool(\n",
    "    \"do_lower_case\", True,\n",
    "    \"是否全部转为小写字母 \"\n",
    "    \"models and False for cased models.\")\n",
    "flags.DEFINE_integer(\"max_seq_length\", 128, \"最大句长\")\n",
    "flags.DEFINE_integer(\"max_predictions_per_seq\", 20,\n",
    "                     \"Maximum number of masked LM predictions per sequence.\")\n",
    "flags.DEFINE_integer(\"random_seed\", 12345, \"Random seed for data generation.\")\n",
    "flags.DEFINE_integer(\n",
    "    \"dupe_factor\", 10,\n",
    "    \"默认重复10次，目的是可以生成不同情况的masks\")\n",
    "flags.DEFINE_float(\"masked_lm_prob\", 0.15, \"Masked LM probability.\")\n",
    "flags.DEFINE_float(\n",
    "    \"short_seq_prob\", 0.1,\n",
    "    \"P构造长度小于指定max_seq_length的样本比例\"\n",
    "    \"maximum length.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 、 main入口\n",
    "def main(_):\n",
    "    tf.logging.set_verbosity(tf.logging.INFO)\n",
    "#     1）构造tokenizer ；\n",
    "    tokenizer = tokenization.FullTokenizer(\n",
    "      vocab_file=FLAGS.vocab_file, do_lower_case=FLAGS.do_lower_case)\n",
    "\n",
    "    input_files = []\n",
    "    for input_pattern in FLAGS.input_file.split(\",\"):\n",
    "        input_files.extend(tf.gfile.Glob(input_pattern))\n",
    "\n",
    "    tf.logging.info(\"*** Reading from input files ***\")\n",
    "    for input_file in input_files:\n",
    "        tf.logging.info(\"  %s\", input_file)\n",
    "\n",
    "    rng = random.Random(FLAGS.random_seed)\n",
    "#     2）构造instances ；\n",
    "    instances = create_training_instances(\n",
    "      input_files, tokenizer, FLAGS.max_seq_length, FLAGS.dupe_factor,\n",
    "      FLAGS.short_seq_prob, FLAGS.masked_lm_prob, FLAGS.max_predictions_per_seq,\n",
    "      rng)\n",
    "\n",
    "    output_files = FLAGS.output_file.split(\",\")\n",
    "    tf.logging.info(\"*** Writing to output files ***\")\n",
    "    for output_file in output_files:\n",
    "        tf.logging.info(\"  %s\", output_file)\n",
    "# 3）保存instances\n",
    "    write_instance_to_example_files(instances, tokenizer, FLAGS.max_seq_length,\n",
    "                                  FLAGS.max_predictions_per_seq, output_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3、构造instances\n",
    "def create_training_instances(input_files, tokenizer, max_seq_length,\n",
    "                              dupe_factor, short_seq_prob, masked_lm_prob,\n",
    "                              max_predictions_per_seq, rng):\n",
    "  \"\"\"Create `TrainingInstance`s from raw text.\"\"\"\n",
    "  all_documents = [[]]\n",
    "  for input_file in input_files:\n",
    "    with tf.gfile.GFile(input_file, \"r\") as reader:\n",
    "      while True:\n",
    "        line = tokenization.convert_to_unicode(reader.readline())\n",
    "        if not line:\n",
    "          break\n",
    "        line = line.strip()\n",
    "\n",
    "        # Empty lines are used as document delimiters\n",
    "        if not line:\n",
    "          all_documents.append([])\n",
    "        tokens = tokenizer.tokenize(line)\n",
    "        if tokens:\n",
    "          all_documents[-1].append(tokens)\n",
    "\n",
    "  # Remove empty documents\n",
    "  all_documents = [x for x in all_documents if x]\n",
    "  rng.shuffle(all_documents)\n",
    "\n",
    "  vocab_words = list(tokenizer.vocab.keys())\n",
    "  instances = []\n",
    "  for _ in range(dupe_factor):\n",
    "    for document_index in range(len(all_documents)):\n",
    "      instances.extend(\n",
    "          create_instances_from_document(\n",
    "              all_documents, document_index, max_seq_length, short_seq_prob,\n",
    "              masked_lm_prob, max_predictions_per_seq, vocab_words, rng))\n",
    "\n",
    "  rng.shuffle(instances)\n",
    "  return instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_instances_from_document(\n",
    "    all_documents, document_index, max_seq_length, short_seq_prob,\n",
    "    masked_lm_prob, max_predictions_per_seq, vocab_words, rng):\n",
    "  \"\"\"Creates `TrainingInstance`s for a single document.\"\"\"\n",
    "  document = all_documents[document_index]\n",
    "\n",
    "  # Account for [CLS], [SEP], [SEP]\n",
    "  max_num_tokens = max_seq_length - 3\n",
    "  target_seq_length = max_num_tokens\n",
    "  if rng.random() < short_seq_prob:\n",
    "    target_seq_length = rng.randint(2, max_num_tokens)\n",
    "\n",
    "  instances = []\n",
    "  current_chunk = []\n",
    "  current_length = 0\n",
    "  i = 0\n",
    "  while i < len(document):\n",
    "    segment = document[i]\n",
    "    current_chunk.append(segment)\n",
    "    current_length += len(segment)\n",
    "    if i == len(document) - 1 or current_length >= target_seq_length:\n",
    "      if current_chunk:\n",
    "        # `a_end` is how many segments from `current_chunk` go into the `A`\n",
    "        # (first) sentence.\n",
    "        a_end = 1\n",
    "        if len(current_chunk) >= 2:\n",
    "          a_end = rng.randint(1, len(current_chunk) - 1)\n",
    "\n",
    "        tokens_a = []\n",
    "        for j in range(a_end):\n",
    "          tokens_a.extend(current_chunk[j])\n",
    "\n",
    "        tokens_b = []\n",
    "        # Random next\n",
    "        is_random_next = False\n",
    "        if len(current_chunk) == 1 or rng.random() < 0.5:\n",
    "          is_random_next = True\n",
    "          target_b_length = target_seq_length - len(tokens_a)\n",
    "\n",
    "          for _ in range(10):\n",
    "            random_document_index = rng.randint(0, len(all_documents) - 1)\n",
    "            if random_document_index != document_index:\n",
    "              break\n",
    "\n",
    "          random_document = all_documents[random_document_index]\n",
    "          random_start = rng.randint(0, len(random_document) - 1)\n",
    "          for j in range(random_start, len(random_document)):\n",
    "            tokens_b.extend(random_document[j])\n",
    "            if len(tokens_b) >= target_b_length:\n",
    "              break\n",
    "  \n",
    "          num_unused_segments = len(current_chunk) - a_end\n",
    "          i -= num_unused_segments\n",
    "        # Actual next\n",
    "        else:\n",
    "          is_random_next = False\n",
    "          for j in range(a_end, len(current_chunk)):\n",
    "            tokens_b.extend(current_chunk[j])\n",
    "        truncate_seq_pair(tokens_a, tokens_b, max_num_tokens, rng)\n",
    "\n",
    "        assert len(tokens_a) >= 1\n",
    "        assert len(tokens_b) >= 1\n",
    "\n",
    "        tokens = []\n",
    "        segment_ids = []\n",
    "        tokens.append(\"[CLS]\")\n",
    "        segment_ids.append(0)\n",
    "        for token in tokens_a:\n",
    "          tokens.append(token)\n",
    "          segment_ids.append(0)\n",
    "\n",
    "        tokens.append(\"[SEP]\")\n",
    "        segment_ids.append(0)\n",
    "\n",
    "        for token in tokens_b:\n",
    "          tokens.append(token)\n",
    "          segment_ids.append(1)\n",
    "        tokens.append(\"[SEP]\")\n",
    "        segment_ids.append(1)\n",
    "\n",
    "        (tokens, masked_lm_positions,\n",
    "         masked_lm_labels) = create_masked_lm_predictions(\n",
    "             tokens, masked_lm_prob, max_predictions_per_seq, vocab_words, rng)\n",
    "        instance = TrainingInstance(\n",
    "            tokens=tokens,\n",
    "            segment_ids=segment_ids,\n",
    "            is_random_next=is_random_next,\n",
    "            masked_lm_positions=masked_lm_positions,\n",
    "            masked_lm_labels=masked_lm_labels)\n",
    "        instances.append(instance)\n",
    "      current_chunk = []\n",
    "      current_length = 0\n",
    "    i += 1\n",
    "  return instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4、保存instance\n",
    "def write_instance_to_example_files(instances, tokenizer, max_seq_length,\n",
    "                                    max_predictions_per_seq, output_files):\n",
    "  \"\"\"Create TF example files from `TrainingInstance`s.\"\"\"\n",
    "  writers = []\n",
    "  for output_file in output_files:\n",
    "    writers.append(tf.python_io.TFRecordWriter(output_file))\n",
    "\n",
    "  writer_index = 0\n",
    "\n",
    "  total_written = 0\n",
    "  for (inst_index, instance) in enumerate(instances):\n",
    "    input_ids = tokenizer.convert_tokens_to_ids(instance.tokens)\n",
    "    input_mask = [1] * len(input_ids)\n",
    "    segment_ids = list(instance.segment_ids)\n",
    "    assert len(input_ids) <= max_seq_length\n",
    "\n",
    "    while len(input_ids) < max_seq_length:\n",
    "      input_ids.append(0)\n",
    "      input_mask.append(0)\n",
    "      segment_ids.append(0)\n",
    "\n",
    "    assert len(input_ids) == max_seq_length\n",
    "    assert len(input_mask) == max_seq_length\n",
    "    assert len(segment_ids) == max_seq_length\n",
    "\n",
    "    masked_lm_positions = list(instance.masked_lm_positions)\n",
    "    masked_lm_ids = tokenizer.convert_tokens_to_ids(instance.masked_lm_labels)\n",
    "    masked_lm_weights = [1.0] * len(masked_lm_ids)\n",
    "\n",
    "    while len(masked_lm_positions) < max_predictions_per_seq:\n",
    "      masked_lm_positions.append(0)\n",
    "      masked_lm_ids.append(0)\n",
    "      masked_lm_weights.append(0.0)\n",
    "\n",
    "    next_sentence_label = 1 if instance.is_random_next else 0\n",
    "\n",
    "    features = collections.OrderedDict()\n",
    "    features[\"input_ids\"] = create_int_feature(input_ids)\n",
    "    features[\"input_mask\"] = create_int_feature(input_mask)\n",
    "    features[\"segment_ids\"] = create_int_feature(segment_ids)\n",
    "    features[\"masked_lm_positions\"] = create_int_feature(masked_lm_positions)\n",
    "    features[\"masked_lm_ids\"] = create_int_feature(masked_lm_ids)\n",
    "    features[\"masked_lm_weights\"] = create_float_feature(masked_lm_weights)\n",
    "    features[\"next_sentence_labels\"] = create_int_feature([next_sentence_label])\n",
    "\n",
    "    tf_example = tf.train.Example(features=tf.train.Features(feature=features))\n",
    "\n",
    "    writers[writer_index].write(tf_example.SerializeToString())\n",
    "    writer_index = (writer_index + 1) % len(writers)\n",
    "\n",
    "    total_written += 1\n",
    "\n",
    "    if inst_index < 20:\n",
    "      tf.logging.info(\"*** Example ***\")\n",
    "      tf.logging.info(\"tokens: %s\" % \" \".join(\n",
    "          [tokenization.printable_text(x) for x in instance.tokens]))\n",
    "\n",
    "      for feature_name in features.keys():\n",
    "        feature = features[feature_name]\n",
    "        values = []\n",
    "        if feature.int64_list.value:\n",
    "          values = feature.int64_list.value\n",
    "        elif feature.float_list.value:\n",
    "          values = feature.float_list.value\n",
    "        tf.logging.info(\n",
    "            \"%s: %s\" % (feature_name, \" \".join([str(x) for x in values])))\n",
    "\n",
    "  for writer in writers:\n",
    "    writer.close()\n",
    "\n",
    "  tf.logging.info(\"Wrote %d total instances\", total_written)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 上面需注意\n",
    "# while len(input_ids) < max_seq_length:\n",
    "#       input_ids.append(0)\n",
    "#       input_mask.append(0)\n",
    "#       segment_ids.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 三、run_pretraining.py 预训练的执行模块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X和Y的确定\n",
    "\n",
    "    input_ids = features[\"input_ids\"]\n",
    "    input_mask = features[\"input_mask\"]\n",
    "    segment_ids = features[\"segment_ids\"]\n",
    "    masked_lm_positions = features[\"masked_lm_positions\"]\n",
    "    masked_lm_ids = features[\"masked_lm_ids\"]\n",
    "    masked_lm_weights = features[\"masked_lm_weights\"]\n",
    "    next_sentence_labels = features[\"next_sentence_labels\"]\n",
    "    model = modeling.BertModel(\n",
    "        config=bert_config,\n",
    "        is_training=is_training,\n",
    "        input_ids=input_ids,\n",
    "        input_mask=input_mask,\n",
    "        token_type_ids=segment_ids,\n",
    "        use_one_hot_embeddings=use_one_hot_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loss\n",
    " (masked_lm_loss,\n",
    "     masked_lm_example_loss, masked_lm_log_probs) = get_masked_lm_output(\n",
    "         bert_config, model.get_sequence_output(), model.get_embedding_table(),\n",
    "         masked_lm_positions, masked_lm_ids, masked_lm_weights)\n",
    "\n",
    "    (next_sentence_loss, next_sentence_example_loss,\n",
    "     next_sentence_log_probs) = get_next_sentence_output(\n",
    "         bert_config, model.get_pooled_output(), next_sentence_labels)\n",
    "\n",
    "    total_loss = masked_lm_loss + next_sentence_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fun-tune  下游任务"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#分类看run_classifier.py这个文件\n",
    "# 1、参数\n",
    "## Required parameters\n",
    "flags.DEFINE_string(\n",
    "    \"data_dir\", None,\n",
    "    \"输入数据路径\"\n",
    "    \"for the task.\")\n",
    "\n",
    "flags.DEFINE_string(\n",
    "    \"bert_config_file\", None,\n",
    "    \"The config json file corresponding to the pre-trained BERT model. \"\n",
    "    \"This specifies the model architecture.\")\n",
    "\n",
    "flags.DEFINE_string(\"task_name\", None, \"The name of the task to train.\")\n",
    "\n",
    "flags.DEFINE_string(\"vocab_file\", None,\n",
    "                    \"The vocabulary file that the BERT model was trained on.\")\n",
    "\n",
    "flags.DEFINE_string(\n",
    "    \"output_dir\", None,\n",
    "    \"输入数据路径\")\n",
    "\n",
    "## Other parameters\n",
    "\n",
    "flags.DEFINE_string(\n",
    "    \"init_checkpoint\", None,\n",
    "    \"Initial checkpoint (usually from a pre-trained BERT model).\")\n",
    "\n",
    "flags.DEFINE_bool(\n",
    "    \"do_lower_case\", True,\n",
    "    \"Whether to lower case the input text. Should be True for uncased \"\n",
    "    \"models and False for cased models.\")\n",
    "\n",
    "flags.DEFINE_integer(\n",
    "    \"max_seq_length\", 128,\n",
    "    \"最大句长\"\n",
    "    \"Sequences longer than this will be truncated, and sequences shorter \"\n",
    "    \"than this will be padded.\")\n",
    "\n",
    "flags.DEFINE_bool(\"do_train\", False, \"Whether to run training.\")\n",
    "\n",
    "flags.DEFINE_bool(\"do_eval\", False, \"Whether to run eval on the dev set.\")\n",
    "\n",
    "flags.DEFINE_bool(\n",
    "    \"do_predict\", False,\n",
    "    \"Whether to run the model in inference mode on the test set.\")\n",
    "\n",
    "flags.DEFINE_integer(\"train_batch_size\", 32, \"Total batch size for training.\")\n",
    "\n",
    "flags.DEFINE_integer(\"eval_batch_size\", 8, \"Total batch size for eval.\")\n",
    "\n",
    "flags.DEFINE_integer(\"predict_batch_size\", 8, \"Total batch size for predict.\")\n",
    "\n",
    "flags.DEFINE_float(\"learning_rate\", 5e-5, \"The initial learning rate for Adam.\")\n",
    "\n",
    "flags.DEFINE_float(\"num_train_epochs\", 3.0,\n",
    "                   \"Total number of training epochs to perform.\")\n",
    "\n",
    "flags.DEFINE_float(\n",
    "    \"warmup_proportion\", 0.1,\n",
    "    \"Proportion of training to perform linear learning rate warmup for. \"\n",
    "    \"E.g., 0.1 = 10% of training.\")\n",
    "\n",
    "flags.DEFINE_integer(\"save_checkpoints_steps\", 1000,\n",
    "                     \"How often to save the model checkpoint.\")\n",
    "\n",
    "flags.DEFINE_integer(\"iterations_per_loop\", 1000,\n",
    "                     \"How many steps to make in each estimator call.\")\n",
    "\n",
    "flags.DEFINE_bool(\"use_tpu\", False, \"Whether to use TPU or GPU/CPU.\")\n",
    "\n",
    "tf.flags.DEFINE_string(\n",
    "    \"tpu_name\", None,\n",
    "    \"The Cloud TPU to use for training. This should be either the name \"\n",
    "    \"used when creating the Cloud TPU, or a grpc://ip.address.of.tpu:8470 \"\n",
    "    \"url.\")\n",
    "\n",
    "tf.flags.DEFINE_string(\n",
    "    \"tpu_zone\", None,\n",
    "    \"[Optional] GCE zone where the Cloud TPU is located in. If not \"\n",
    "    \"specified, we will attempt to automatically detect the GCE project from \"\n",
    "    \"metadata.\")\n",
    "\n",
    "tf.flags.DEFINE_string(\n",
    "    \"gcp_project\", None,\n",
    "    \"[Optional] Project name for the Cloud TPU-enabled project. If not \"\n",
    "    \"specified, we will attempt to automatically detect the GCE project from \"\n",
    "    \"metadata.\")\n",
    "\n",
    "tf.flags.DEFINE_string(\"master\", None, \"[Optional] TensorFlow master URL.\")\n",
    "\n",
    "flags.DEFINE_integer(\n",
    "    \"num_tpu_cores\", 8,\n",
    "    \"Only used if `use_tpu` is True. Total number of TPU cores to use.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2、数据预处理\n",
    "class InputExample(object):\n",
    "  \"\"\"A single training/test example for simple sequence classification.\"\"\"\n",
    "  def __init__(self, guid, text_a, text_b=None, label=None):\n",
    "    self.guid = guid\n",
    "    self.text_a = text_a\n",
    "    self.text_b = text_b\n",
    "    self.label = label\n",
    "# 这是输入语料样本的数据结构。\n",
    "\n",
    "# guid是该样本的唯一ID，text_a和text_b表示句子对，lable表示句子对关系，如果是test数据集则label统一为0。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputFeatures(object):\n",
    "  \"\"\"A single set of features of data.\"\"\"\n",
    "  def __init__(self, input_ids, input_mask, segment_ids, label_id):\n",
    "    self.input_ids = input_ids\n",
    "    self.input_mask = input_mask\n",
    "    self.segment_ids = segment_ids\n",
    "    self.label_id = label_id\n",
    "# tokenization过后的样本数据结构，input_ids其实就是tokens的索引，input_mask不用解释，segment_ids对应模型的token_type_ids以上三者构成模型输入的X，label_id是标签，对应Y。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#数据读取处理\n",
    "class DataProcessor(DataProcessor):\n",
    "  \"\"\"Processor for the MRPC data set (GLUE version).\"\"\"\n",
    "\n",
    "  def get_train_examples(self, data_dir):\n",
    "    \"\"\"See base class.\"\"\"\n",
    "    return self._create_examples(\n",
    "        self._read_tsv(os.path.join(data_dir, \"train.tsv\")), \"train\")\n",
    "\n",
    "  def get_dev_examples(self, data_dir):\n",
    "    \"\"\"See base class.\"\"\"\n",
    "    return self._create_examples(\n",
    "        self._read_tsv(os.path.join(data_dir, \"dev.tsv\")), \"dev\")\n",
    "\n",
    "  def get_test_examples(self, data_dir):\n",
    "    \"\"\"See base class.\"\"\"\n",
    "    return self._create_examples(\n",
    "        self._read_tsv(os.path.join(data_dir, \"test.tsv\")), \"test\")\n",
    "\n",
    "  def get_labels(self):\n",
    "    \"\"\"See base class.\"\"\"\n",
    "    return [\"0\", \"1\"]\n",
    "\n",
    "  def _create_examples(self, lines, set_type):\n",
    "    \"\"\"Creates examples for the training and dev sets.\"\"\"\n",
    "    examples = []\n",
    "    for (i, line) in enumerate(lines):\n",
    "      if i == 0:\n",
    "        continue\n",
    "      guid = \"%s-%s\" % (set_type, i)\n",
    "      text_a = tokenization.convert_to_unicode(line[3])\n",
    "      text_b = tokenization.convert_to_unicode(line[4])\n",
    "      if set_type == \"test\":\n",
    "        label = \"0\"\n",
    "      else:\n",
    "        label = tokenization.convert_to_unicode(line[0])\n",
    "      examples.append(\n",
    "          InputExample(guid=guid, text_a=text_a, text_b=text_b, label=label))\n",
    "    return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 输出的格式为InputExample数据结构。\n",
    "# def file_based_convert_examples_to_features(\n",
    "# 将examples转换成features 把一个InputExample数据转换成InputFeatures数据结构\n",
    "# def convert_single_example(ex_index, example, label_list, max_seq_length,\n",
    "#                            tokenizer):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3、模型构建\n",
    "def create_model(bert_config, is_training, input_ids, input_mask, segment_ids,\n",
    "                 labels, num_labels, use_one_hot_embeddings):\n",
    "  \"\"\"Creates a classification model.\"\"\"\n",
    "  model = modeling.BertModel(\n",
    "      config=bert_config,\n",
    "      is_training=is_training,\n",
    "      input_ids=input_ids,\n",
    "      input_mask=input_mask,\n",
    "      token_type_ids=segment_ids,\n",
    "      use_one_hot_embeddings=use_one_hot_embeddings)\n",
    "\n",
    "  output_layer = model.get_pooled_output()\n",
    "\n",
    "  hidden_size = output_layer.shape[-1].value\n",
    "\n",
    "  output_weights = tf.get_variable(\n",
    "      \"output_weights\", [num_labels, hidden_size],\n",
    "      initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
    "\n",
    "  output_bias = tf.get_variable(\n",
    "      \"output_bias\", [num_labels], initializer=tf.zeros_initializer())\n",
    "\n",
    "  with tf.variable_scope(\"loss\"):\n",
    "    if is_training:\n",
    "      # I.e., 0.1 dropout\n",
    "      output_layer = tf.nn.dropout(output_layer, keep_prob=0.9)\n",
    "\n",
    "    logits = tf.matmul(output_layer, output_weights, transpose_b=True)\n",
    "    logits = tf.nn.bias_add(logits, output_bias)\n",
    "    probabilities = tf.nn.softmax(logits, axis=-1)\n",
    "    log_probs = tf.nn.log_softmax(logits, axis=-1)\n",
    "\n",
    "    one_hot_labels = tf.one_hot(labels, depth=num_labels, dtype=tf.float32)\n",
    "\n",
    "    per_example_loss = -tf.reduce_sum(one_hot_labels * log_probs, axis=-1)\n",
    "    loss = tf.reduce_mean(per_example_loss)\n",
    "\n",
    "    return (loss, per_example_loss, logits, probabilities)\n",
    "#X和Y都已经构造好了，将X作为模型的输入，剩下的就是将模型输出和Y进行计算得到loss。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#待调：参考：https://www.jianshu.com/p/22e462f01d8c；https://www.jianshu.com/p/116bfdb9119a"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
