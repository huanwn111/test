{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#深度模型DeepCrossing\n",
    "# DeepCrossing的结构了，比较清晰和简单，没有引入特殊的模型结构，只是常规的Embedding+多层神经网络。但这个网络模型的出现，有革命意义。DeepCrossing模型中没有任何人工特征工程的参与，只需要简单的特征处理，原始特征经Embedding Layer输入神经网络层，自主交叉和学习。 相比于FM，FFM只具备二阶特征交叉能力的模型，DeepCrossing可以通过调整神经网络的深度进行特征之间的“深度交叉”，这也是Deep Crossing名称的由来。\n",
    "\n",
    "# 如果是用于点击率预估模型的损失函数就是对数损失函数：\n",
    "#模型结构\n",
    "# 2.1 Embedding Layer\n",
    "# 将稀疏的类别型特征转成稠密的Embedding向量，Tensorflow中有实现好的层可以直接用。 和NLP里面的embedding技术异曲同工， 比如Word2Vec， 语言模型等。\n",
    "# 2.2 Stacking Layer\n",
    "# 这个层是把不同的Embedding特征和数值型特征拼接在一起，形成新的包含全部特征的特征向量，该层通常也称为连接层，最后将数值特征和Embedding特征拼接起来作为DNN的输入，这里TF是通过Concatnate层进行拼接。\n",
    "# 2.3 Multiple Residual Units Layer\n",
    "# 该层的主要结构是MLP， 但DeepCrossing采用了残差网络进行的连接。Deep Crossing模型使用稍微修改过的残差单元，它不使用卷积内核，改为了两层神经网络。我们可以看到，残差单元是通过两层ReLU变换再将原输入特征相加回来实现的。\n",
    "# 2.4 Scoring Layer\n",
    "# 这个作为输出层，为了拟合优化目标存在。 对于CTR预估二分类问题， Scoring往往采用逻辑回归，模型通过叠加多个残差块加深网络的深度，最后将结果转换成一个概率值输出。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#代码实现\n",
    "def DeepCrossing(dnn_feature_columns):\n",
    "    # 构建输入层，即所有特征对应的Input()层，这里使用字典的形式返回，方便后续构建模型\n",
    "    dense_input_dict, sparse_input_dict = build_input_layers(dnn_feature_columns)\n",
    "    # 构建模型的输入层，模型的输入层不能是字典的形式，应该将字典的形式转换成列表的形式\n",
    "    # 注意：这里实际的输入与Input()层的对应，是通过模型输入时候的字典数据的key与对应name的Input层\n",
    "    input_layers = list(dense_input_dict.values()) + list(sparse_input_dict.values())\n",
    "        \n",
    "    # 构建维度为k的embedding层，这里使用字典的形式返回，方便后面搭建模型\n",
    "    embedding_layer_dict = build_embedding_layers(dnn_feature_columns, sparse_input_dict, is_linear=False)\n",
    "\n",
    "    #将所有的dense特征拼接到一起\n",
    "    dense_dnn_list = list(dense_input_dict.values())\n",
    "    dense_dnn_inputs = Concatenate(axis=1)(dense_dnn_list) # B x n (n表示数值特征的数量)\n",
    "\n",
    "    # 因为需要将其与dense特征拼接到一起所以需要Flatten，不进行Flatten的Embedding层输出的维度为：Bx1xdim\n",
    "    sparse_dnn_list = concat_embedding_list(dnn_feature_columns, sparse_input_dict, embedding_layer_dict, flatten=True) \n",
    "\n",
    "    sparse_dnn_inputs = Concatenate(axis=1)(sparse_dnn_list) # B x m*dim (n表示类别特征的数量，dim表示embedding的维度)\n",
    "\n",
    "    # 将dense特征和Sparse特征拼接到一起\n",
    "    dnn_inputs = Concatenate(axis=1)([dense_dnn_inputs, sparse_dnn_inputs]) # B x (n + m*dim)\n",
    "\n",
    "    # 输入到dnn中，需要提前定义需要几个残差块\n",
    "    output_layer = get_dnn_logits(dnn_inputs, block_nums=3)\n",
    "\n",
    "    model = Model(input_layers, output_layer)\n",
    "    \n",
    "    return model\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
