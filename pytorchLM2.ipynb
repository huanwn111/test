{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pytorchLM 2 较成熟悉案例\n",
    "#参考一：https://blog.csdn.net/shenfuli/article/details/105053645\n",
    "#参考二：pytorch官网\n",
    "#https://pytorch.org/tutorials/advanced/dynamic_quantization_tutorial.html?highlight=lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G:\\ph\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\ipykernel\\parentpoller.py:116: UserWarning: Parent poll failed.  If the frontend dies,\n",
      "                the kernel may be left running.  Please let us know\n",
      "                about your system (bitness, Python, etc.) at\n",
      "                ipython-dev@scipy.org\n",
      "  ipython-dev@scipy.org\"\"\")\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from collections import defaultdict, Counter\n",
    "from pathlib import Path\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "def set_random_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "set_random_seed(2020)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device:  cuda\n",
      "PyTorch Version: 1.5.0\n",
      "DATA_ROOT: data/\n"
     ]
    }
   ],
   "source": [
    "# #打印看是否用了GPU (cuda)还是CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('device: ',device)\n",
    "DATA_ROOT = 'data/'\n",
    "print('PyTorch Version:', torch.__version__)\n",
    "print('DATA_ROOT:',DATA_ROOT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dictionary(object):\n",
    "    def __init__(self):\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = []\n",
    "        \n",
    "        self.word2idx['<UNK>'] = 0\n",
    "        self.word2idx['<PAD>'] = 1\n",
    "        self.idx2word.append('<UNK>')\n",
    "        self.idx2word.append('<PAD>')\n",
    "\n",
    "    def add_word(self, word):\n",
    "        if word not in self.word2idx:\n",
    "            self.idx2word.append(word)\n",
    "            self.word2idx[word] = len(self.idx2word) - 1\n",
    "        return self.word2idx[word]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.idx2word)\n",
    "\n",
    "class Corpus(object):\n",
    "    def __init__(self, path):\n",
    "        self.vocab = Dictionary()\n",
    "        self.train_data = self.tokenize(os.path.join(path, 'train.txt'))#os.path.join(,)相当于path + 'train.txt'\n",
    "        self.valid_data = self.tokenize(os.path.join(path, 'dev.txt'))\n",
    "        self.test_data = self.tokenize(os.path.join(path, 'test.txt'))\n",
    "\n",
    "    def tokenize(self, path):\n",
    "        \"\"\"Tokenizes a text file.\"\"\"\n",
    "        assert os.path.exists(path)\n",
    "        # Add words to the dictionary\n",
    "        with open(path, 'r', encoding=\"utf8\") as f:\n",
    "            for line in f:\n",
    "                words = line.split() + ['<PAD>']#结尾不用<eos>，用<PAD>，一起mask\n",
    "                for word in words:\n",
    "                    self.vocab.add_word(word)\n",
    "\n",
    "        # Tokenize file content\n",
    "        with open(path, 'r', encoding=\"utf8\") as f:\n",
    "            idss = []\n",
    "            for line in f:\n",
    "                words = line.split() + ['<PAD>']#结尾不用<eos>，用<PAD>，一起mask\n",
    "                ids = []\n",
    "                for word in words:\n",
    "                    ids.append(self.vocab.word2idx[word])\n",
    "                #idss.append(torch.tensor(ids).type(torch.int64))#拷贝数据转tensor格式\n",
    "                idss.append(ids)\n",
    "            #ids = torch.cat(idss)#拼接\n",
    "\n",
    "        return idss#ids\n",
    "    \n",
    "#     def tokenize(self, text_path):\n",
    "        \n",
    "#         with open(text_path,'r',encoding='utf-8') as f:\n",
    "#             index_data = []  # 索引数据，存储每个样本的单词索引列表\n",
    "#             for s in f.readlines():\n",
    "#                 index_data.append(\n",
    "#                     self.sentence_to_index(s)\n",
    "#                 )\n",
    "#         if self.sort_by_len:  # 为了提升训练速度，可以考虑将样本按照长度排序，这样可以减少padding\n",
    "#             index_data = sorted(index_data, key=lambda x: len(x), reverse=True)\n",
    "#         return index_data\n",
    "\n",
    "\n",
    "    def sentence_to_index(self, s):\n",
    "        return [self.vocab.word2idx[w] for w in s.split()]\n",
    "    \n",
    "    def index_to_sentence(self, x):\n",
    "        return ' '.join([self.vocab.idx2word[i] for i in x])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_data_filepath = 'data/'\n",
    "\n",
    "corpus = Corpus(model_data_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2, 3, 4, 1], [2, 3, 5, 1], [6, 7, 8, 9, 10, 8, 11, 12, 1], [13, 5, 14, 15, 16, 17, 18, 1], [19, 20, 21, 22, 23, 1], [24, 25, 26, 27, 28, 29, 30, 9, 31, 32, 1], [33, 34, 35, 36, 37, 38, 39, 40, 41, 1], [33, 42, 43, 44, 45, 34, 46, 3, 47, 48, 1], [49, 50, 51, 52, 36, 53, 54, 55, 1], [56, 51, 57, 58, 54, 59, 60, 61, 62, 51, 52, 45, 63, 64, 65, 66, 18, 1], [67, 68, 69, 70, 71, 41, 39, 1], [49, 51, 37, 38, 39, 40, 41, 1], [72, 51, 57, 58, 73, 33, 74, 75, 76, 1], [77, 9, 56, 51, 57, 58, 78, 76, 1], [33, 42, 43, 44, 79, 47, 48, 1], [33, 42, 43, 44, 80, 34, 81, 82, 83, 84, 85, 86, 87, 1], [88, 89, 90, 1], [13, 34, 91, 92, 93, 94, 61, 47, 48, 1], [95, 96, 97, 9, 98, 99, 96, 97, 100, 101, 93, 102, 1], [103, 104, 1], [105, 106, 61, 107, 108, 109, 1], [19, 20, 21, 22, 23, 110, 111, 112, 29, 30, 9, 31, 32, 1], [113, 49, 114, 115, 102, 116, 74, 40, 41, 1], [33, 5, 60, 117, 117, 118, 119, 120, 1], [121, 122, 123, 92, 124, 125, 126, 127, 128, 129, 130, 63, 64, 1], [19, 20, 21, 22, 23, 110, 111, 112, 29, 30, 9, 31, 32, 1], [131, 51, 37, 38, 39, 40, 41, 1], [121, 122, 132, 133, 134, 59, 125, 127, 128, 129, 130, 63, 64, 1], [33, 106, 61, 107, 135, 20, 44, 1], [136, 99, 117, 33, 34, 107, 20, 44, 1], [19, 20, 21, 22, 23, 110, 111, 112, 29, 30, 9, 31, 32, 1], [137, 138, 139, 140, 1], [95, 96, 97, 9, 98, 99, 96, 97, 100, 101, 93, 102, 1], [13, 106, 60, 107, 106, 141, 3, 1], [19, 20, 21, 22, 23, 110, 111, 112, 29, 30, 9, 31, 32, 1], [13, 34, 142, 131, 143, 46, 3, 108, 1], [24, 25, 26, 27, 28, 29, 30, 1], [49, 51, 37, 38, 39, 40, 41, 1], [144, 51, 57, 58, 145, 73, 74, 75, 76, 1], [131, 51, 37, 38, 39, 40, 41, 1], [146, 9, 147, 51, 57, 58, 78, 76, 1], [4, 45, 2, 148, 63, 64, 65, 66, 18, 1], [67, 68, 69, 3, 55, 119, 120, 149, 1], [6, 3, 150, 151, 74, 152, 153, 40, 41, 1], [13, 34, 154, 155, 8, 60, 156, 145, 11, 12, 1], [24, 25, 26, 27, 28, 29, 30, 1], [105, 106, 157, 141, 8, 11, 12, 21, 158, 108, 109, 1], [137, 159, 139, 149, 1], [106, 96, 97, 95, 160, 139, 161, 1], [137, 138, 47, 48, 1], [95, 96, 97, 9, 98, 99, 96, 97, 100, 101, 93, 102, 1], [19, 20, 21, 22, 23, 9, 31, 32, 1], [33, 34, 91, 107, 88, 162, 163, 1], [164, 34, 34, 88, 165, 84, 85, 166, 167, 168, 110, 13, 34, 169, 107, 9, 170, 171, 110, 170, 88, 88, 172, 102, 173, 1], [88, 45, 174, 148, 152, 104, 173, 1], [175, 176, 39, 88, 40, 41, 1], [33, 34, 149, 88, 84, 96, 97, 177, 178, 1], [13, 34, 51, 96, 97, 110, 149, 88, 74, 96, 97, 179, 180, 1], [88, 96, 97, 100, 101, 93, 102, 40, 41, 1], [67, 68, 4, 13, 107, 181, 39, 55, 118, 158, 4, 45, 2, 148, 182, 183, 1], [184, 14, 185, 97, 13, 157, 135, 186, 157, 104, 187, 188, 189, 119, 120, 1], [19, 20, 21, 190, 43, 29, 30, 191, 21, 32, 192, 1], [4, 93, 102, 1], [193, 149, 1], [4, 3, 55, 1], [33, 5, 135, 3, 55, 1], [33, 34, 81, 194, 195, 196, 137, 88, 162, 163, 1], [88, 45, 174, 148, 152, 104, 173, 197, 172, 102, 173, 1], [175, 176, 39, 88, 40, 41, 1], [88, 96, 97, 100, 101, 93, 102, 1], [33, 34, 149, 88, 196, 96, 97, 198, 92, 179, 180, 1], [49, 51, 37, 38, 39, 40, 41, 1], [77, 9, 56, 51, 57, 58, 145, 78, 76, 1], [121, 122, 132, 133, 124, 125, 126, 199, 200, 201, 202, 127, 128, 129, 130, 63, 64, 1], [24, 25, 31, 32, 1], [88, 92, 134, 59, 125, 127, 128, 129, 130, 63, 64, 1], [88, 45, 174, 148, 152, 104, 173, 1], [88, 170, 203, 57, 204, 1], [175, 176, 39, 88, 40, 41, 1], [13, 34, 91, 107, 63, 64, 65, 66, 18, 1], [67, 68, 69, 205, 76, 104, 119, 120, 1], [88, 45, 174, 148, 152, 104, 173, 1], [175, 176, 39, 88, 40, 41, 1], [88, 96, 97, 100, 101, 93, 102, 40, 41, 1], [95, 96, 97, 9, 98, 99, 96, 97, 100, 101, 93, 102, 1], [4, 2, 148, 3, 55, 119, 120, 1], [33, 5, 3, 55, 1], [19, 20, 21, 22, 23, 110, 111, 112, 29, 30, 9, 31, 32, 1], [33, 5, 3, 55, 1], [199, 200, 13, 106, 206, 87, 1], [13, 34, 123, 79, 47, 48, 1], [88, 170, 110, 135, 88, 88, 172, 102, 173, 1], [88, 45, 207, 208, 152, 104, 173, 1], [88, 45, 174, 148, 63, 64, 65, 66, 1], [67, 68, 1], [108, 39, 209, 53, 210, 211, 212, 1], [2, 148, 39, 93, 102, 119, 120, 149, 1], [24, 25, 20, 21, 22, 23, 1], [123, 131, 51, 2, 148, 51, 52, 213, 90, 39, 36, 214, 1], [111, 196, 131, 215, 9, 216, 51, 52, 207, 208, 36, 53, 54, 55, 1], [123, 131, 51, 37, 38, 39, 41, 1], [13, 106, 60, 107, 217, 102, 173, 1], [33, 106, 61, 107, 20, 44, 1], [24, 25, 218, 219, 220, 29, 1], [105, 106, 61, 107, 108, 109, 1], [95, 96, 97, 9, 98, 99, 96, 97, 100, 101, 93, 102, 1], [221, 151, 74, 40, 41, 1], [193, 138, 94, 61, 2, 148, 217, 102, 173, 1], [19, 20, 21, 22, 23, 110, 111, 112, 29, 30, 9, 31, 32, 1], [164, 34, 222, 223, 3, 39, 212, 41, 1], [67, 68, 1], [154, 194, 8, 47, 48, 119, 120, 149, 1], [224, 13, 34, 69, 225, 1], [13, 91, 107, 88, 172, 102, 173, 1], [13, 34, 226, 227, 44, 88, 172, 102, 1], [167, 168, 228, 229, 109, 1], [96, 97, 93, 102, 1], [230, 79, 9, 231, 79, 110, 164, 61, 232, 185, 97, 233, 12, 1], [164, 34, 142, 185, 97, 234, 235, 1], [19, 20, 21, 22, 23, 110, 236, 18, 29, 30, 1], [199, 125, 201, 202, 193, 159, 127, 128, 130, 200, 1], [19, 20, 21, 22, 23, 1], [13, 34, 237, 238, 36, 61, 156, 36, 214, 1], [164, 61, 232, 96, 97, 127, 128, 129, 239, 63, 64, 1], [13, 34, 135, 240, 185, 97, 104, 187, 119, 120, 1], [19, 20, 21, 22, 23, 110, 190, 43, 29, 30, 1], [241, 242, 243, 37, 244, 1], [164, 34, 49, 45, 96, 97, 100, 101, 93, 102, 1], [164, 34, 49, 45, 96, 97, 9, 13, 34, 51, 96, 97, 110, 226, 227, 96, 97, 179, 180, 1], [245, 21, 246, 157, 106, 92, 18, 247, 40, 41, 1], [164, 34, 123, 79, 47, 48, 1], [137, 159, 52, 47, 139, 149, 1], [95, 96, 97, 110, 98, 97, 8, 248, 217, 102, 1], [19, 20, 21, 22, 23, 158, 110, 175, 249, 250, 251, 1], [158, 31, 252, 1], [105, 106, 60, 107, 2, 148, 63, 64, 17, 253, 18, 1], [254, 255, 39, 20, 256, 119, 120, 1], [257, 258, 80, 111, 112, 1], [13, 34, 123, 79, 47, 48, 1], [245, 21, 246, 157, 106, 92, 18, 247, 40, 41, 1], [106, 141, 55, 1], [95, 96, 97, 110, 98, 97, 8, 248, 217, 102, 1], [19, 20, 21, 22, 23, 158, 31, 252, 1], [4, 45, 2, 148, 135, 3, 55, 119, 120, 1], [19, 20, 21, 22, 23, 110, 139, 259, 125, 126, 1], [33, 5, 260, 261, 3, 55, 119, 120, 1], [19, 20, 21, 111, 112, 190, 43, 29, 30, 1], [67, 68, 13, 60, 106, 20, 256, 1], [19, 20, 21, 22, 23, 1], [24, 25, 31, 252, 1], [105, 106, 207, 208, 108, 109, 1], [137, 138, 207, 262, 47, 48, 1], [95, 96, 97, 217, 102, 1], [24, 25, 263, 264, 265, 31, 252, 266, 139, 259, 125, 126, 1], [121, 122, 60, 230, 92, 124, 125, 126, 127, 128, 129, 130, 63, 64, 1], [19, 20, 21, 22, 23, 110, 139, 259, 125, 126, 1], [98, 97, 110, 95, 96, 97, 217, 102, 1], [24, 25, 263, 264, 265, 31, 252, 266, 139, 259, 125, 126, 1], [164, 34, 226, 227, 44, 15, 2, 148, 63, 64, 17, 253, 18, 1], [254, 255, 39, 79, 162, 119, 120, 1], [19, 20, 21, 134, 29, 30, 1], [88, 89, 90, 1], [106, 267, 268, 139, 2, 1], [106, 141, 55, 1], [95, 96, 97, 110, 98, 97, 8, 248, 217, 102, 1], [19, 20, 21, 22, 23, 158, 31, 252, 1]]\n"
     ]
    }
   ],
   "source": [
    "print(corpus.train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 3, 4, 1]\n",
      "多 囊 肝 <PAD>\n",
      "[2, 3, 5, 1]\n",
      "多 囊 肾 <PAD>\n",
      "[6, 7, 8, 9, 10, 8, 11, 12, 1]\n",
      "胆 总 管 、 胰 管 扩 张 <PAD>\n",
      "[13, 5, 14, 15, 16, 17, 18, 1]\n",
      "右 肾 门 区 致 密 影 <PAD>\n"
     ]
    }
   ],
   "source": [
    "for i in range(4):\n",
    "    print(list(corpus.train_data[i]))\n",
    "    print(corpus.index_to_sentence(list(corpus.train_data[i])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "#定义Dataset 输入输出切分\n",
    "class MyDataSet(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self, index_data):\n",
    "        self.index_data = index_data\n",
    "        \n",
    "    def __getitem__(self, i):\n",
    "        # 根据语言模型定义，这里我们要用前n-1个单词预测后n-1个单词\n",
    "        feature = self.index_data[i][:-1]\n",
    "        output = self.index_data[i][1:]\n",
    "        return  feature,output\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.index_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集大小： 166\n",
      "训练集样本：\n",
      "\t输入： [2, 3, 5]\n",
      "\t      多 囊 肾\n",
      "\\结果： [3, 5, 1]\n",
      "\t      囊 肾 <PAD>\n"
     ]
    }
   ],
   "source": [
    "#测试上面dataset\n",
    "train_set = MyDataSet(corpus.train_data)\n",
    "print('训练集大小：', len(train_set))\n",
    "print('训练集样本：')\n",
    "print('\\t输入：', train_set[1][0])\n",
    "print('\\t     ', corpus.index_to_sentence(train_set[1][0]))\n",
    "print('\\结果：', train_set[1][1])\n",
    "print('\\t     ', corpus.index_to_sentence(train_set[1][1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "vocab = Dictionary()\n",
    "print(vocab.word2idx['<PAD>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "#定义DataLoader，对每个batch预处理，加mask等\n",
    "\n",
    "PAD_IDX = vocab.word2idx['<PAD>'] #\n",
    "def lm_collate_fn(batch):\n",
    "    \"\"\"\n",
    "        DataLoader 中对每个batch 进行预处理的函数\n",
    "        \n",
    "        输入batch\n",
    "    \"\"\"\n",
    "    # 这里输入的batch格式为[(input_1, target_1), ... ,(input_n, target_n)]\n",
    "    # 我们要将其格式转换为[(input_1, ... , input_n), (target_1, ... , target_n)]\n",
    "    batch = list(zip(*batch))\n",
    "    # 生成长度列表\n",
    "    lengths = torch.LongTensor( [ len(x) for x in batch[0] ]).to(device)\n",
    "    # 对输入和目标进行padding\n",
    "    inputs = [torch.LongTensor(x).to(device) for x in batch[0]]\n",
    "    inputs = nn.utils.rnn.pad_sequence(inputs, batch_first=True,padding_value=PAD_IDX) # pad_sequence 可以按照最大长度句子进行填充统一长度标准（0填充）\n",
    "    targets = [torch.LongTensor(x).to(device) for x in batch[1]]\n",
    "    targets = nn.utils.rnn.pad_sequence(targets, batch_first=True,padding_value=PAD_IDX)\n",
    "    \n",
    "    # 因为输入数据没有 “1” 的索引，存在“1” 表示是padding_index 的结构 ，由此生成mask矩阵\n",
    "    mask = (inputs != 1).float().to(device) # 1 表示该位置存储／ 0 表示该位置不存在 是填充的数据padding（注意：计算loss 的时候需要去掉）\n",
    "    #print(\"==MASK==\",mask)\n",
    "    # 在之后的训练中因为还要进行pack_padded_sequence操作，所以在这里按照长度降序排列\n",
    "    lengths, sorted_index = lengths.sort(descending=True)\n",
    "    # 根据排序后的perm_index ，进行重新获取数据列表\n",
    "    inputs = inputs[sorted_index]\n",
    "    targets = targets[sorted_index]\n",
    "    mask = mask[sorted_index]\n",
    "    \n",
    "    return inputs, targets, lengths, mask\n",
    "    #返回输入、输出、长度、mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "输入：\n",
      "tensor([[164,  34,  34,  ..., 172, 102, 173],\n",
      "        [ 56,  51,  57,  ...,   1,   1,   1],\n",
      "        [ 67,  68,   4,  ...,   1,   1,   1],\n",
      "        ...,\n",
      "        [  4,  93, 102,  ...,   1,   1,   1],\n",
      "        [193, 149,   1,  ...,   1,   1,   1],\n",
      "        [103, 104,   1,  ...,   1,   1,   1]], device='cuda:0')\n",
      "------------------------------------------------------------\n",
      "目标：\n",
      "tensor([[ 34,  34,  88,  ..., 102, 173,   1],\n",
      "        [ 51,  57,  58,  ...,   1,   1,   1],\n",
      "        [ 68,   4,  13,  ...,   1,   1,   1],\n",
      "        ...,\n",
      "        [ 93, 102,   1,  ...,   1,   1,   1],\n",
      "        [149,   1,   1,  ...,   1,   1,   1],\n",
      "        [104,   1,   1,  ...,   1,   1,   1]], device='cuda:0')\n",
      "------------------------------------------------------------\n",
      "Mask：\n",
      "tensor([[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
      "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
      "        [1., 1., 0.,  ..., 0., 0., 0.],\n",
      "        [1., 1., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "------------------------------------------------------------\n",
      "每个样本的实际长度：\n",
      "tensor([25, 17, 16, 15, 13, 13, 13, 13, 13, 13, 13, 13, 12, 12, 12, 11, 11, 10,\n",
      "        10, 10,  9,  9,  9,  9,  9,  9,  9,  9,  9,  8,  8,  8,  8,  8,  8,  8,\n",
      "         8,  8,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  6,  6,\n",
      "         5,  4,  4,  4,  3,  3,  3,  3,  2,  2], device='cuda:0')\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#测试上面\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    dataset=train_set,\n",
    "    batch_size=64,\n",
    "    shuffle=False,\n",
    "    collate_fn=lm_collate_fn\n",
    ")\n",
    "inputs, targets, lengths, mask = next(iter(test_loader))\n",
    "print('输入：')\n",
    "print(inputs)\n",
    "print('-' * 60)\n",
    "print('目标：')\n",
    "print(targets)\n",
    "print('-' * 60)\n",
    "print('Mask：')\n",
    "print(mask)\n",
    "print('-' * 60)\n",
    "print('每个样本的实际长度：')\n",
    "print(lengths)\n",
    "print('-' * 60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BiLSTM(\n",
       "  (drop): Dropout(p=0.5, inplace=False)\n",
       "  (embed): Embedding(269, 200)\n",
       "  (encoder): LSTM(200, 200, batch_first=True, bidirectional=True)\n",
       "  (decoder): Linear(in_features=400, out_features=269, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#定义网络 BiLSTM\n",
    "\n",
    "class BiLSTM(nn.Module):\n",
    "    \"\"\"语言模型网络架构\n",
    "    \n",
    "    Args:\n",
    "        vocab_size: 词表中的单词数目\n",
    "        embedding_size: 词向量维度\n",
    "        hidden_size: LSTM隐含状态的维度\n",
    "        dropout: Dropout概率\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, embedding_size=200, hidden_size=200, dropout=0.5,num_layers=1):\n",
    "        \"\"\"\n",
    "            input_size(embedding_size) – The number of expected features in the input x\n",
    "            hidden_size – The number of features in the hidden state h\n",
    "            num_layers – Number of recurrent layers.\n",
    "            batch_first – If True, then the input and output tensors are provided as (batch, seq, feature). Default: False\n",
    "        \"\"\"\n",
    "        super(BiLSTM, self).__init__()\n",
    "        self.num_layers=num_layers\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.embed = nn.Embedding(vocab_size, embedding_size)\n",
    "        self.encoder = nn.LSTM(\n",
    "                            input_size = embedding_size, \n",
    "                            hidden_size = hidden_size, \n",
    "                            num_layers = 1,\n",
    "                            batch_first=True,bidirectional=True) \n",
    "        #  bidirectional ＝ True 设置False 可以进行对比，效果非常的明显\n",
    "            \n",
    "        self.decoder = nn.Linear(2 * hidden_size, vocab_size)\n",
    "        \n",
    " \n",
    "    \n",
    "    def forward(self, inputs, lengths):\n",
    "        # inputs shape: (batch_size, max_length)\n",
    "        # x_emb shape: (batch_size, max_length, embed_size)\n",
    "        x_emb = self.drop(self.embed(inputs))\n",
    "        \n",
    "        packed_emb = nn.utils.rnn.pack_padded_sequence(\n",
    "            x_emb, \n",
    "            lengths, \n",
    "            batch_first=True\n",
    "        )\n",
    "        # 这里LSTM的h_0,c_0使用全0的默认初始化，LSTM层经过后丢弃\n",
    "        packed_out, _ = self.encoder(packed_emb)\n",
    "        # x_out shape: (batch_size, max_length, hidden_size)\n",
    "        x_out, _ = nn.utils.rnn.pad_packed_sequence(\n",
    "            packed_out, batch_first=True\n",
    "        )\n",
    "        \n",
    "        # outputs shape: (batch, max_length, vocab_size)\n",
    "        return self.decoder(self.drop(x_out))\n",
    "        \n",
    "model = BiLSTM(len(corpus.vocab), 200, 200)\n",
    "model.to(device)#将模型加载到指定的设备，device上面有定义 device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')，就是cuda\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型输入Shape： torch.Size([64, 25])\n",
      "模型输出Shape： torch.Size([64, 25, 269])\n"
     ]
    }
   ],
   "source": [
    "#打印测试上面\n",
    "inputs, targets, lengths, mask = next(iter(test_loader))\n",
    "outputs = model(inputs, lengths)\n",
    "print('模型输入Shape：', inputs.shape)\n",
    "print('模型输出Shape：', outputs.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "#定义损失函数\n",
    "class MaskCrossEntropyLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MaskCrossEntropyLoss, self).__init__()\n",
    "        self.celoss = nn.CrossEntropyLoss(reduction='none')\n",
    "    \n",
    "    def forward(self, outputs, targets, mask):\n",
    "        # outputs shape: (batch_size * max_len, vocab_size)\n",
    "        \n",
    "        outputs = outputs.view(-1, outputs.size(2)) # outputs.size(2) 获取第2 个维度的大小\n",
    "        #print('outputs: ',outputs.shape)\n",
    "        # targets shape: (batch_size * max_len)\n",
    "        targets = targets.view(-1)\n",
    "        #print('targets: ',targets.shape)\n",
    "        # mask shape: (batch_size * max_len)\n",
    "        mask = mask.view(-1)\n",
    "        #print('mask: ',mask.shape)\n",
    "        #print('loss: ',self.celoss(outputs, targets).shape)\n",
    "        \n",
    "        #print(mask)\n",
    "        #print(self.celoss(outputs, targets))\n",
    "        loss = self.celoss(outputs, targets) * mask # 把pading的loss设置为0\n",
    "        return torch.sum(loss) / torch.sum(mask) # 非0的loss之和 ➗ 所有非0的个数  平均loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "损失值： tensor(5.6059, device='cuda:0', grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#测试上面\n",
    "inputs, targets, lengths, mask = next(iter(test_loader))\n",
    "outputs = model(inputs, lengths)\n",
    "criterion = MaskCrossEntropyLoss().to(device)\n",
    "loss = criterion(outputs, targets, mask)\n",
    "print('损失值：', loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "#模型训练预测\n",
    "#定义学习器\n",
    "class LanguageModelLearner:\n",
    "    def __init__(self, corpus, embedding_size=200, hidden_size=200, dropout=0.5, \n",
    "                 batch_size=128, early_stopping_round=5):\n",
    "        self.corpus = corpus\n",
    "        self.batch_size = batch_size # 每次加载记录数\n",
    "        self.early_stopping_round = early_stopping_round\n",
    "        self.model = BiLSTM(len(corpus.vocab), embedding_size, hidden_size, dropout).to(device) # 初始化model\n",
    "        self.criterion = MaskCrossEntropyLoss().to(device) # 自定义CrossEntroyLoss(删除了padding 的数据－效果会更好)\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters()) # 采用Adam 剃度下降算法 更新权重 \n",
    "        self.history = defaultdict(list) # list ，存储训练结果\n",
    "        \n",
    "    def fit(self, num_epochs):\n",
    "        # 定义训练集dataloader\n",
    "        train_set = MyDataSet(self.corpus.train_data)\n",
    "        train_loader = torch.utils.data.DataLoader(\n",
    "            dataset=train_set,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "            collate_fn=lm_collate_fn # 使用自定义 lm_collate_fn 对训练集长度统一化操作\n",
    "        )\n",
    "        \n",
    "        # 定义验证集dataloader\n",
    "        valid_set = MyDataSet(self.corpus.valid_data)\n",
    "        valid_loader = torch.utils.data.DataLoader(\n",
    "            dataset=valid_set,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "            collate_fn=lm_collate_fn\n",
    "        )\n",
    "        \n",
    "        # 记录验证集没有提高的轮数，用于EarlyStopping\n",
    "        no_improve_round = 0\n",
    "        \n",
    "        for epoch in range(num_epochs):            \n",
    "            train_loss, train_acc, train_words = self._make_train_step(train_loader)\n",
    "            #if (epoch + 1) % 10 == 0:\n",
    "            print(f'Epoch {epoch+1}:')\n",
    "            print('Train Step --> Loss: {:.3f}, Acc: {:.3f}, Words: {}'.format(\n",
    "                train_loss, train_acc, train_words))\n",
    "            # 记录训练信息\n",
    "            self.history['train_loss'].append(train_loss)\n",
    "            self.history['train_acc'].append(train_acc)\n",
    "        \n",
    "            valid_loss, valid_acc, valid_words = self._make_valid_step(valid_loader)\n",
    "            #if (epoch + 1) % 10 == 0:\n",
    "            print('Valid Step --> Loss: {:.3f}, Acc: {:.3f}, Words: {}'.format(\n",
    "                valid_loss, valid_acc, valid_words))\n",
    "            self.history['valid_loss'].append(valid_loss)\n",
    "            self.history['valid_acc'].append(valid_acc)\n",
    "            \n",
    "            # 根据验证集的准确率进行EarlyStopping\n",
    "            if self.history['valid_acc'][-1] < max(self.history['valid_acc']):\n",
    "                no_improve_round += 1\n",
    "            else:\n",
    "                no_improve_round = 0\n",
    "            if no_improve_round == self.early_stopping_round:\n",
    "                print(f'Early Stopping at Epoch {epoch+1}')\n",
    "                break\n",
    "            \n",
    "        \n",
    "    def predict(self):\n",
    "        test_set = MyDataSet(self.corpus.test_data)\n",
    "        # 这里注意，为了方便之后分析不要shuffle，batch_size设置为1\n",
    "        test_loader = torch.utils.data.DataLoader(\n",
    "            dataset=test_set,\n",
    "            batch_size=1,\n",
    "            shuffle=False,\n",
    "            collate_fn=lm_collate_fn\n",
    "        )\n",
    "        \n",
    "        # 验证模式\n",
    "        self.model.eval()\n",
    "        \n",
    "        # 总损失\n",
    "        total_loss = 0.0\n",
    "        # 正确预测的数目，单词总数\n",
    "        total_correct, total_words = 0, 0\n",
    "        # 预测结果字典，包含preds和targets\n",
    "        test_result = defaultdict(list) \n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs, targets, lengths, mask in test_loader:\n",
    "                # 计算模型输出\n",
    "                outputs = self.model(inputs, lengths)\n",
    "                \n",
    "                # 统计当前预测正确的数目\n",
    "                total_correct += (outputs.argmax(-1) == targets).sum().item()\n",
    "                # 统计当前总预测单词数\n",
    "                total_words += torch.sum(lengths).item()\n",
    "                \n",
    "                # 记录结果\n",
    "                test_result['preds'].append(outputs.argmax(-1).data.cpu().numpy()[0])\n",
    "                test_result['targets'].append(targets.data.cpu().numpy()[0])\n",
    "                \n",
    "                # 计算模型Mask交叉熵损失\n",
    "                loss = self.criterion(outputs, targets, mask)\n",
    "                # 统计总损失\n",
    "                total_loss += loss.item() * torch.sum(mask).item()\n",
    "        return total_loss / total_words, total_correct / total_words, total_words, test_result\n",
    "        \n",
    "    def _make_train_step(self, train_loader):\n",
    "        # 训练模式\n",
    "        self.model.train()\n",
    "        \n",
    "        # 总损失\n",
    "        total_loss = 0.0\n",
    "        # 正确预测的数目，单词总数\n",
    "        total_correct, total_words = 0, 0\n",
    "        \n",
    "        for inputs, targets, lengths, mask in train_loader:\n",
    "            # 计算模型输出\n",
    "            outputs = self.model(inputs, lengths)\n",
    "            \n",
    "            # 统计当前预测正确的数目\n",
    "            total_correct += (outputs.argmax(-1) == targets).sum().item()\n",
    "            # 统计当前总预测单词数\n",
    "            total_words += torch.sum(lengths).item()\n",
    "            \n",
    "            # 计算模型Mask交叉熵损失\n",
    "            loss = self.criterion(outputs, targets, mask)\n",
    "            # 统计总损失\n",
    "            total_loss += loss.item() * torch.sum(mask).item() # torch.sum(mask) 表示真正有效的单词\n",
    "                        \n",
    "            # 反向传播\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward() # w = d_loss/dw \n",
    "            self.optimizer.step() #w -= w-learning_rate*w\n",
    "        return total_loss / total_words, total_correct / total_words, total_words\n",
    "    \n",
    "    def _make_valid_step(self, valid_loader):\n",
    "        # 验证模式\n",
    "        self.model.eval()\n",
    "        \n",
    "        # 总损失\n",
    "        total_loss = 0.0\n",
    "        # 正确预测的数目，单词总数\n",
    "        total_correct, total_words = 0, 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs, targets, lengths, mask in valid_loader:\n",
    "                # 计算模型输出\n",
    "                outputs = self.model(inputs, lengths)\n",
    "                \n",
    "                # 统计当前预测正确的数目\n",
    "                total_correct += (outputs.argmax(-1) == targets).sum().item()\n",
    "                # 统计当前总预测单词数\n",
    "                total_words += torch.sum(lengths).item()\n",
    "                \n",
    "                # 计算模型Mask交叉熵损失\n",
    "                loss = self.criterion(outputs, targets, mask)\n",
    "                # 统计总损失\n",
    "                total_loss += loss.item() * torch.sum(mask).item()\n",
    "        return total_loss / total_words, total_correct / total_words, total_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:\n",
      "Train Step --> Loss: 5.594, Acc: 0.008, Words: 1367\n",
      "Valid Step --> Loss: 5.489, Acc: 0.153, Words: 1367\n",
      "Epoch 2:\n",
      "Train Step --> Loss: 5.506, Acc: 0.038, Words: 1367\n",
      "Valid Step --> Loss: 5.388, Acc: 0.351, Words: 1367\n",
      "Epoch 3:\n",
      "Train Step --> Loss: 5.421, Acc: 0.138, Words: 1367\n",
      "Valid Step --> Loss: 5.286, Acc: 0.454, Words: 1367\n",
      "Epoch 4:\n",
      "Train Step --> Loss: 5.338, Acc: 0.231, Words: 1367\n",
      "Valid Step --> Loss: 5.181, Acc: 0.509, Words: 1367\n",
      "Epoch 5:\n",
      "Train Step --> Loss: 5.258, Acc: 0.639, Words: 1367\n",
      "Valid Step --> Loss: 5.070, Acc: 2.357, Words: 1367\n",
      "Epoch 6:\n",
      "Train Step --> Loss: 5.161, Acc: 2.182, Words: 1367\n",
      "Valid Step --> Loss: 4.950, Acc: 1.861, Words: 1367\n",
      "Epoch 7:\n",
      "Train Step --> Loss: 5.062, Acc: 1.890, Words: 1367\n",
      "Valid Step --> Loss: 4.818, Acc: 2.364, Words: 1367\n",
      "Epoch 8:\n",
      "Train Step --> Loss: 4.947, Acc: 1.735, Words: 1367\n",
      "Valid Step --> Loss: 4.670, Acc: 2.355, Words: 1367\n",
      "Epoch 9:\n",
      "Train Step --> Loss: 4.825, Acc: 1.750, Words: 1367\n",
      "Valid Step --> Loss: 4.499, Acc: 2.031, Words: 1367\n",
      "Epoch 10:\n",
      "Train Step --> Loss: 4.692, Acc: 1.953, Words: 1367\n",
      "Valid Step --> Loss: 4.301, Acc: 1.817, Words: 1367\n"
     ]
    }
   ],
   "source": [
    "#设定参数，开始训练\n",
    "torch.cuda.empty_cache()\n",
    "learner = LanguageModelLearner(corpus, embedding_size=200, hidden_size=200, dropout=0.5, batch_size=128)\n",
    "learner.fit(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "测试集上的结果 --> Loss: 4.301, Acc: 0.530, Words: 1367\n"
     ]
    }
   ],
   "source": [
    "#模型预测\n",
    "test_loss, test_acc, test_words, test_result = learner.predict()\n",
    "print('测试集上的结果 --> Loss: {:.3f}, Acc: {:.3f}, Words: {}'.format(\n",
    "    test_loss, test_acc, test_words))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "预测句子数量： 166\n",
      "------------------------------------------------------------\n",
      "结果样例：\n",
      "预测值\t [51 51 37 38 39 40  1]\n",
      "实际值\t [131  51  37  38  39  41   1]\n",
      "预测句子\t 椎 椎 退 行 性 改 <PAD>\n",
      "实际句子\t 腰 椎 退 行 性 变 <PAD>\n"
     ]
    }
   ],
   "source": [
    "print('预测句子数量：', len(test_result['preds']))\n",
    "print('-' * 60)\n",
    "\n",
    "\n",
    "sample_index = 100#好好处理下数据。\n",
    "print('结果样例：')\n",
    "print('预测值\\t', test_result['preds'][sample_index])\n",
    "print('实际值\\t', test_result['targets'][sample_index])\n",
    "print('预测句子\\t', corpus.index_to_sentence(test_result['preds'][sample_index]))\n",
    "print('实际句子\\t', corpus.index_to_sentence(test_result['targets'][sample_index]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch]",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
