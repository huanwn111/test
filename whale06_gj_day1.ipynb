{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "【Task1】随机森林算法梳理\n",
    "【参考框架】欢迎有自己的框架\n",
    "1. 集成学习概念\n",
    "2. 个体学习器概念\n",
    "3. boosting  bagging\n",
    "4. 结合策略(平均法，投票法，学习法)\n",
    "5. 随机森林思想\n",
    "6. 随机森林的推广\n",
    "7. 优缺点\n",
    "8. sklearn参数\n",
    "9.应用场景"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练数据 {'data': array([[5.1, 3.5, 1.4, 0.2],\n",
      "       [4.9, 3. , 1.4, 0.2],\n",
      "       [4.7, 3.2, 1.3, 0.2],\n",
      "       [4.6, 3.1, 1.5, 0.2],\n",
      "       [5. , 3.6, 1.4, 0.2],\n",
      "       [5.4, 3.9, 1.7, 0.4],\n",
      "       [4.6, 3.4, 1.4, 0.3],\n",
      "       [5. , 3.4, 1.5, 0.2],\n",
      "       [4.4, 2.9, 1.4, 0.2],\n",
      "       [4.9, 3.1, 1.5, 0.1],\n",
      "       [5.4, 3.7, 1.5, 0.2],\n",
      "       [4.8, 3.4, 1.6, 0.2],\n",
      "       [4.8, 3. , 1.4, 0.1],\n",
      "       [4.3, 3. , 1.1, 0.1],\n",
      "       [5.8, 4. , 1.2, 0.2],\n",
      "       [5.7, 4.4, 1.5, 0.4],\n",
      "       [5.4, 3.9, 1.3, 0.4],\n",
      "       [5.1, 3.5, 1.4, 0.3],\n",
      "       [5.7, 3.8, 1.7, 0.3],\n",
      "       [5.1, 3.8, 1.5, 0.3],\n",
      "       [5.4, 3.4, 1.7, 0.2],\n",
      "       [5.1, 3.7, 1.5, 0.4],\n",
      "       [4.6, 3.6, 1. , 0.2],\n",
      "       [5.1, 3.3, 1.7, 0.5],\n",
      "       [4.8, 3.4, 1.9, 0.2],\n",
      "       [5. , 3. , 1.6, 0.2],\n",
      "       [5. , 3.4, 1.6, 0.4],\n",
      "       [5.2, 3.5, 1.5, 0.2],\n",
      "       [5.2, 3.4, 1.4, 0.2],\n",
      "       [4.7, 3.2, 1.6, 0.2],\n",
      "       [4.8, 3.1, 1.6, 0.2],\n",
      "       [5.4, 3.4, 1.5, 0.4],\n",
      "       [5.2, 4.1, 1.5, 0.1],\n",
      "       [5.5, 4.2, 1.4, 0.2],\n",
      "       [4.9, 3.1, 1.5, 0.1],\n",
      "       [5. , 3.2, 1.2, 0.2],\n",
      "       [5.5, 3.5, 1.3, 0.2],\n",
      "       [4.9, 3.1, 1.5, 0.1],\n",
      "       [4.4, 3. , 1.3, 0.2],\n",
      "       [5.1, 3.4, 1.5, 0.2],\n",
      "       [5. , 3.5, 1.3, 0.3],\n",
      "       [4.5, 2.3, 1.3, 0.3],\n",
      "       [4.4, 3.2, 1.3, 0.2],\n",
      "       [5. , 3.5, 1.6, 0.6],\n",
      "       [5.1, 3.8, 1.9, 0.4],\n",
      "       [4.8, 3. , 1.4, 0.3],\n",
      "       [5.1, 3.8, 1.6, 0.2],\n",
      "       [4.6, 3.2, 1.4, 0.2],\n",
      "       [5.3, 3.7, 1.5, 0.2],\n",
      "       [5. , 3.3, 1.4, 0.2],\n",
      "       [7. , 3.2, 4.7, 1.4],\n",
      "       [6.4, 3.2, 4.5, 1.5],\n",
      "       [6.9, 3.1, 4.9, 1.5],\n",
      "       [5.5, 2.3, 4. , 1.3],\n",
      "       [6.5, 2.8, 4.6, 1.5],\n",
      "       [5.7, 2.8, 4.5, 1.3],\n",
      "       [6.3, 3.3, 4.7, 1.6],\n",
      "       [4.9, 2.4, 3.3, 1. ],\n",
      "       [6.6, 2.9, 4.6, 1.3],\n",
      "       [5.2, 2.7, 3.9, 1.4],\n",
      "       [5. , 2. , 3.5, 1. ],\n",
      "       [5.9, 3. , 4.2, 1.5],\n",
      "       [6. , 2.2, 4. , 1. ],\n",
      "       [6.1, 2.9, 4.7, 1.4],\n",
      "       [5.6, 2.9, 3.6, 1.3],\n",
      "       [6.7, 3.1, 4.4, 1.4],\n",
      "       [5.6, 3. , 4.5, 1.5],\n",
      "       [5.8, 2.7, 4.1, 1. ],\n",
      "       [6.2, 2.2, 4.5, 1.5],\n",
      "       [5.6, 2.5, 3.9, 1.1],\n",
      "       [5.9, 3.2, 4.8, 1.8],\n",
      "       [6.1, 2.8, 4. , 1.3],\n",
      "       [6.3, 2.5, 4.9, 1.5],\n",
      "       [6.1, 2.8, 4.7, 1.2],\n",
      "       [6.4, 2.9, 4.3, 1.3],\n",
      "       [6.6, 3. , 4.4, 1.4],\n",
      "       [6.8, 2.8, 4.8, 1.4],\n",
      "       [6.7, 3. , 5. , 1.7],\n",
      "       [6. , 2.9, 4.5, 1.5],\n",
      "       [5.7, 2.6, 3.5, 1. ],\n",
      "       [5.5, 2.4, 3.8, 1.1],\n",
      "       [5.5, 2.4, 3.7, 1. ],\n",
      "       [5.8, 2.7, 3.9, 1.2],\n",
      "       [6. , 2.7, 5.1, 1.6],\n",
      "       [5.4, 3. , 4.5, 1.5],\n",
      "       [6. , 3.4, 4.5, 1.6],\n",
      "       [6.7, 3.1, 4.7, 1.5],\n",
      "       [6.3, 2.3, 4.4, 1.3],\n",
      "       [5.6, 3. , 4.1, 1.3],\n",
      "       [5.5, 2.5, 4. , 1.3],\n",
      "       [5.5, 2.6, 4.4, 1.2],\n",
      "       [6.1, 3. , 4.6, 1.4],\n",
      "       [5.8, 2.6, 4. , 1.2],\n",
      "       [5. , 2.3, 3.3, 1. ],\n",
      "       [5.6, 2.7, 4.2, 1.3],\n",
      "       [5.7, 3. , 4.2, 1.2],\n",
      "       [5.7, 2.9, 4.2, 1.3],\n",
      "       [6.2, 2.9, 4.3, 1.3],\n",
      "       [5.1, 2.5, 3. , 1.1],\n",
      "       [5.7, 2.8, 4.1, 1.3],\n",
      "       [6.3, 3.3, 6. , 2.5],\n",
      "       [5.8, 2.7, 5.1, 1.9],\n",
      "       [7.1, 3. , 5.9, 2.1],\n",
      "       [6.3, 2.9, 5.6, 1.8],\n",
      "       [6.5, 3. , 5.8, 2.2],\n",
      "       [7.6, 3. , 6.6, 2.1],\n",
      "       [4.9, 2.5, 4.5, 1.7],\n",
      "       [7.3, 2.9, 6.3, 1.8],\n",
      "       [6.7, 2.5, 5.8, 1.8],\n",
      "       [7.2, 3.6, 6.1, 2.5],\n",
      "       [6.5, 3.2, 5.1, 2. ],\n",
      "       [6.4, 2.7, 5.3, 1.9],\n",
      "       [6.8, 3. , 5.5, 2.1],\n",
      "       [5.7, 2.5, 5. , 2. ],\n",
      "       [5.8, 2.8, 5.1, 2.4],\n",
      "       [6.4, 3.2, 5.3, 2.3],\n",
      "       [6.5, 3. , 5.5, 1.8],\n",
      "       [7.7, 3.8, 6.7, 2.2],\n",
      "       [7.7, 2.6, 6.9, 2.3],\n",
      "       [6. , 2.2, 5. , 1.5],\n",
      "       [6.9, 3.2, 5.7, 2.3],\n",
      "       [5.6, 2.8, 4.9, 2. ],\n",
      "       [7.7, 2.8, 6.7, 2. ],\n",
      "       [6.3, 2.7, 4.9, 1.8],\n",
      "       [6.7, 3.3, 5.7, 2.1],\n",
      "       [7.2, 3.2, 6. , 1.8],\n",
      "       [6.2, 2.8, 4.8, 1.8],\n",
      "       [6.1, 3. , 4.9, 1.8],\n",
      "       [6.4, 2.8, 5.6, 2.1],\n",
      "       [7.2, 3. , 5.8, 1.6],\n",
      "       [7.4, 2.8, 6.1, 1.9],\n",
      "       [7.9, 3.8, 6.4, 2. ],\n",
      "       [6.4, 2.8, 5.6, 2.2],\n",
      "       [6.3, 2.8, 5.1, 1.5],\n",
      "       [6.1, 2.6, 5.6, 1.4],\n",
      "       [7.7, 3. , 6.1, 2.3],\n",
      "       [6.3, 3.4, 5.6, 2.4],\n",
      "       [6.4, 3.1, 5.5, 1.8],\n",
      "       [6. , 3. , 4.8, 1.8],\n",
      "       [6.9, 3.1, 5.4, 2.1],\n",
      "       [6.7, 3.1, 5.6, 2.4],\n",
      "       [6.9, 3.1, 5.1, 2.3],\n",
      "       [5.8, 2.7, 5.1, 1.9],\n",
      "       [6.8, 3.2, 5.9, 2.3],\n",
      "       [6.7, 3.3, 5.7, 2.5],\n",
      "       [6.7, 3. , 5.2, 2.3],\n",
      "       [6.3, 2.5, 5. , 1.9],\n",
      "       [6.5, 3. , 5.2, 2. ],\n",
      "       [6.2, 3.4, 5.4, 2.3],\n",
      "       [5.9, 3. , 5.1, 1.8]]), 'target': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]), 'target_names': array(['setosa', 'versicolor', 'virginica'], dtype='<U10'), 'DESCR': 'Iris Plants Database\\n====================\\n\\nNotes\\n-----\\nData Set Characteristics:\\n    :Number of Instances: 150 (50 in each of three classes)\\n    :Number of Attributes: 4 numeric, predictive attributes and the class\\n    :Attribute Information:\\n        - sepal length in cm\\n        - sepal width in cm\\n        - petal length in cm\\n        - petal width in cm\\n        - class:\\n                - Iris-Setosa\\n                - Iris-Versicolour\\n                - Iris-Virginica\\n    :Summary Statistics:\\n\\n    ============== ==== ==== ======= ===== ====================\\n                    Min  Max   Mean    SD   Class Correlation\\n    ============== ==== ==== ======= ===== ====================\\n    sepal length:   4.3  7.9   5.84   0.83    0.7826\\n    sepal width:    2.0  4.4   3.05   0.43   -0.4194\\n    petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\\n    petal width:    0.1  2.5   1.20  0.76     0.9565  (high!)\\n    ============== ==== ==== ======= ===== ====================\\n\\n    :Missing Attribute Values: None\\n    :Class Distribution: 33.3% for each of 3 classes.\\n    :Creator: R.A. Fisher\\n    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\\n    :Date: July, 1988\\n\\nThis is a copy of UCI ML iris datasets.\\nhttp://archive.ics.uci.edu/ml/datasets/Iris\\n\\nThe famous Iris database, first used by Sir R.A Fisher\\n\\nThis is perhaps the best known database to be found in the\\npattern recognition literature.  Fisher\\'s paper is a classic in the field and\\nis referenced frequently to this day.  (See Duda & Hart, for example.)  The\\ndata set contains 3 classes of 50 instances each, where each class refers to a\\ntype of iris plant.  One class is linearly separable from the other 2; the\\nlatter are NOT linearly separable from each other.\\n\\nReferences\\n----------\\n   - Fisher,R.A. \"The use of multiple measurements in taxonomic problems\"\\n     Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to\\n     Mathematical Statistics\" (John Wiley, NY, 1950).\\n   - Duda,R.O., & Hart,P.E. (1973) Pattern Classification and Scene Analysis.\\n     (Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page 218.\\n   - Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System\\n     Structure and Classification Rule for Recognition in Partially Exposed\\n     Environments\".  IEEE Transactions on Pattern Analysis and Machine\\n     Intelligence, Vol. PAMI-2, No. 1, 67-71.\\n   - Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\".  IEEE Transactions\\n     on Information Theory, May 1972, 431-433.\\n   - See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al\"s AUTOCLASS II\\n     conceptual clustering system finds 3 classes in the data.\\n   - Many, many more ...\\n', 'feature_names': ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']}\n"
     ]
    }
   ],
   "source": [
    "#==RF随机森林应用==\n",
    "from sklearn import datasets\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import numpy as np\n",
    "\n",
    "myDatas = datasets.load_iris()\n",
    "print(\"训练数据\",myDatas)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "测试数据 [[4.5 3.  8.  2.5]\n",
      " [4.5 0.  1.  8. ]]\n",
      "分类结果 [2 0]\n",
      "回归结果 [2 0]\n"
     ]
    }
   ],
   "source": [
    "#测试数据集\n",
    "testDatas = np.array([[4.5,3.0,8.0,2.5],[4.5,0.0,1.0,8]])\n",
    "\n",
    "#分类训练\n",
    "rfc = RandomForestClassifier(n_estimators = 10,max_depth = 3)\n",
    "rfc.fit(myDatas.data,myDatas.target)\n",
    "rfcRs = rfc.predict(testDatas)\n",
    "\n",
    "#回归训练\n",
    "rfr = RandomForestRegressor(n_estimators=10,max_depth=3)\n",
    "rfr.fit(myDatas.data,myDatas.target)\n",
    "rfrRs = rfc.predict(testDatas)\n",
    "\n",
    "\n",
    "\n",
    "print(\"测试数据\",testDatas)\n",
    "print(\"分类结果\",rfcRs)\n",
    "print(\"回归结果\",rfrRs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 集成学习概念:\n",
    "\n",
    "多个弱学习分类器算法结合，综合使用，通过取长补短加权等方式变强学习器.\n",
    "如多棵决策树组成随机森林RF,其它算法还有AdaBoost、GBDT、XGBoost、LightGBM、CatBoost等。\n",
    "类别：同质集成（同种类型个体学习器结合，这种个体学习器叫基学习器）；\n",
    "异质集成（由不同类型学习算法组成，这种个体学习器叫组件学习器）\n",
    "如何训练多个弱分类器以及如何组合多个弱分类器，常用的方法有boosting、bagging.\n",
    "\n",
    "2. 个体学习器概念:\n",
    "单一的机器学习算法，如单一的决策树算法\n",
    "\n",
    "3. boosting  bagging:\n",
    "\n",
    "boosting(译：增进 推进 提升) \n",
    "串行学习，先训练单个学习器，再根据学习结果调整后续训练样本分布，对做错的加关注，再调整的基础上训练下一个学习器。\n",
    "\n",
    "bagging(译：打包)\n",
    "并行学习，同时训练多个单一学习器，用同一批训练数据随机采样（有放回的，多次供多个学习器使用）进行分类训练，通过训练结果，根据各学习器训练效果采取加权微调，再一次采样训练时考虑之前的权重分配样本，直到训练到最终优质结果。最后选择最优的分类器，或取各分类器结果的平均做为最终的分类结果。\n",
    "\n",
    "\n",
    "4. 结合策略(平均法，投票法，学习法)：\n",
    "\n",
    "如3中所说，结合策略是指各个体学习器结合生成最终结果的方法。\n",
    "一般结合方法分为：平均法，投票法，学习法。\n",
    "\n",
    "平均法（数值型输出）：\n",
    "简单平均：将各个体学习器结果求平均做为最终学习结果。\n",
    "\n",
    "加权平均：将各学习器结果带各自的权重求平均做为最终学习结果。\n",
    "该策略典型代表是AdaBoost算法。\n",
    "\n",
    "//个体学习器性能相差大时一般用加权平均法\n",
    "\n",
    "投票法（分类任务）：\n",
    "\n",
    "绝对多数投票：取各分类器大多数（一半以上）分类结果做最终分类结果，如10个个体分类器，有8个分类一致属于A类，则取A这个结果做为最终的分类结果。\n",
    "\n",
    "相对多数投票：同时有多个分类labels得最高票，则从中随机选一个做最终分类结果。\n",
    "\n",
    "\n",
    "加权投票：\n",
    "与加权平均类似，指采样过程中动态修正各个体分类器权值，最终求得最终分类结果。\n",
    "\n",
    "学习法：将个体学习器的学习结果，不直接取结果，而是输入到一个学习器中，把结果进行再次训练学习，取结果。最终这个学习器叫次级学习器或元学习器（常见的有Stacking和Blending），前面的训练第一阶段数据的个体学习器叫初级学习器。\n",
    "学习法的结合策略更为强大，训练大量数据时可用。\n",
    "\n",
    "5. 随机森林思想:\n",
    "利用随机方式将多个决策树组合成一个森林。它是bagging并行集成方法。\n",
    "实现步骤有4个方面：\n",
    "\n",
    "随机选样本：有放回随机采样得到不同子决策树的训练集，比如一个训练样本数量为N，有放回采样采N次，每次采一个，放回，继续采，这样会采到重复样本。这样构成的新训练集，给各个子决策树使用，有效利用有限样本，是一个洗牌作用，用这个新样本作训练集。\n",
    "\n",
    "随机选特征：\n",
    "单个决策树是通过比较计算最大增益的特征作为最优选特征，划分分类节点。\n",
    "随机森林不是计算所有特征信息增益，而是从总特征中随机抽取一部分，计算这部分特征的增益，来选最优特征（属性），来确定分类切分节点。注意，特征随机是无放回的（可能因为本身多个树使用多次放回随机同一份数据训练，随机多个特征计算比较本来就已经是很复杂的，两步随机已经足够用，且大量数据多用多训越好，特征就那么多，只是相互比较选一个最优，没有必要再随机放回打乱，增加运算负担）。\n",
    "\n",
    "构建决策树：用上面随机训练样本构建多个决策树，用随机选特征方法选各树特征切分树。\n",
    "\n",
    "随机森林投票分类：\n",
    "最终分类结果通过上面4中说的投票法，来选择绝对多数分类，或随机选并列最高分类之一，或加权随机选分类结果，做为最终森林的分类结果。\n",
    "\n",
    "6. 随机森林的推广：\n",
    "\n",
    "Extra Tree：\n",
    "是随机森林RF的变种，原理同。\n",
    "二者区别：\n",
    "RF随机采样决定子决策树训练集。Extra Tree每个树都用了原始数据。\n",
    "RF选特征切分分类时和传统决策树一个，基于信息增益、信息增益率、基尼系数等。\n",
    "原则方法。Extra Tree比较极端，它是随机选一个特征。\n",
    "由此，它分出的树会比RF庞大，但它的方差会减少（随机选择，排除人为干扰，使波动更平稳自然），所以泛化能力（适用更大众）比RF强。\n",
    "\n",
    "TRTE（Totally Random Trees Embedding）：\n",
    "TRTE （译：完全随机树嵌入矩阵）\n",
    "它是将初始数据落在决策树中叶子节点的位置用向量编码表示，\n",
    "比如数据落入树1第3节点表示为00100，落入2树1节点表示为10000，第3树5节点表示为00001\n",
    "则这数据的特征映射码为（0,0,1,0,0,1,0,0,0,0,0,0,0,0,1）\n",
    "通过这种数据转化方式，把低维数据变高维，再分类。\n",
    "分类方法是通过把相似的特征码聚类到一起来完成。\n",
    "\n",
    "TRTE的数据转化方式是非监督的。\n",
    "转化后的高维数据可用于无监督分类，也可用有监督回归分类器分类。\n",
    "\n",
    "但是要注意：这种转换数据方式也有可能转成低维的，比如词袋模型。\n",
    "特征有2000个，可能做完TRTE后只剩几百个。（这和词袋模型有太多重复词有关，决策树比较特征切分过程相当于做了过滤）\n",
    "\n",
    "\n",
    "IForest（Isolation Forest）：\n",
    "译：隔离森林、孤立森林\n",
    "它的思想是异常点检测：用一个很小的切分标准，很少的数据，只需把异常的区分出来，隔离即可。\n",
    "所以，它和RF的区别如下：\n",
    "IForest 随机采样时只需少量数据；\n",
    "建决策树时划分特征随机选（随便挑一个），切分的阈值也是随机选一个；\n",
    "深度max_depth较小（决策树的规模不大）；\n",
    "判断异常的方法：通过算子节点的深度和平均深度比较，算异常概率。\n",
    "\n",
    "7. 随机森林的优缺点：\n",
    "\n",
    "RF优点：\n",
    "训练可并行，大量样本训练时有速度优势；\n",
    "随机选决策树划分特征列表，提高训练性能，即使是样本较高维时也一样；\n",
    "决策树的节点可列出各特征的重要性权重百分比，得到特征重要性列表；\n",
    "随机抽样，模型方差小，泛化能力强；\n",
    "实现简单（随机抽样、随机选特征、建树、投票决策结果）\n",
    "对部分特征丢失（本来就是随机选特征切分的，丢弃了一些了）不敏感；\n",
    "\n",
    "RF缺点：\n",
    "噪声（非决定性特征）过大的特征上，容易过拟合（即选了一个非常多权重，但并没有实际意义的特征做切分点，换其它常规测试集时不能泛化）；\n",
    "取值较多的划分特征对RF的决策会产生更大的影响，可能影响模型效果（比如上面，噪声特征很大，但并不影响它的决策地位，这样的模型是没有意义的）。\n",
    "\n",
    "8. sklearn参数\n",
    "sklearn.ensemble.RandomForestClassifier(\n",
    "\tn_estimators=10,#【随机森林树的个数】\n",
    "\tcriterion='gini', \n",
    "#采用什么标准衡量特征重要性，此处是基尼系数  \n",
    "\tmax_depth=None, \n",
    " #森林最大深度   \n",
    "\tmin_samples_split=2, \n",
    "#树分裂的最小个数   \n",
    "\tmin_samples_leaf=1,\n",
    "  #【某叶节点上样本的最小数量（少于该值时该节点不再切分）】 \n",
    "\tmin_weight_fraction_leaf=0.0,\n",
    "#叶节点最小加权分数\n",
    "\tmax_features='auto',\n",
    "  #生成最佳森林时最大特征数  \n",
    "\tmax_leaf_nodes=None, \n",
    " #最大叶节点   \n",
    "\tmin_impurity_decrease=0.0, \n",
    "    \n",
    "\tmin_impurity_split=None,\n",
    "    \n",
    "\tbootstrap=True, \n",
    "    \n",
    "\toob_score=False, \n",
    "    \n",
    "\tn_jobs=1, \n",
    "    \n",
    "\trandom_state=None,\n",
    " #随机种子\n",
    "\tverbose=0, \n",
    "    \n",
    "\twarm_start=False, \n",
    "    \n",
    "\tclass_weight=None\n",
    ")\n",
    "\n",
    "9.随机森林应用场景：\n",
    "适用于非连续性（非线性）特征分类；\n",
    "算法bagging思想并行集成，算法容易处理高维数据（不需降维）；\n",
    "需要大量训练数据时；\n",
    "对分类准确性要求较高时；\n",
    "\n",
    "其它分类方法应用场景比较：\n",
    "参考：\n",
    "https://blog.csdn.net/u010770184/article/details/53841743"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
