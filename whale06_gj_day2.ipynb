{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 【Task2 GBDT算法梳理】\n",
    "【参考框架】欢迎有自己的框架\n",
    "1,GBDT\n",
    "2,前向分布算法\n",
    "3,负梯度拟合\n",
    "4,损失函数\n",
    "5,回归\n",
    "6,二分类，多分类\n",
    "7,正则化\n",
    "8,优缺点\n",
    "9,sklearn参数\n",
    "10,应用场景"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "1,GBDT：\n",
    "\n",
    "Gradient Boosting Decision Tree（译：梯度、变化率、梯度变化曲线、倾斜, 提升、发展, 决策树）\n",
    "梯度上升树\n",
    "集成学习算法的一种，通过多个弱学习器按一定规则结合，达到强学习器效果。\n",
    "GBDT 采用 boosting方法 逐步提升串行学习\n",
    "（先训练单个学习器，再根据学习结果调整后续训练样本分布，对做错的加关注，\n",
    "再调整的基础上训练下一个学习器。提升是加法和前向分布的结合）。\n",
    "RF是并行bagging学习。\n",
    "GBDT都是回归树，不是分类树（但也可通过调整用于分类任务）,\n",
    "RF既可以分类也可以回归。\n",
    "（分类问题比如不同性别的分类；回归问题比如年龄等连续性数值型数据的变化回归 ）\n",
    "分类树是通过在每个叶子节点设置阈值来划分分类，选叶子节点的方式是通过信息熵和信息增益\n",
    "或基尼系数优选最适合的；\n",
    "回归树的核心是--每棵树是之前所有树结论和的残差（真实值-预测值），\n",
    "即，再加一个预测值即可得到真实值的累加量。\n",
    "比如：真实年龄18岁，第一棵树预测了12岁，差6岁，6就是残差（真实18-预测12）。\n",
    "把第二棵树设为6岁，则树一+树二=真实年龄。\n",
    "如果树二是5岁，则还有1岁的残差，第三棵树设为1岁，继续学习。\n",
    "Gradient（译：梯度，变化，倾斜）的梯度变化就体现在GBDT它是算差的，\n",
    "它的损失函数是以误差作衡量标准，所以求最优就是求损失最小的梯度斜率变化回归问题。\n",
    "所以它叫梯度提升树，核心在于梯度变化更新回归。\n",
    "\n",
    "2,前向分布算法：\n",
    "boosting提升算法是通过加法模型(每个基学习器累加学习)实现。\n",
    "GBDT的核心思想，如前所说，是算之前树结论和的残差（目的是让之前树结论的和补上一个残差最接近真实值）\n",
    "所以它算的不是简单的把基学习器结果一个个累加起来，而是根据前面的结果，\n",
    "每一步只学习一个基函数及其系数，然后逐步逼近优化目标，\n",
    "算法得到了简化，将同时求解从m=1到M的所有参数优化问题，简化为逐次求解各个参数分别单独优化问题。\n",
    "\n",
    "优化所有参数与前向分布优化分步参数的公式对比：\n",
    "见文末\n",
    "\n",
    "3,负梯度拟合\n",
    "梯度提升树GBDT用梯度下降的值前面加负号求损失函数最小的过程叫负梯度拟合。\n",
    "这个梯度就是残差的近似值。使损失函数最小的这个梯度（残差）就是补上它以后最接近真实值（前树和+残差=真实值）。\n",
    "\n",
    "4,损失函数\n",
    "不同问题的提升树算法，其主要区别在于损失函数不同。\n",
    "平方损失函数常用于回归问题，对数、指数损失函数用于分类问题，绝对损失函数用于决策问题。\n",
    "\n",
    "用于分类：\n",
    "\n",
    "指数损失函数公式：\n",
    "L(y,h(x))=exp(−yh(x))\n",
    "对数损失函数分类（二分类，多元分类）\n",
    "二分类公式：\n",
    "L(y,h(x))=log(1+exp(−yh(x)))\n",
    "\n",
    "用于回归：\n",
    "\n",
    "均方差：\n",
    "L(y,h(x))=(y−h(x))^2\n",
    "绝对损失：\n",
    "L(y,h(x))=|y−h(x)|\n",
    "对应的负梯度误差等于sign(y_i−h(x_i))\n",
    "Huber损失（均方差和绝对损失的折衷）：\n",
    "对于远离中心的异常点，采用绝对损失，中心附近的点采用均方差。这个界限一般用分位数点度量。\n",
    "分位数损失：\n",
    "它对应的是分位数回归的损失函数。\n",
    "\n",
    "5,回归\n",
    "步骤：\n",
    "输入样本；\n",
    "初始化基学习器；\n",
    "迭代每个基学习器；\n",
    "计算当次迭代的负梯度；\n",
    "更新参数得到强学习器；\n",
    "\n",
    "6,二分类，多分类\n",
    "由于分类问题输出的不是连续数值，是离散的类别，这个我们不能根据GBDT算法的核心思想计算树与树之间的差值。\n",
    "所以需要把损失函数转成指数或对数，把结果变成一个[-1,1]之间的分类概率值，变连续性数值型，\n",
    "才可以共用计算残差的公式来求负梯度。\n",
    "\n",
    "二分类GBDT，类似于逻辑回归的对数似然损失函数，公式为：\n",
    "L(y,h(x))=log(1+exp(−y_h(x)))\n",
    "\n",
    "多分类：\n",
    "对于多分类任务，GDBT的做法是采用一对多的策略，\n",
    "如：每个类别训练M个分类器，K个类别，\n",
    "通过两层for循环遍历依次为每个分类器（每棵树）拟合K个类别。\n",
    "\n",
    "7,正则化\n",
    "GBDT正则化防止过拟和（模型训练效果过优，用于测试集时泛化能力弱，适用覆盖不广泛）\n",
    "方法一：\n",
    "给加基学习器加法模型加正则化项中和平衡；\n",
    "方法二：\n",
    "用随机梯度提升树（Stochastic Gradient Boosting Tree, SGBT），\n",
    "即通过子采样比例实现正则化，取一部分样本做GBDT决策树拟合，不用全部样本，\n",
    "这样可以减少方差，\n",
    "方差衡量每个值偏离均值的程度，选取的越少方差变化越小，但可能偏离真实值（偏差）越大，\n",
    "所以样本比例也不宜过小。\n",
    "//使用了子采样，程序可以通过不同的采样分发到多个任务去做boosting的迭代过程，\n",
    "//形成新树，实现并行。\n",
    "方法三：\n",
    "对每个基学习器CART回归树进行正则化剪枝，去掉不必要的节点，使细化复杂的树简化，泛化能力更强。\n",
    "\n",
    "8,优缺点\n",
    "优点：\n",
    "可以灵活处理各种类型数据（连续值，离散值）\n",
    "和SVM支持向量机相比，不用多次调参（它的调参是只考虑当前步骤的前向分布算法），也可以有较高的预测准确率；\n",
    "使用一些健壮的损失函数（如Huber的大区域分位数区分方式），对异常值监测能力很强; \n",
    "缺点：\n",
    "弱学习器之间存在前后依赖关系，难以并行训练数据。\n",
    "可以通过自采样的SGBT（Stochastic Gradient Boosting Tree随机梯度提升树）\n",
    "来达到部分并行。\n",
    "\n",
    "9,sklearn参数\n",
    "class sklearn.ensemble.GradientBoostingClassifier(\n",
    "            loss=’deviance’, #损失函数\n",
    "            learning_rate=0.1,#步长,学习率\n",
    "            n_estimators=100, #弱学习器个数，也即最大迭代次数\n",
    "            subsample=1.0,#子采样比例\n",
    "            criterion=’friedman_mse’,\n",
    "            min_samples_split=2,#树分裂的最小个数\n",
    "            min_samples_leaf=1, #【某叶节点上样本的最小数量（少于该值时该节点不再切分）】 \n",
    "            min_weight_fraction_leaf=0.0, \n",
    "            max_depth=3, #树最大深度\n",
    "            min_impurity_decrease=0.0, \n",
    "            min_impurity_split=None, \n",
    "            init=None, \n",
    "            random_state=None, #随机种子\n",
    "            max_features=None, #最大特征数\n",
    "            verbose=0, \n",
    "            max_leaf_nodes=None, #最多叶节点数\n",
    "            warm_start=False, \n",
    "            presort=’auto’,\n",
    "            alpha='0.9'#分位数值,用于Huber)\n",
    "\n",
    "10,应用场景：\n",
    "\n",
    "用于所有回归问题，\n",
    "逻辑回归LR用于线性回归，\n",
    "GBDT线性非线性都可，\n",
    "也可用于二分类问题（设定阈值，大于阈值为正例，反之为负例）。\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
