{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "import pandas as pd\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "# from collections import Counter\n",
    "# from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# from sklearn.linear_model import RidgeClassifier\n",
    "# from sklearn.metrics import f1_score\n",
    "# from sklearn.svm import SVC\n",
    "# from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>2967 6758 339 2021 1854 3731 4109 3792 4149 15...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11</td>\n",
       "      <td>4464 486 6352 5619 2465 4802 1452 3137 5778 54...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>7346 4068 5074 3747 5681 6093 1777 2226 7354 6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>7159 948 4866 2109 5520 2490 211 3956 5520 549...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>3646 3055 3055 2490 4659 6065 3370 5814 2465 5...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                               text\n",
       "0      2  2967 6758 339 2021 1854 3731 4109 3792 4149 15...\n",
       "1     11  4464 486 6352 5619 2465 4802 1452 3137 5778 54...\n",
       "2      3  7346 4068 5074 3747 5681 6093 1777 2226 7354 6...\n",
       "3      2  7159 948 4866 2109 5520 2490 211 3956 5520 549...\n",
       "4      3  3646 3055 3055 2490 4659 6065 3370 5814 2465 5..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#数据处理\n",
    "#读数据\n",
    "df = pd.read_csv(\"data//train_set.csv\",sep = '\\t',nrows=20000)#index_col=0去掉索引\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#数据复制\n",
    "def copyNews(dataDf,tag):\n",
    "    smallNewsDf = dataDf[dataDf['label']==tag]\n",
    "    dataDf = pd.concat([dataDf,smallNewsDf,smallNewsDf])\n",
    "    return dataDf\n",
    "df = copyNews(df,13)\n",
    "df = copyNews(df,12)\n",
    "df = copyNews(df,11)\n",
    "df = copyNews(df,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3750，648，900排名前三的很可能是标点符号移除字符\n",
    "def delWord(text):\n",
    "    text = text.replace('3750','')\n",
    "    text = text.replace('648','')\n",
    "    text = text.replace('900','')\n",
    "    return text\n",
    "df['text'] =df['text'].apply(delWord)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 22184 entries, 0 to 19996\n",
      "Data columns (total 2 columns):\n",
      "label    22184 non-null int64\n",
      "text     22184 non-null object\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 519.9+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#word2vec训练词向量\n",
    "#训练词向量、保存\n",
    "def trainWord2vec(sents,savePath = 'word2vecModel'):\n",
    "    model = Word2Vec(sents,\n",
    "                     size=50,#词向量维度\n",
    "                     window=12,#扫描句子窗口大小，即中心词与周围词最大距离，默认值为5#根据句子长短要改，注意\n",
    "                     min_count = 2,#过滤低频词，默认5\n",
    "                     max_vocab_size = None,#设置RAM限制\n",
    "                     sample = 0.001,#默认0.001，高频词汇随机降采样阈值，范围（0,le-5），默认le-3\n",
    "                     seed=1,\n",
    "                     workers = 3,#训练模型的线程\n",
    "                     min_alpha = 0.0001,#学习率，默认0.0001\n",
    "                     sg = 1,#0为CBOW算法 周围词预测中心词，1为skip-gram算法 中心词预测周围词\n",
    "                     hs = 0,#0 为negative sampling负采样，1为 hierarchical softmax 层级softmax 二叉树\n",
    "                     negative = 5, #负采样设置多少个noise words噪声词，>0时会采用负采样。默认5\n",
    "                     cbow_mean = 1,#只有使用cbow时起作用，即sg=0时。为1取周围词均值。为0时取周围词（上下文词）之和。\n",
    "                     iter =3,#迭代遍历语料库的次数，默认5\n",
    "                     null_word = 0,\n",
    "                     trim_rule = None,#设置词汇表的整理规则，哪些词去留，None时min_count会被使用\n",
    "                     sorted_vocab = 1,#1为降序，对word index词频排序，由多到少，默认1\n",
    "                     batch_words = 10000,#默认10000，每批传词数量\n",
    "                     compute_loss = False#是否计算loss，默认False\n",
    "                    )\n",
    "    #这里可以加模型中的方法\n",
    "    #……\n",
    "    #保存\n",
    "    model.save(savePath)\n",
    "\n",
    "#读模型\n",
    "def readWord2VecModel(savePath):\n",
    "    newModel = Word2Vec.load(savePath)\n",
    "    return newModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text_list'] = df['text'].apply(lambda x:x.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataList = list(df['text_list'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainWord2vec(dataList,\"data//dataWord2Vec\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataWord2Vec = readWord2VecModel(\"data//dataWord2Vec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataWord2Vec.wv.vocab.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G:\\ph\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.20526437,  0.25692332, -0.19281515, -0.06688049,  0.00972781,\n",
       "        0.15549842,  0.25335723, -0.13391559, -0.04933024, -0.0709722 ,\n",
       "        0.10698353,  0.20760068, -0.0171919 ,  0.05800337, -0.07830507,\n",
       "        0.10510696, -0.32054543, -0.33522087, -0.01314532,  0.01824752,\n",
       "       -0.07801764, -0.11822597,  0.10061953, -0.01857546,  0.04716896,\n",
       "       -0.17063451, -0.25374296, -0.12003005, -0.32022393, -0.05361705,\n",
       "       -0.09556497,  0.11839112, -0.02262084,  0.02047583, -0.06488313,\n",
       "        0.06642535, -0.0056026 ,  0.5001628 ,  0.0399593 ,  0.17953101,\n",
       "        0.01899508,  0.26151606,  0.2785155 ,  0.03797152, -0.04676527,\n",
       "        0.00943195,  0.03291901, -0.16415478,  0.00297387, -0.00206015,\n",
       "        0.12788042,  0.04247842, -0.01513307, -0.1937139 , -0.24787813,\n",
       "        0.12838095, -0.01673364,  0.12094463, -0.06302661, -0.19069491,\n",
       "       -0.11849233, -0.1599827 , -0.06362164,  0.15121706, -0.09405729,\n",
       "       -0.09155288, -0.09097308,  0.20847571,  0.15890183, -0.14971542,\n",
       "        0.17459017,  0.0162772 , -0.15271029,  0.04385382,  0.11285927,\n",
       "        0.13118799,  0.00609839, -0.12181883, -0.14803752,  0.33112478,\n",
       "        0.24196011, -0.07139246, -0.14015347, -0.16316211,  0.2307022 ,\n",
       "        0.18326008, -0.10299309, -0.13919805,  0.01134909,  0.12578394,\n",
       "       -0.05645741, -0.02416129,  0.3947384 ,  0.02240204,  0.04130474,\n",
       "       -0.44402078,  0.0155309 ,  0.02114842, -0.3338052 , -0.08744562],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dataWord2Vec['6758']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TFidf\n",
    "# 词频逆文档频率-与上面单纯的数数相关，这里带了概率特征\n",
    "tfidf = TfidfVectorizer(ngram_range = (1,3), max_features=3000)\n",
    "dataVec = tfidf.fit_transform(df['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15000, 3000)\n",
      "(15000,)\n",
      "(7184, 3000)\n",
      "(7184,)\n"
     ]
    }
   ],
   "source": [
    "# 划分数据集\n",
    "train_x = dataVec[:15000]\n",
    "train_y = df['label'][:15000]\n",
    "test_x = dataVec[15000:]\n",
    "test_y = df['label'][15000:]\n",
    "print(train_x.shape)\n",
    "print(train_y.shape)\n",
    "print(test_x.shape)\n",
    "print(test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#textCNN文本分类实现\n",
    "def CNN_model(train_x,train_y,test_x,test_y):\n",
    "    model = keras.models.Sequential()\n",
    "#     model.add(Embedding(len(vocab)+1,300,input_length=50))#传入编码转向量\n",
    "    model.add(keras.layers.Conv1D(256,5,padding='same'))\n",
    "    model.add(keras.layers.MaxPooling1D(3,3,padding='same'))\n",
    "    model.add(keras.layers.Conv1D(128,5,padding='same'))\n",
    "    model.add(keras.layers.MaxPooling1D(3,3,padding='same'))\n",
    "    model.add(keras.layers.Conv1D(64,3,padding='same'))\n",
    "    model.add(keras.layers.Flatten)\n",
    "    model.add(keras.layers.Dropout(0.1))\n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "    model.add(keras.layers.Dense(256,activation='relu'))\n",
    "    model.add(keras.layers.Dropout(0.1))\n",
    "    model.add(keras.layers.Dense(14,activiation = 'softmax'))\n",
    "    model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "    model.fit(train_x,train_y,epochs=2,batch_size=800)\n",
    "    y_pred = model.predict_classes(test_x)\n",
    "    y_pred = list(map(str,y_predict))\n",
    "    print(\"准确率\",metrics.accuracy_score(test_y,y_pred))\n",
    "    print(\"f1，\",metrics.f1_score(test_y,y_pred,average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CNN_model(train_x,train_y,test_x,test_y)#【待调试】"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RNN\n",
    "def LSTM_model(train_x,train_y,test_x,test_y):\n",
    "    model = keras.models.Sequential()\n",
    "#     model.add(Embedding(len(vocab)+1,300,input_length=50))#传入编码转向量\n",
    "    model.add(keras.layers.LSTM(128,dropout= 0.2))\n",
    "    model.add(keras.layers.Dense(128))\n",
    "    model.add(keras.layers.Activation('relu'))\n",
    "    model.add(keras.layers.Dropout(0.1))\n",
    "    model.add(keras.layers.Dense(14,activation=\"softmax\"))\n",
    "    model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "    model.fit(train_x,train_y,epochs=2,batch_size=800,validation_data=(test_x,test_y))\n",
    "    score,acc  = model.evaluate(test_x,test_y,batch_size=800)\n",
    "    print(\"score:\",score)\n",
    "    print(\"acc\",acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LSTM_model(train_x.toarray(),train_y,test_x.toarray(),test_y)#【输入维度待调】"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
