{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "【Task3 XGB算法梳理】\n",
    "【参考框架】欢迎有自己的框架\n",
    "XGB\n",
    "\n",
    "1，算法原理\n",
    "2，损失函数\n",
    "3，分裂结点算法\n",
    "4，正则化\n",
    "5，对缺失值处理\n",
    "6，优缺点\n",
    "7，应用场景\n",
    "8，sklearn参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "9、xgb sklearn应用\n",
    "'''\n",
    "from sklearn import datasets\n",
    "import numpy as np\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from xgboost.sklearn import XGBRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(105, 4)\n",
      "(45, 4)\n",
      "(105,)\n",
      "(45,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "myDatas = datasets.load_iris()\n",
    "#print(\"训练数据\",myDatas)\n",
    "#划分训练集、测试集\n",
    "X_train,X_test,y_train,y_test = train_test_split(\n",
    "    myDatas.data,myDatas.target,#load_iris的原始数据集\n",
    "    test_size = 0.3,\n",
    "    random_state = 7\n",
    ")\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "预测精确度： 0.9333333333333333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G:\\Users\\Administrator\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#==分类任务==\n",
    "#实例化xgb分类器对象\n",
    "clfXgb = XGBClassifier(n_estimators=10,max_depth=3,learning_rate=0.1)\n",
    "#learning_rate = 0.1 根据经验是最合适的学率，精确度0.93，改成0.5，精确度降为0.91了。\n",
    "#训练分类\n",
    "clfXgb.fit(X_train,y_train)\n",
    "#预测\n",
    "clfXgbPred = clfXgb.predict(X_test)\n",
    "\n",
    "#评估得分\n",
    "acc = accuracy_score(y_test,clfXgbPred)\n",
    "\n",
    "print(\"预测精确度：\",acc)\n",
    "\n",
    "#==以上就完成了sklearn的XGBoost分类预测了==\n",
    "\n",
    "#==回归任务==\n",
    "#同上，换成XGBRegressor\n",
    "#rXgb = XGBRegressor(n_estimators=10,max_depth=3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-mlogloss:0.871668\n",
      "Will train until validation_0-mlogloss hasn't improved in 3 rounds.\n",
      "[1]\tvalidation_0-mlogloss:0.714822\n",
      "[2]\tvalidation_0-mlogloss:0.600959\n",
      "[3]\tvalidation_0-mlogloss:0.51605\n",
      "[4]\tvalidation_0-mlogloss:0.451743\n",
      "[5]\tvalidation_0-mlogloss:0.402619\n",
      "[6]\tvalidation_0-mlogloss:0.364957\n",
      "[7]\tvalidation_0-mlogloss:0.336098\n",
      "[8]\tvalidation_0-mlogloss:0.310766\n",
      "[9]\tvalidation_0-mlogloss:0.290692\n",
      "\n",
      "预测精确度: 0.9333333333333333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G:\\Users\\Administrator\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "#==下面来看下每一步的test预测值，\n",
    "#看是否XGBoost每加一棵树都会让集成学习效果优化提升(这是XGBoost的核心思想)==\n",
    "\n",
    "eval_set = [(X_test,y_test)]#构造一个测试集\n",
    "\n",
    "clfXgb.fit(X_train,y_train,early_stopping_rounds=3,\n",
    "           eval_metric='mlogloss',eval_set = eval_set,verbose = True)\n",
    "#参数：模型饱和后再加3次停止该模型\n",
    "#指定mlogloss为损失函数，用来做模型优化标准，使logloss最小。\n",
    "#测试值\n",
    "clfXgbPred_2 = clfXgb.predict(X_test)\n",
    "#把预测值装进预测值列表\n",
    "predictions = [round(v) for v in clfXgbPred_2]\n",
    "#遍历预测结果评估\n",
    "acc_2 = accuracy_score(y_test,predictions)#每个测试结果和它对应的所有预测值比较分别评估\n",
    "\n",
    "print(\"\\n预测精确度:\",acc_2)\n",
    "#由结果可见XGB的确是在每步加入新村时使得集成学习向优化提升（损失越来越小，预测越来越接近真实值）\n",
    "#另外上面设了early_stopping_rounds 为3 说明从底下往上数3个0.56时模型就已经是饱和状态了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#==下面看一个xgboost的功能\n",
    "#plot_importantce，\n",
    "#可以查看特征重要性==\n",
    "from xgboost import plot_importance\n",
    "from matplotlib import pyplot\n",
    "\n",
    "model = XGBClassifier()#实例化分类器对象\n",
    "model.fit(myDatas.data,myDatas.target)#对象fit传原始数据集即可\n",
    "rs = plot_importance(model)#算特征重要性\n",
    "pyplot.show(rs)#画图\n",
    "#图上列出了4个特征重要性\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==网格调参，\n",
    "#模型建好以后要选一些合适的参数，让模型最优（损失最小）才是目的，\n",
    "#然后把这些参数应用到模型，重新建模保存，服务于更多任务的测试工作==\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "最优学习率: {'learning_rate': 0.1} ，得分：-0.169467 \n",
      "每次迭代的平均值： [-1.08640095 -0.98505858 -0.169467   -0.18732891 -0.18775922]\n",
      "对应的本次迭代学习率参数： [{'learning_rate': 0.0001}, {'learning_rate': 0.001}, {'learning_rate': 0.1}, {'learning_rate': 0.2}, {'learning_rate': 0.3}]\n"
     ]
    }
   ],
   "source": [
    "myMode = XGBClassifier()#正常这里是不会人为揸定参数，要求最合适参数，上面实例只是根据经验传了固定参数展示分类实现，\n",
    "#以学习率为例，找一个最合适的学习率\n",
    "#设几个不同学习率的列表，后面来遍历它，看哪个学习率下分类精确度最高，就用哪个学习率代回模型重新建模\n",
    "learning_rate=[0.0001,0.001,0.1,0.2,0.3]\n",
    "#这次使用交叉验证（交替充份使用有限数据）划分数据集\n",
    "#实例化交叉验证类\n",
    "kfold = StratifiedKFold(n_splits=2,shuffle=True,random_state=7)\n",
    "#n_splits分成几组测试验证对\n",
    "#实例化网格调参类（传入交叉验实例对象及XGB分类对象）\n",
    "grid_search = GridSearchCV(myMode,#传入XGB分类对象\n",
    "                           dict(learning_rate=learning_rate),#这里要字典格式打包传参\n",
    "                           scoring = 'neg_log_loss',#评估损失函数选择\n",
    "                           n_jobs = 1,#当前所有空闲CPU都去跑这个模型\n",
    "                           cv = kfold#指定交叉验证实例对象\n",
    "                          )\n",
    "#用最终结合好的对象fit原始数据即可自动完成交叉验证并调参\n",
    "gridRs = grid_search.fit(myDatas.data,myDatas.target)\n",
    "\n",
    "#打印最优学习率和其得分\n",
    "print(\"最优学习率: %s ，得分：%f \" % (gridRs.best_params_,gridRs.best_score_))\n",
    "means = gridRs.cv_results_['mean_test_score']\n",
    "params = gridRs.cv_results_['params']\n",
    "#打印平均分\n",
    "print(\"每次迭代的平均值：\",means)\n",
    "print(\"对应的本次迭代学习率参数：\",params)\n",
    "\n",
    "'''\n",
    "#以上调参完即可确定最优的learning_rate在模型中使用了，代回去再次建模，\n",
    "#才可得到开篇中的最精确的预测值。\n",
    "#试下把开篇learning_rate = 0.1 改成0.5，精确度从0.93降为0.91了。\n",
    "#此模型才可用于其它同类任务的预测工作，总的流程是这样的。\n",
    "#此处只调了一个参数举例，其它参数必要时也要调\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "XGB\n",
    "\n",
    "1，算法原理\n",
    "XGB(extreme Gradient Boosting) 最强梯度提升树\n",
    "它是一种集成学习算法，把弱分类器（决策树）组合，逐步提升效果，变强学习器。\n",
    "它的提升原理是：把之前树的预测结果保留下来，加上当前树的结果，使效果最好（使加起来的预测结果最接近真实值），\n",
    "但这棵树怎么加，要不要加，不是乱加的，前提是要使整体效果提升，\n",
    "所以这个算法也当成是一个提升boosting算法。\n",
    "怎么做到新加的树是使整体效果提升的？\n",
    "就要通过计算损失函数，使它是下降的（损失降低的）\n",
    "\n",
    "2，损失函数\n",
    "损失函数 L = (预测值-真实值)^2 \n",
    "目标函数（要求的函数） = L损失函数 + 正则惩罚项\n",
    "正则惩罚项 = 惩罚系数（力度）*T叶子结点数+L2正则项(0.5*参数*样本权重平方和)\n",
    "L损失（预测值-真实值）这个差值也叫残差（梯度提升决策树也叫残差决策树）\n",
    "求它的最小值（每棵树的要累加起来），就是我们的目标，\n",
    "XGBoost的原理就是让这个差值最小，也即预测值最接近真实值，\n",
    "\n",
    "公式最终化简为一个叶节点权重w = 一阶导g之和/二阶导h之和+参数，\n",
    "要使得把它代回原目标函数损失最小。\n",
    "\n",
    "它公式推导化简求解过程中用了泰勒展开（用近似的简单表达来代替复杂的函数代表求解），\n",
    "求一阶导和给一阶导求二阶导（给导数求导即求一个倾斜角度的角度，在前面的基础上的变化）的算梯度方式，\n",
    "更精确快速定位下降的方向。\n",
    "\n",
    "3，分裂结点算法\n",
    "分裂结点即在哪里切分做为划分子左右树的依据，\n",
    "决策树用的是算熵值、基尼系数，来找一个最大特征项来切分，\n",
    "XGBoost是指定分割点，用左子树+右子树分数-没切割时的树得分 这样得到一个切分后的预测结果增益变化值，\n",
    "指定很多个分割点迭代，增益变化最大的（如果没变化，等于没切一样，所以找增益最大的点来切割）这个分割点阈值作为切分依据，\n",
    "在此处分裂结点。\n",
    "\n",
    "\n",
    "4，正则化\n",
    "决策树的叶子结点过多会过拟和，所以引入了正则化，\n",
    "在损失函数后面+正则化项\n",
    "正则化项 = 惩罚系数（力度参数）*T叶子结点数+L2正则项(0.5*参数*叶子权重平方和)\n",
    "两部分的两个参数都是人为指定，分别控制叶子结点数和单个结点的权重，让两者平衡，共同作用，使损失函数最小。\n",
    "\n",
    "5，对缺失值处理\n",
    "对缺失值XGBoost可以自动学习分裂方向，\n",
    "自动学习的实现是通过以下方式：\n",
    "在找分裂点时，不遍历迭代缺失样本，减少计算，\n",
    "分配样本时，缺失的样本同时分到左右子树，计算哪边的增益大就自动分到哪边去。\n",
    "但在测试时如果遇到缺失值，会分到右子树。\n",
    "\n",
    "6，优缺点\n",
    "优点：\n",
    "a,支持线性分类器（相当于引入L1 L2正则惩罚项的LR和线性回归，目标函数公式=误差平方和+正则项，似LR）;\n",
    "b,代价函数（）用了二阶Talor展开，引入一阶导和二阶导,提高模型拟和的速度；\n",
    "（损失函数：一个样本的误差\n",
    "代价函数：整个训练集上所有样本误差的平均\n",
    "目标函数：代价函数 + 正则化项）\n",
    "c,可以给缺失值自动划分方向；\n",
    "d,同RF,支持样本(行)随机抽取，也支持特征(列)随机抽取，降低运算，防过拟合；\n",
    "e,代价函数引入正则化项，控制模型（树）复杂度，\n",
    "正则化项包含全部叶子节点个数，每个叶子节点得分的L2模的平方和（代表叶子节点权重的影响）\n",
    "从贝叶斯（先验累积思想）方差角度考虑，正则降低了模型的方差，防过拟和；\n",
    "f,每次迭代后为叶子分配结合学习速率，减低每棵树权重，减少每棵树影响，灵活调整后面的学习空间；\n",
    "g,支持并行，不是树并行，是把特征值先预排序，存起来，可以重复并行使用计算分裂点；\n",
    "h,分裂依据分开后与未分前的差值增益，不用每个节点排序算增益，减少计算量，可以并行计算，\n",
    "可以引入阈值限制树分裂，控制树的规模；\n",
    "总结优点：快速高效可容错\n",
    "\n",
    "缺点：\n",
    "a,容易过拟合；\n",
    "b,调参困难；\n",
    "\n",
    "7，应用场景\n",
    "\n",
    "用于分类；\n",
    "用于回归；\n",
    "\n",
    "8，sklearn参数\n",
    "\n",
    "XGBClassifier参数:\n",
    "    learning_rate = 0.1, #学习率\n",
    "    n_estimators=100, #分多少棵树\n",
    "    max_depth=5, #树最大深度\n",
    "    min_child_weight=1,#最小权重系数\n",
    "    gamma=0,#惩罚系数（力度）\n",
    "    lambda,#正则化\n",
    "    alpha,#正则化\n",
    "    subsample=0.8,#随机选80%个样本，不想随机就指定1\n",
    "    colsample_bytree = 0.8 #随机选特征\n",
    "    objective = 'binary:logistic'#损失函数loss function,求这个函数的一阶二阶导\n",
    "    scale_pos_weight = 1 #要不要指定一个均衡的树\n",
    "    seed=7 #随机种子，每次复现都是一样的\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
