{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 卷积神经网络 Task7\n",
    "\n",
    "* 卷积运算的定义、动机（稀疏权重、参数共享、等变表示）。一维卷积运算和二维卷积运算。\n",
    "* 反卷积(tf.nn.conv2d_transpose)\n",
    "* 池化运算的定义、种类（最大池化、平均池化等）、动机。\n",
    "* Text-CNN的原理。\n",
    "* 利用Text-CNN模型来进行文本分类。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "利用Text-CNN模型来进行文本分类。\n",
    "'''\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import datetime\n",
    "from tensorflow.contrib import learn\n",
    "#初始化tensorflow 读取数据类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==【读数据模块】==\n",
    "def load_data_and_labels(posiData,negaData):\n",
    "    \n",
    "    positiveText = open(posiData,'rb').read().decode('utf-8')\n",
    "    negativeText = open(negaData,'rb').read().decode('utf-8')\n",
    "    \n",
    "    #拆分文本集\n",
    "    positive_cases = positiveText.split('\\n')[:-1]\n",
    "    negative_cases = negativeText.split('\\n')[:-1]\n",
    "    #邮件集以\\n分隔数组，清除最后一个空元素\n",
    "    \n",
    "    positive_cases = [s.strip() for s in positive_cases]#清除文档两端空格\n",
    "    negative_cases = [s.strip() for s in negative_cases]\n",
    "   \n",
    "\n",
    "    #组合特征集X_text与labels分类y\n",
    "    X_text = positive_cases + negative_cases#正负例混合\n",
    "    X_text = [clean_str(sent) for sent in X_text]#清除空格\n",
    "    \n",
    "    #labels定义，在正例中设为onehot矩阵[0,1]负例中为[1,0]，两类两列\n",
    "    positive_label = [[0,1] for _ in positive_cases]\n",
    "    negative_label = [[1,0] for _ in negative_cases]\n",
    "    #下划线表示 临时变量，仅用一次,只要循环，不取值\n",
    "    y = np.concatenate([positive_label,negative_label],0)#垂直方向合并\n",
    "    \n",
    "    return [X_text,y]\n",
    "    \n",
    "    \n",
    "#清洗数据，标点符号之类的过滤  \n",
    "def clean_str(string):\n",
    "\n",
    "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n",
    "    string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
    "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
    "    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
    "    string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
    "    string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
    "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
    "    string = re.sub(r\",\", \" , \", string)\n",
    "    string = re.sub(r\"!\", \" ! \", string)\n",
    "    string = re.sub(r\"\\(\", \" \\( \", string)\n",
    "    string = re.sub(r\"\\)\", \" \\) \", string)\n",
    "    string = re.sub(r\"\\?\", \" \\? \", string)\n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "    return string.strip().lower()\n",
    "    \n",
    "#定义训练迭代函数\n",
    "def batch_iter(data,batch_size,num_epoch,shuffle=True):\n",
    "    data = np.array(data)\n",
    "    data_size = len(data)\n",
    "    num_batches_per_epoch = int((len(data)-1)/batch_size) +1\n",
    "    #epoche 所有数据都迭代一次，\n",
    "    #每个里面有batch\n",
    "    #一个epoch里有多少个batchsize,+1是为了保本一些\n",
    "    \n",
    "    #遍历epoch组\n",
    "    for epoch in range(num_epoch):\n",
    "        if shuffle:\n",
    "            shuffle_indices = np.random.permutation(np.arange(data_size))\n",
    "            #permutation，洗牌并返回一个洗牌后的矩阵副本索引\n",
    "            shuffle_data = data[shuffle_indices]#//方法：在函数中先用data指代完成操作，再传参进来\n",
    "        else:\n",
    "            shuffle_data = data\n",
    "        #遍历每个epoch里的batch组\n",
    "        for batch_num in range(num_batches_per_epoch):\n",
    "            #定义数据切片起始点\n",
    "            start_index = batch_num*batch_size\n",
    "            end_index = min((batch_num+1)*batch_size,data_size)#比较看是不是文章末尾了\n",
    "            \n",
    "            yield shuffle_data[start_index:end_index]\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n【==步骤总结：==\\n预处理数据\\n转成128维向量\\n多个n-gram filter\\n遍历filter\\n卷积、池化pool\\n全连接\\n最后得到结果】\\n'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#==【构造CNN网络结构】==\n",
    "\n",
    "class TextCNN():\n",
    "    def __init__(self,sequence_length,num_classes,vocab_size,\n",
    "                 embedding_size,filter_size,num_filters,l2_reg_lambda=0.0):\n",
    "        #初始化\n",
    "        self.input_x = tf.placeholder(tf.int32,[None,sequence_length],name = 'input_x')\n",
    "        #输入特征x，长度为句长\n",
    "        self.input_y = tf.placeholder(tf.int32,[None,num_classes],name='input_y')\n",
    "        #输入labels y，长度为分类长度\n",
    "        self.dropout_keep_prob = tf.placeholder(tf.float32,name = 'dropout')\n",
    "        \n",
    "        #正则惩罚项，可先写为常量\n",
    "        l2_loss = tf.constant(0.0)#0.0小数点，定义常数，一个数，不是0,0\n",
    "        \n",
    "        #==embedding层(向量化嵌入矩阵，隐层)处理==\n",
    "        with tf.device('/cpu:0'),tf.name_scope('embedding'):\n",
    "            #初始化w权重\n",
    "            self.W = tf.Variable(tf.random_uniform([vocab_size,embedding_size],-1.0,1.0,name='W'))\n",
    "            #vocab_size文本最大长度（文档不重复词向量onehot编码1万维），映射成embedding_size 128维\n",
    "            #-1.0 1.0向量编码初始化范围 uniform不重复的\n",
    "            \n",
    "            #embedding_lookup窗口滑动取单词，把输入词转成向量（嵌到上面初始化过的W矩阵）\n",
    "            self.embedded_chars = tf.nn.embedding_lookup(self.W,self.input_x)#tf.nn.embedding_lookup 别少了up\n",
    "            #上面组合了滑动窗口宽高，再加一维滑动窗口深度filter变四维\n",
    "            self.embedded_chars_expanded = tf.expand_dims(self.embedded_chars,-1)\n",
    "            \n",
    "            #以上完成了从输入文档的1万维词转为128维向量的操作\n",
    "            \n",
    "        #==卷积层+最大池化层处理==\n",
    "        pooled_outputs = []\n",
    "        \n",
    "        #循环遍历不同大小的filter_size，即一次滑动组装多少个单词， 参数数定义了3，4，5三种\n",
    "        for i,f_size in enumerate(filter_size):\n",
    "            #在命名域中构造不同个数的单词组合向量\n",
    "            with tf.name_scope('conv-maxpool-%s'%f_size):\n",
    "                #写filter shape\n",
    "                filter_shape = [f_size,embedding_size,1,num_filters]\n",
    "                #参数[filter_height, filter_width, in_channels, out_channels]\n",
    "                #[filter高一次滑动取多少个单词，filter宽词向量维度128，深度chanel通道如图象的RGB深度是3个 NLP没有深度默认为1，输出特征图数]\n",
    "                W = tf.Variable(tf.truncated_normal(filter_shape,stddev = 0.1),name='W')\n",
    "                #tf.truncated_normal高斯初始化（初始化为符合高斯正太分布的；stddev = 0.1标准，学习率\n",
    "                #初始化偏移量b为常数即可,shape 1维的 有多少输出特征图就有多少个b\n",
    "                b = tf.Variable(tf.constant(0.1,shape=[num_filters]),name='b')\n",
    "                \n",
    "                #代入以上参数，tf.nn.conv2d开始卷积\n",
    "                conv = tf.nn.conv2d(self.embedded_chars_expanded,\n",
    "                                    W,\n",
    "                                    strides = [1,1,1,1],\n",
    "                                    padding = 'VALID',\n",
    "                                    name = 'conv'\n",
    "                                    )\n",
    "                #第一参：要卷积的对象（组装好的宽高深 四维输入词向量隐层）\n",
    "                #strides滑动步长\n",
    "                #padding不足filter窗口大小时是否填充补列，上面已补为最长单词大小，不需要补了，\n",
    "                #SAME补充，VALID舍弃\n",
    "                \n",
    "                #每个卷积层都跟一个relu激活函数,\n",
    "                h = tf.nn.relu(tf.nn.bias_add(conv,b),name='relu')\n",
    "                #relu里面执行了一个wx+b，relu函数把线性回归转成了非线性非连续性分类概率\n",
    "                \n",
    "                #最大池化层（压缩特征图，以取最大值代表的方式）\n",
    "                pooled = tf.nn.max_pool(h,\n",
    "                    ksize = [1,sequence_length - f_size + 1,1,1],\n",
    "                    strides = [1,1,1,1],\n",
    "                    padding = 'VALID',\n",
    "                    name = 'pool'\n",
    "                )\n",
    "                #h，连前面卷积的输出，\n",
    "                #ksize 池化窗口大小，四维向量，一般是[1, height, width, 1]，\n",
    "                #第1和第4参是batch和channels，因为不在batch和channels上做池化，所以这两个维度设为了1\n",
    "                #最终输出的特征图高度算法公式h句长-filter+2padding/stride 即 sequence_length-f_size+1\n",
    "                \n",
    "                #池化结果存入池化层列表拼合成完整特征图\n",
    "                pooled_outputs.append(pooled)\n",
    "                \n",
    "        #把特征图矩阵拉平，让FC全连接层可以接上\n",
    "        #self.h_pool = tf.concat(3,pooled_outputs)#tf 1.8报错\n",
    "        self.h_pool = tf.concat(pooled_outputs,3)\n",
    "        #concat(3 在第4个维度连接，即深度，多个池化层整体拼成一个。\n",
    "        #拼batch和高宽都是不对的，那些已处理完了，整体把分着的池化层拼起来，即第4维深度。0为第1维\n",
    "        #定义总池化层个数，即总的filter个数一样的，池化是从filter挨个浓缩来的。\n",
    "        num_filters_total = num_filters *len(filter_size)\n",
    "        self.h_pool_flat = tf.reshape(self.h_pool,[-1,num_filters_total])\n",
    "\n",
    "        #给上面结果加一层dropout\n",
    "        with tf.name_scope('dropout'):\n",
    "            self.h_drop = tf.nn.dropout(self.h_pool_flat,self.dropout_keep_prob)\n",
    "            #dropout_keep_prob dropout的比例\n",
    "        \n",
    "        #全连接输出层初始化w\n",
    "        with tf.name_scope('output'):\n",
    "            W = tf.get_variable('W',\n",
    "            shape = [num_filters_total,num_classes],\n",
    "            initializer = tf.contrib.layers.xavier_initializer()\n",
    "            )\n",
    "            #shape 输入：拉平的pool层，输出：分类数\n",
    "            #initializer初始化\n",
    "            #定义b(之前竟然忘记了)，注意这个b的shape和分类数一样，输出层对应的是输出分类个b\n",
    "            b = tf.Variable(tf.constant(0.1,shape=[num_classes]),name='b')\n",
    "           \n",
    "            l2_loss += tf.nn.l2_loss(W)#W的正则惩罚\n",
    "            l2_loss += tf.nn.l2_loss(b)#b的正则惩罚\n",
    "\n",
    "            #得分\n",
    "            self.scores = tf.nn.xw_plus_b(self.h_drop,W,b,name='scores')\n",
    "            # tf.nn.xw_plus_b((x, weights) + biases)函数\n",
    "            #wx plus加 b\n",
    "            #相当于matmul(x, weights) + biases\n",
    "            #h_drop drop后的最终特征图集\n",
    "\n",
    "            #预测\n",
    "            self.predictions = tf.argmax(self.scores,1,name = 'predictions')\n",
    "            #1，axis，argmax返回每一行中的最大位置索引，\n",
    "            #即self.predictions是一个scores得分最高的对应的索引，\n",
    "            #后面用于和得分比较，一致的即预测对了\n",
    "\n",
    "        #损失函数层（softmax分类层）\n",
    "        with tf.name_scope('loss'):\n",
    "            #softmax_cross_entropy_with_logits不要少了_with_logits\n",
    "            losses = tf.nn.softmax_cross_entropy_with_logits(\n",
    "                logits=self.scores,labels = self.input_y)\n",
    "            #softmax交叉熵 （预测分，labels或target输入的分类结果）\n",
    "            self.loss = tf.reduce_mean(losses)+l2_loss*l2_reg_lambda\n",
    "            #求loss平均\n",
    "         \n",
    "        #精确度\n",
    "        with tf.name_scope('acuracy'):\n",
    "            correct_predictions = tf.equal(self.predictions,tf.argmax(self.input_y,1))\n",
    "            #tf.argmax(self.input_y,1)两个参数，不要写成tf.argmax(self.input_y),1\n",
    "            #预测值得分最高值索引与输入值最大相同，则表示预测正确。axis=1按行\n",
    "            self.accuracy = tf.reduce_mean(tf.cast(correct_predictions,'float'),name='accuracy')\n",
    "            #算平均准确率，\n",
    "            #cast bool型False True转数值型 0 1，转成1 0 才能计算平均值\n",
    "                        \n",
    "#TextCNN类完\n",
    "'''\n",
    "【==步骤总结：==\n",
    "预处理数据\n",
    "转成128维向量\n",
    "多个n-gram filter\n",
    "遍历filter\n",
    "卷积、池化pool\n",
    "全连接\n",
    "最后得到结果】\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "参数：\n",
      "L2_REG_LAMBDA=0.0\n",
      "ALLOW_SOFT_PLACEMENT=True\n",
      "BATCH_SIZE=32\n",
      "CHECKPOINT_EVERY=100\n",
      "DEV_SAMPLE_PERCENTAGE=0.1\n",
      "DROPOUT=0.5\n",
      "EMBEDDING=128\n",
      "EVALUATE_EVERY=100\n",
      "F=\n",
      "FILTER_SIZE=3,4,5\n",
      "LOG_DEVICE_PLACEMENT=True\n",
      "NEGATIVE_DATA_FILE=mail/rt-polarity.neg\n",
      "NUM_CHECKPOINTS=5\n",
      "NUM_EPOCHS=200\n",
      "NUM_FILTERS=128\n",
      "POSITIVE_DATA_FILE=mail/rt-polarity.pos\n"
     ]
    }
   ],
   "source": [
    "#==【train模块 - 参数字义】==\n",
    "\n",
    "#==设计参数 用tensorflow标准格式，可以直接使用像类中的self.属性，（不用直接函数中写传的方式\n",
    "#tf.flags.DEFINE_...定义参数（名字，默认值，描述）\n",
    "\n",
    "#==数据参数==\n",
    "tf.flags.DEFINE_float('dev_sample_percentage',.1,'训练集的比例')#.1 10%,别忘了点\n",
    "tf.flags.DEFINE_string('positive_data_file','mail/rt-polarity.pos','正例数据路径')\n",
    "tf.flags.DEFINE_string('negative_data_file','mail/rt-polarity.neg','负例数据路径')\n",
    "\n",
    "#==神经网络参数==\n",
    "tf.flags.DEFINE_integer('embedding',128,'隐层维度')\n",
    "#输入单词的向量维度128列\n",
    "tf.flags.DEFINE_string('filter_size','3,4,5','region滑动窗口大小')\n",
    "#每次滑动组装3，4或5个单词组成多组filter，相当于组合n-gram\n",
    "tf.flags.DEFINE_integer('num_filters',128,'filter数量')\n",
    "#卷积后想要多少特征图\n",
    "tf.flags.DEFINE_float('dropout',0.5,'dropout无用神经元弃用比')\n",
    "tf.flags.DEFINE_float('L2_reg_lambda',0.0,'L2正则惩罚项')\n",
    "\n",
    "#==训练参数==\n",
    "tf.flags.DEFINE_integer('batch_size',32,'模型训练迭代分组数')\n",
    "tf.flags.DEFINE_integer('num_epochs',200,'模型训练迭代轮数')\n",
    "#迭代200轮，每轮把所有64组遍历一次\n",
    "tf.flags.DEFINE_integer('evaluate_every',100,'打印间隔数')\n",
    "#迭代100次打印一次\n",
    "tf.flags.DEFINE_integer('checkpoint_every',100,'模型保存间隔')\n",
    "#迭代100次保存一次模型\n",
    "tf.flags.DEFINE_integer('num_checkpoints',5,'模型最多保存多少条？')\n",
    "\n",
    "#==模型训练设备相关配置==\n",
    "tf.flags.DEFINE_boolean('allow_soft_placement',True,'tf自由选择设备')\n",
    "tf.flags.DEFINE_boolean('log_device_placement',True,'获取tf自动指派的设备并打印')\n",
    "\n",
    "tf.app.flags.DEFINE_string('f', '', 'kernel')\n",
    "#Unknown command line flag 'f' BUG解决\n",
    "\n",
    "#参数解析\n",
    "FLAGS = tf.flags.FLAGS\n",
    "#以下把参数变字典格式 \n",
    "#FLAGS._parse_flags()#tf 1.8版报错\n",
    "FLAGS.flag_values_dict()\n",
    "    \n",
    "\n",
    "\n",
    "#打印参数\n",
    "print(\"\\n参数：\")\n",
    "#tf 1.8版报错\n",
    "#for key,value in sorted(FLAGS._flags.items()):\n",
    "#    print(\"{}={}\".format(key.upper(),value))\n",
    "for key in sorted(FLAGS):\n",
    "    value = FLAGS[key].value\n",
    "    print(\"{}={}\".format(key.upper(),value))    \n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==【train模块 - 读数据开始训练】==\n",
    "#==读数据==\n",
    "X_text, y = load_data_and_labels(FLAGS.positive_data_file,FLAGS.negative_data_file)\n",
    "\n",
    "#预处理数据，\n",
    "#邮件长短不一,不统一，特征图大小也不一样。\n",
    "#所以要让所有邮件大小长度一样，让输入矩阵一样大小，\n",
    "#看X_text中最大的邮件是多大，其它的padding补充\n",
    "max_doc_length = max(len(X.split(' ')) for X in X_text)\n",
    "vocab_processorObj = learn.preprocessing.VocabularyProcessor(max_doc_length)\n",
    "X = np.array(list(vocab_processorObj.fit_transform(X_text)))#外加list\n",
    "#learn.preprocessing用最大文档长度填充\n",
    "\n",
    "#==开始训练==\n",
    "np.random.seed(10)#随机种子\n",
    "#洗牌 y长度（即是X_text长度，y好取）矩阵传给permutation，返回一个洗牌后的矩阵副本\n",
    "shuffle_indices = np.random.permutation(np.arange(len(y)))#arange 一个r\n",
    "X_shuffled = X[shuffle_indices]#index代入洗牌\n",
    "y_shuffled = y[shuffle_indices]\n",
    "\n",
    "#交叉验证 切分数据集为训练集、验证集\n",
    "sample_cut_index = -1*int(FLAGS.dev_sample_percentage*float(len(y)))\n",
    "X_train,X_test = X_shuffled[:sample_cut_index],X_shuffled[sample_cut_index:]#两变量可以定义在一起\n",
    "y_train,y_test = y_shuffled[:sample_cut_index],y_shuffled[sample_cut_index:]\n",
    "#打印\n",
    "print(\"最大文档长度：{:d}\".format(len(vocab_processorObj.vocabulary_)))\n",
    "print(\"训练集/测试集 : {:d}/{:d}\".format(len(y_train),len(y_test)))\n",
    "\n",
    "#==tf图结构处理，执行TextCNN传数据参数==\n",
    "with tf.Graph().as_default():\n",
    "    #自动指派设备(cpu或gpu)\n",
    "    session_conf = tf.ConfigProto(\n",
    "          allow_soft_placement = FLAGS.allow_soft_placement,\n",
    "          log_device_placement = FLAGS.log_device_placement\n",
    "      )\n",
    "      \n",
    "    sess = tf.Session(config = session_conf)\n",
    "      \n",
    "    with sess.as_default():\n",
    "        #TextCNN类实例化\n",
    "        cnn = TextCNN(\n",
    "              sequence_length = X_train.shape[1],\n",
    "              num_classes = y_train.shape[1],\n",
    "              vocab_size = len(vocab_processorObj.vocabulary_),\n",
    "              embedding_size = FLAGS.embedding,\n",
    "              filter_size = list(map(int,FLAGS.filter_size.split(','))),\n",
    "              num_filters = FLAGS.num_filters,\n",
    "              l2_reg_lambda = FLAGS.L2_reg_lambda\n",
    "          )\n",
    "        #X_train.shape[1]文档长度，\n",
    "        #分类数y_train.shape[1] labels 有2列\n",
    "        #vocab_size处理好的size是多大,文本最大长度\n",
    "        #embedding_size隐层维度\n",
    "        #filter_size卷积窗口尺寸,\n",
    "        #FLAGES.fitler_size.split(',')上面逗号定义的3,4,5,每次卷积单词数，\n",
    "        #num_filters filter数（特征图数量）\n",
    "        #l2_reg_lambda正则惩罚项\n",
    "      \n",
    "      \n",
    "        #训练模型\n",
    "        #定义step\n",
    "        global_step = tf.Variable(0,name='global_step')\n",
    "        #实例化优化器train.Adam，梯度下降也可以，\n",
    "        optimizer = tf.train.AdamOptimizer(1e-3)\n",
    "        grads_and_vars = optimizer.compute_gradients(cnn.loss)\n",
    "        train_op = optimizer.apply_gradients(grads_and_vars,global_step)\n",
    "\n",
    "        #==保存模型相关==\n",
    "        #======\n",
    "        #保存模型\n",
    "        saver = tf.train.Saver(tf.global_variables(),max_to_keep = FLAGS.num_checkpoints)\n",
    "        #tf.global_variables() 报 No variables to save\n",
    "\n",
    "\n",
    "        #run变量初始化\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        #定义train_step函数\n",
    "        def train_step(x_batch,y_batch):\n",
    "            feed_dict = {\n",
    "                cnn.input_x : x_batch,\n",
    "                cnn.input_y : y_batch,\n",
    "                cnn.dropout_keep_prob : FLAGS.dropout\n",
    "            }\n",
    "\n",
    "            _,step,loss,accuracy = sess.run(\n",
    "                [train_op,global_step,cnn.loss,cnn.accuracy],\n",
    "                feed_dict\n",
    "            )\n",
    "            #summaries,\n",
    "            #train_summary_op,未定义，先注释\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            print(\"{}:step {},loss {:g}, acc {:g}\".format(time_str,step,loss,accuracy))\n",
    "\n",
    "\n",
    "        #定义dev_step函数 测试验证集(上面粘下来dropout改为1.0即可)\n",
    "        def dev_step(x_batch,y_batch):\n",
    "            feed_dict = {\n",
    "                cnn.input_x : x_batch,\n",
    "                cnn.input_y : y_batch,\n",
    "                cnn.dropout_keep_prob : 1.0\n",
    "            }\n",
    "\n",
    "            _,step,loss,accuracy = sess.run(\n",
    "                [train_op,global_step,cnn.loss,cnn.accuracy],\n",
    "                feed_dict\n",
    "            )\n",
    "            #summaries,\n",
    "            #train_summary_op,\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            print(\"{}:step {},loss {:g}, acc {:g}\".format(time_str,step,loss,accuracy))\n",
    "        \n",
    "        #执行迭代函数batch_iter\n",
    "        batches = batch_iter(list(zip(X_train,y_train)),FLAGS.batch_size,FLAGS.num_epochs)#list(zip(x,y))\n",
    "        #【【注意：本步非常重要，要zip打包，整体组成list，变成可迭代的训练集测试集，\n",
    "        #才能用next_batch（或用自己定义的其它分step迭代函数）向下迭代执行\n",
    "        #自己做的数据也要用这种方式zip起来再feed给tf才能用于迭代训练,\n",
    "        #本例数据路径参数已定义在tf DEFINE中，若需单独feed可考虑zip组装数据，\n",
    "        #或用tf DEFINE定义数据路径\n",
    "        #whale4_day6及其它分类数据衔接那里可参考本段总结】】    \n",
    "\n",
    "        #遍历batches\n",
    "        for batch in batches:\n",
    "            x_batch,y_batch = zip(*batch)\n",
    "            train_step(x_batch,y_batch)\n",
    "            current_step = tf.train.global_step(sess,global_step)#按步sess（训练）\n",
    "            #global_step 和next_batch还有计数器n = n+1 差不多，走步用的，在训练中是计数的作用，每训练一个batch就加1\n",
    "            if current_step % FLAGS.evaluate_every == 0:#按之前设置的间隔条件验证测试\n",
    "                print(\"\\n evaluate_every验证测试\")\n",
    "                dev_step(X_test,y_test)\n",
    "            if current_step % FLAGS.checkpoint_every == 0:#按条件保存结果\n",
    "                path = saver.save(sess,'./textCnnModel',global_step = current_step)\n",
    "                print('model模型已保存')\n",
    "\n",
    "\n",
    "#注意：保存模型和下面的迭代都要放到两层width\n",
    "#with tf.Graph().as_default():\n",
    "#\twith sess.as_default():\n",
    "#\t\t里面\n",
    "#两个def函数 train_step dev_step定义在使用该函数的上面"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#===看训练结果可知==\n",
    "'''\n",
    "#step 101,loss 0.760483, acc 0.579737\n",
    "#step 2401,loss 0.258065, acc 0.904315\n",
    "#step 4701,loss 0.0492993, acc 0.983114\n",
    "#step 5401,loss 0.030173, acc 0.990619\n",
    "#文本分类模型可用\n",
    "'''\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "卷积运算的定义、动机（稀疏权重、参数共享、等变表示）。一维卷积运算和二维卷积运算。\n",
    "\n",
    "卷积运算: 与经典的全连接神经网络相比，卷积神经网络运算的原理是通过加入卷积层和池化层，\n",
    "来对特征进行逐步细化浓缩提取。\n",
    "实现方式是通过滑动窗口的方式，在原始数据中划分出一个小区域矩阵，\n",
    "在卷积层conv层与通过窗口filter取前一层特征图上的小区域与下一层神经元做内积和操作(即wx+b)，\n",
    "经激活函数将线性转为非线性结果，\n",
    "将结果，即提取的特征点汇到一个特征图，\n",
    "再通过池化层pooling层 根据求max或mean操作，保留一个特征数，将特征图浓缩，\n",
    "再经过下一层卷积，池化，逐步浓缩，细化特征，\n",
    "最后经过两个全连接层整合特征，再softmax把特征分类。\n",
    "filter在取前一层数据小窗口时，是滑动向前依次提取，\n",
    "由stride指定每次滑动多远。\n",
    "\n",
    "稀疏权重：又叫稀疏连接，就是指上面说到的，每次只关注和处理视觉范围前后的影象。\n",
    "稀疏即散布离散，相对非连续性的一个概念。\n",
    "\n",
    "参数共享：因为每个特征图小窗口在与下一层神经元做内积和操作进行wx+b操作时都带有一个w,\n",
    "有多少个神经元就有多少个w，把相同的w共享避免大量操作，这个过程叫参数共享\n",
    "\n",
    "等变表示: 等比变化，可以通过局部特性按照规律，\n",
    "映射出整体或其它部分（如通过正脸特征映射画出侧脸）的特征，\n",
    "反之也可以从整体抽离局部特征浓缩代表整体，等变特性是上面卷积得以实现的依据。\n",
    "\n",
    "一维卷积运算：比如图象RGB通道，只有一个，灰度图，算上面卷积运算时只对一个平面，\n",
    "做filter过滤小窗口提取特征，再pooling压缩等操作。\n",
    "\n",
    "二维护卷积运算：chanel加入一个深度，比如图片识别的特征提取，一张彩色图，\n",
    "有三个颜色通道RGB，即，输入就是 长X宽X3,3就是深度。\n",
    "后面对应的filter也是三个分别提取三个通道的特征。\n",
    "\n",
    "反卷积(tf.nn.conv2d_transpose)：\n",
    "反卷积操作是卷积的反向，指上面conv pooling 一系列完成得到最终结果后，\n",
    "反向传播回来求梯度使loss操失最小，来取最合适的参数w,b的操作。\n",
    "\n",
    "tensorflow的conv2d_transpose函数可以实现反卷积，参数介绍如下：\n",
    "tf.nn.conv2d_transpose(\n",
    "value, #输入值，需要反卷积的输入，即从后一层反传回来的梯度值，要求tensor格式\n",
    "filter, #卷积核，tensor格式，四维[f高，f宽，f个数，chanel深度或图像通道数]\n",
    "output_shape, #输出的shape\n",
    "strides, #滑动尺度，每一维滑动的步长\n",
    "padding=\"SAME\", #边缘填充：滑动窗口到边缘，有空出位补0操作，为了提取完整特征。VALID是舍弃这部分特征。\n",
    "data_format=\"NHWC\", #输入参数的格式，默认NHWC顺序，即[batch, height, width, in_channels]\n",
    ")\n",
    "\n",
    "池化运算的定义、种类（最大池化、平均池化等）、动机。\n",
    "\n",
    "池化运算: 池化操作是在卷积后面跟着的一层将特征图filter提取的小区域按照规则，\n",
    "取最大Max或取平均Mean来 保留一个代表性矩阵值。\n",
    "动机：对特征进一步压缩细化。\n",
    "这一步和卷积层的实现原理一样，只是没有wx+b 内积和 操作。\n",
    "最大池化：Max，取前一层特征矩阵中最大值，如[[1,2][4,5]] 只取5放入特征图中。\n",
    "平均池化：mean,前一层特征矩阵中多个值取平均产，如[[1,2][4,5]] 取 （1+2+4+5）/4 = 6\n",
    "\n",
    "Text-CNN的原理:\n",
    "\n",
    "TextCNN 是利用卷积神经网络对文本进行分类的算法。\n",
    "卷积具有局部特征提取的功能, 所以可用 CNN 来提取句子中类似 n-gram （中心词前后的词组对）的关键信息。\n",
    "\n",
    "Text-CNN与CNN处理图象的对比：\n",
    "\n",
    "处理图像数据，CNN的filter卷积核宽高一致，自己指定\n",
    "但Text-CNN 因为是以词为最小颗粒滑动，所以filter滑动窗口的宽度（卷积核宽），\n",
    "不能自己指定（不能把一个词像图片一个拆碎了就没有意义了），\n",
    "所以Text-CNN filter宽度要与词向量的维度一致，之前输入的每一行向量代表一个词，\n",
    "高度可以自行指定，filter滑动卷积时提取特征时考虑了词义同时考虑了词序及其上下文,\n",
    "相当于取n-gram操作（类似CBOW 由上下文算中心词概率,skip-gram 由中心词算上下文思想），\n",
    "可以使用多个filter，设置不同的高度，提取多重特征 （比如使用多组filter等价于 uni-gram bi-gram tri-gram操作）。\n",
    "\n",
    "上面是卷积层，卷积后relu激活函数转线性为非线性数据，\n",
    "池化层Text-CNN是取一个Max，最重要的特征向量，\n",
    "\n",
    "再下一层conv,relu,pooling……\n",
    "\n",
    "最后在全连接层把每个特征值拼接起来(前面加一层dropout防过拟和)。\n",
    "\n",
    "再经过softmax分类。\n",
    "\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
