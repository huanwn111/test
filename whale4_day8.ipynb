{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 卷积神经网络 Task7\n",
    "\n",
    "* 卷积运算的定义、动机（稀疏权重、参数共享、等变表示）。一维卷积运算和二维卷积运算。\n",
    "* 反卷积(tf.nn.conv2d_transpose)\n",
    "* 池化运算的定义、种类（最大池化、平均池化等）、动机。\n",
    "* Text-CNN的原理。\n",
    "* 利用Text-CNN模型来进行文本分类。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "卷积运算的定义、动机（稀疏权重、参数共享、等变表示）。一维卷积运算和二维卷积运算。\n",
    "\n",
    "卷积运算: 与经典的全连接神经网络相比，卷积神经网络运算的原理是通过加入卷积层和池化层，\n",
    "来对特征进行逐步细化浓缩提取。\n",
    "实现方式是通过滑动窗口的方式，在原始数据中划分出一个小区域矩阵，\n",
    "在卷积层conv层与通过窗口filter取前一层特征图上的小区域与下一层神经元做内积和操作(即wx+b)，\n",
    "经激活函数将线性转为非线性结果，\n",
    "将结果，即提取的特征点汇到一个特征图，\n",
    "再通过池化层pooling层 根据求max或mean操作，保留一个特征数，将特征图浓缩，\n",
    "再经过下一层卷积，池化，逐步浓缩，细化特征，\n",
    "最后经过两个全连接层整合特征，再softmax把特征分类。\n",
    "filter在取前一层数据小窗口时，是滑动向前依次提取，\n",
    "由stride指定每次滑动多远。\n",
    "\n",
    "稀疏权重：又叫稀疏连接，就是指上面说到的，每次只关注和处理视觉范围前后的影象。\n",
    "稀疏即散布离散，相对非连续性的一个概念。\n",
    "\n",
    "参数共享：因为每个特征图小窗口在与下一层神经元做内积和操作进行wx+b操作时都带有一个w,\n",
    "有多少个神经元就有多少个w，把相同的w共享避免大量操作，这个过程叫参数共享\n",
    "\n",
    "等变表示: 等比变化，可以通过局部特性按照规律，\n",
    "映射出整体或其它部分（如通过正脸特征映射画出侧脸）的特征，\n",
    "反之也可以从整体抽离局部特征浓缩代表整体，等变特性是上面卷积得以实现的依据。\n",
    "\n",
    "一维卷积运算：比如图象RGB通道，只有一个，灰度图，算上面卷积运算时只对一个平面，\n",
    "做filter过滤小窗口提取特征，再pooling压缩等操作。\n",
    "\n",
    "二维护卷积运算：chanel加入一个深度，比如图片识别的特征提取，一张彩色图，\n",
    "有三个颜色通道RGB，即，输入就是 长X宽X3,3就是深度。\n",
    "后面对应的filter也是三个分别提取三个通道的特征。\n",
    "\n",
    "反卷积(tf.nn.conv2d_transpose)：\n",
    "反卷积操作是卷积的反向，指上面conv pooling 一系列完成得到最终结果后，\n",
    "反向传播回来求梯度使loss操失最小，来取最合适的参数w,b的操作。\n",
    "\n",
    "tensorflow的conv2d_transpose函数可以实现反卷积，参数介绍如下：\n",
    "tf.nn.conv2d_transpose(\n",
    "value, #输入值，需要反卷积的输入，即从后一层反传回来的梯度值，要求tensor格式\n",
    "filter, #卷积核，tensor格式，四维[f高，f宽，f个数，chanel深度或图像通道数]\n",
    "output_shape, #输出的shape\n",
    "strides, #滑动尺度，每一维滑动的步长\n",
    "padding=\"SAME\", #边缘填充：滑动窗口到边缘，有空出位补0操作，为了提取完整特征。VALID是舍弃这部分特征。\n",
    "data_format=\"NHWC\", #输入参数的格式，默认NHWC顺序，即[batch, height, width, in_channels]\n",
    ")\n",
    "\n",
    "池化运算的定义、种类（最大池化、平均池化等）、动机。\n",
    "\n",
    "池化运算: 池化操作是在卷积后面跟着的一层将特征图filter提取的小区域按照规则，\n",
    "取最大Max或取平均Mean来 保留一个代表性矩阵值。\n",
    "动机：对特征进一步压缩细化。\n",
    "这一步和卷积层的实现原理一样，只是没有wx+b 内积和 操作。\n",
    "最大池化：Max，取前一层特征矩阵中最大值，如[[1,2][4,5]] 只取5放入特征图中。\n",
    "平均池化：mean,前一层特征矩阵中多个值取平均产，如[[1,2][4,5]] 取 （1+2+4+5）/4 = 6\n",
    "\n",
    "Text-CNN的原理:\n",
    "\n",
    "TextCNN 是利用卷积神经网络对文本进行分类的算法。\n",
    "卷积具有局部特征提取的功能, 所以可用 CNN 来提取句子中类似 n-gram （中心词前后的词组对）的关键信息。\n",
    "\n",
    "Text-CNN与CNN处理图象的对比：\n",
    "\n",
    "处理图像数据，CNN的filter卷积核宽高一致，自己指定\n",
    "但Text-CNN 因为是以词为最小颗粒滑动，所以filter滑动窗口的宽度（卷积核宽），\n",
    "不能自己指定（不能把一个词像图片一个拆碎了就没有意义了），\n",
    "所以Text-CNN filter宽度要与词向量的维度一致，之前输入的每一行向量代表一个词，\n",
    "高度可以自行指定，filter滑动卷积时提取特征时考虑了词义同时考虑了词序及其上下文,\n",
    "相当于取n-gram操作（类似CBOW 由上下文算中心词概率,skip-gram 由中心词算上下文思想），\n",
    "可以使用多个filter，设置不同的高度，提取多重特征 （比如使用多组filter等价于 uni-gram bi-gram tri-gram操作）。\n",
    "\n",
    "上面是卷积层，卷积后relu激活函数转线性为非线性数据，\n",
    "池化层Text-CNN是取一个Max，最重要的特征向量，\n",
    "\n",
    "再下一层conv,relu,pooling……\n",
    "\n",
    "最后在全连接层把每个特征值拼接起来(前面加一层dropout防过拟和)。\n",
    "\n",
    "再经过softmax分类。\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "利用Text-CNN模型来进行文本分类。\n",
    "cnn手写字体识别\n",
    "'''\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "#初始化tensorflow 读取数据类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-4-901332a25912>:2: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From G:\\Users\\Administrator\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From G:\\Users\\Administrator\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MinistData\\train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From G:\\Users\\Administrator\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MinistData\\train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From G:\\Users\\Administrator\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting MinistData\\t10k-images-idx3-ubyte.gz\n",
      "Extracting MinistData\\t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From G:\\Users\\Administrator\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "#读取minist数据集\n",
    "mnist = input_data.read_data_sets('MinistData',one_hot=True)\n",
    "#数据已下载，存在同目录MinistData文件夹下\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集数据个数： 55000\n",
      "验证集数据个数： 5000\n",
      "测试集数据个数： 10000\n"
     ]
    }
   ],
   "source": [
    "print(\"训练集数据个数：\",mnist.train.num_examples)\n",
    "print(\"验证集数据个数：\",mnist.validation.num_examples)\n",
    "print(\"测试集数据个数：\",mnist.test.num_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAADZxJREFUeJzt3X+o1fUdx/HXe6YUFf1g6SSdN+2Xqz9c3WJRDNcyagQ2aNaFlquxu8Igw2AiQf7RIIZmg6C40WUG022xftxibGoEJq6lhnjbbCvCplOumqVXikJ974/7NW52v59zPOf7Pd9z7/v5ALnnfN/fH28Ovu73e+73x8fcXQDi+UbVDQCoBuEHgiL8QFCEHwiK8ANBEX4gKMIPBEX4gaAIPxDUKa3cmJlxOSFQMne3euZras9vZjeZ2b/N7H0zW9zMugC0ljV6bb+ZjZP0H0lzJO2StElSl7v/K7EMe36gZK3Y818t6X13/8Ddv5D0B0lzm1gfgBZqJvznS9o57P2ubNpXmFm3mW02s81NbAtAwZr5g99IhxZfO6x39x5JPRKH/UA7aWbPv0vS1GHvp0ja3Vw7AFqlmfBvknSRmV1gZhMk3SGpr5i2AJSt4cN+dz9iZvdL+pukcZJ63f2fhXUGoFQNn+praGN85wdK15KLfACMXoQfCIrwA0ERfiAowg8ERfiBoAg/EBThB4Ii/EBQhB8IivADQRF+ICjCDwRF+IGgCD8QFOEHgiL8QFCEHwiK8ANBEX4gKMIPBEX4gaAIPxAU4QeCIvxAUIQfCIrwA0ERfiAowg8E1fAQ3ZJkZjskDUo6KumIu3cW0RSA8jUV/swP3H1/AesB0EIc9gNBNRt+l7TGzLaYWXcRDQFojWYP+691991mNlHSWjN7193XD58h+6XALwagzZi7F7Mis6WSDrv7ssQ8xWwMQC53t3rma/iw38xON7Mzj7+WdKOkdxpdH4DWauawf5KkF83s+HpWuftfC+kKQOkKO+yva2Mc9gOlK/2wH8DoRviBoAg/EBThB4Ii/EBQhB8Iqoi7+lCxu+++O7dW61TuRx99lKzPnDkzWd+4cWOyvmHDhmQd1WHPDwRF+IGgCD8QFOEHgiL8QFCEHwiK8ANBjZnz/F1dXcn6FVdckaynzpW3u7PPPrvhZY8ePZqsT5gwIVn/7LPPkvVPP/00t9bf359cdt68ecn6vn37knWksecHgiL8QFCEHwiK8ANBEX4gKMIPBEX4gaBG1aO7ly9fnlt74IEHksuOGzeumU2jAq+//nqyXuvajoGBgSLbGTV4dDeAJMIPBEX4gaAIPxAU4QeCIvxAUIQfCKrmeX4z65V0i6S97n55Nu1cSX+U1CFph6R57v5xzY01eZ5/586dubUpU6Ykl922bVuyXuu+9DLVerb9Sy+91KJOTt6cOXOS9bvuuiu31tHR0dS2a10HcPvtt+fWxvKzAIo8z/87STedMG2xpNfc/SJJr2XvAYwiNcPv7uslHThh8lxJK7PXKyXdWnBfAErW6Hf+Se6+R5KynxOLawlAK5T+DD8z65bUXfZ2AJycRvf8A2Y2WZKyn3vzZnT3HnfvdPfOBrcFoASNhr9P0vzs9XxJLxfTDoBWqRl+M1st6e+SLjGzXWb2c0mPSZpjZu9JmpO9BzCKjKr7+S+++OLc2mWXXZZcdt26dcn64OBgQz0hbfr06bm1V199NbnszJkzm9r2Qw89lFtLPRtitON+fgBJhB8IivADQRF+ICjCDwRF+IGgRtWpPowtt912W7L+/PPPN7X+/fv359bOO++8ptbdzjjVByCJ8ANBEX4gKMIPBEX4gaAIPxAU4QeCIvxAUIQfCIrwA0ERfiAowg8ERfiBoAg/EBThB4IqfbguxHbffffl1q666qpSt33qqafm1q688srkslu2bCm6nbbDnh8IivADQRF+ICjCDwRF+IGgCD8QFOEHgqr53H4z65V0i6S97n55Nm2ppF9I2pfNtsTd/1JzYzy3vxSTJ0/Ord15553JZRcuXFh0O1+R6s2srsfLl+LQoUPJ+llnndWiTopX5HP7fyfpphGmr3D3Wdm/msEH0F5qht/d10s60IJeALRQM9/57zezbWbWa2bnFNYRgJZoNPxPSZohaZakPZKW581oZt1mttnMNje4LQAlaCj87j7g7kfd/ZikZyRdnZi3x9073b2z0SYBFK+h8JvZ8D/h/ljSO8W0A6BVat7Sa2arJc2W9E0z2yXpEUmzzWyWJJe0Q9IvS+wRQAlqht/du0aY/GwJvYR1ww03JOu17j3v7u7OrU2fPr2hnsa63t7eqluoHFf4AUERfiAowg8ERfiBoAg/EBThB4Li0d0FuPDCC5P1p59+Olm//vrrk/Uyb3398MMPk/WPP/64qfU//PDDubXPP/88ueyTTz6ZrF9yySUN9SRJu3fvbnjZsYI9PxAU4QeCIvxAUIQfCIrwA0ERfiAowg8ExXn+Oj344IO5tQULFiSXnTFjRrJ++PDhZP2TTz5J1p944oncWq3z2Rs3bkzWa10HUKaDBw82tfzg4GBu7ZVXXmlq3WMBe34gKMIPBEX4gaAIPxAU4QeCIvxAUIQfCIrz/HW65pprcmu1zuP39fUl68uX5452Jklav359sj5azZo1K1mfNm1aU+tPPS/g3XffbWrdYwF7fiAowg8ERfiBoAg/EBThB4Ii/EBQhB8IquZ5fjObKuk5Sd+SdExSj7v/1szOlfRHSR2Sdkia5+7NPeS9jd177725tW3btiWXffTRR4tuZ0yoNd7BpEmTmlr/unXrmlp+rKtnz39E0iJ3nynpe5IWmNl3JC2W9Jq7XyTptew9gFGiZvjdfY+7v529HpS0XdL5kuZKWpnNtlLSrWU1CaB4J/Wd38w6JH1X0j8kTXL3PdLQLwhJE4tuDkB56r6238zOkPRnSQvd/VC948eZWbek7sbaA1CWuvb8ZjZeQ8H/vbu/kE0eMLPJWX2ypL0jLevuPe7e6e6dRTQMoBg1w29Du/hnJW1398eHlfokzc9ez5f0cvHtASiLuXt6BrPrJL0hqV9Dp/okaYmGvvf/SdK3Jf1X0k/c/UCNdaU3hlCWLVuWrC9atChZr/VI85tvvjm39uabbyaXHc3cva7v5DW/87v7Bkl5K/vhyTQFoH1whR8QFOEHgiL8QFCEHwiK8ANBEX4gKB7djVL19/fn1i699NKm1r1mzZpkfSyfyy8Ce34gKMIPBEX4gaAIPxAU4QeCIvxAUIQfCIrz/ChVR0dHbu2UU9L//Q4ePJisr1ixopGWkGHPDwRF+IGgCD8QFOEHgiL8QFCEHwiK8ANBcZ4fTenq6krWTzvttNza4OBgctnu7vQob9yv3xz2/EBQhB8IivADQRF+ICjCDwRF+IGgCD8QlLl7egazqZKek/QtScck9bj7b81sqaRfSNqXzbrE3f9SY13pjaHtjB8/Pll/6623kvXUs/lXr16dXPaee+5J1jEyd7d65qvnIp8jkha5+9tmdqakLWa2NqutcPdljTYJoDo1w+/ueyTtyV4Pmtl2SeeX3RiAcp3Ud34z65D0XUn/yCbdb2bbzKzXzM7JWabbzDab2eamOgVQqLrDb2ZnSPqzpIXufkjSU5JmSJqloSOD5SMt5+497t7p7p0F9AugIHWF38zGayj4v3f3FyTJ3Qfc/ai7H5P0jKSry2sTQNFqht/MTNKzkra7++PDpk8eNtuPJb1TfHsAylLPX/uvlfRTSf1mtjWbtkRSl5nNkuSSdkj6ZSkdolK1TgWvWrUqWd+6dWtube3atbk1lK+ev/ZvkDTSecPkOX0A7Y0r/ICgCD8QFOEHgiL8QFCEHwiK8ANB1bylt9CNcUsvULp6b+llzw8ERfiBoAg/EBThB4Ii/EBQhB8IivADQbV6iO79kj4c9v6b2bR21K69tWtfEr01qsjeptU7Y0sv8vnaxs02t+uz/dq1t3btS6K3RlXVG4f9QFCEHwiq6vD3VLz9lHbtrV37kuitUZX0Vul3fgDVqXrPD6AilYTfzG4ys3+b2ftmtriKHvKY2Q4z6zezrVUPMZYNg7bXzN4ZNu1cM1trZu9lP0ccJq2i3paa2f+yz26rmf2oot6mmtnrZrbdzP5pZg9k0yv97BJ9VfK5tfyw38zGSfqPpDmSdknaJKnL3f/V0kZymNkOSZ3uXvk5YTP7vqTDkp5z98uzab+RdMDdH8t+cZ7j7r9qk96WSjpc9cjN2YAyk4ePLC3pVkk/U4WfXaKveargc6tiz3+1pPfd/QN3/0LSHyTNraCPtufu6yUdOGHyXEkrs9crNfSfp+VyemsL7r7H3d/OXg9KOj6ydKWfXaKvSlQR/vMl7Rz2fpfaa8hvl7TGzLaYWXfVzYxgUjZs+vHh0ydW3M+Jao7c3EonjCzdNp9dIyNeF62K8I/0iKF2OuVwrbtfIelmSQuyw1vUp66Rm1tlhJGl20KjI14XrYrw75I0ddj7KZJ2V9DHiNx9d/Zzr6QX1X6jDw8cHyQ1+7m34n6+1E4jN480srTa4LNrpxGvqwj/JkkXmdkFZjZB0h2S+iro42vM7PTsDzEys9Ml3aj2G324T9L87PV8SS9X2MtXtMvIzXkjS6viz67dRryu5CKf7FTGE5LGSep191+3vIkRmNl0De3tpaE7HldV2ZuZrZY0W0N3fQ1IekTSS5L+JOnbkv4r6Sfu3vI/vOX0NltDh65fjtx8/Dt2i3u7TtIbkvolHcsmL9HQ9+vKPrtEX12q4HPjCj8gKK7wA4Ii/EBQhB8IivADQRF+ICjCDwRF+IGgCD8Q1P8Bp+YC7BbcNBcAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAADhNJREFUeJzt3V2MVPUZx/HfU9Eb9EJZBKKwWGOw1Qslq2kiEo0BoTEBLjS+xNC0ssZoUrQXxZeoCYKmKRa4QddIxER8CbCVGKwa0yBNGsKbUWRBjaFAISyIiRovjO7Tiz00K+75n2HmzJxZnu8nMTszz5yZp9P9cWb2mXP+5u4CEM8vqm4AQDUIPxAU4QeCIvxAUIQfCIrwA0ERfiAowg8ERfiBoEa18snMjK8TAk3m7lbL/Rra85vZLDPbZ2afm9miRh4LQGtZvd/tN7OzJH0qaYakQ5K2SbrD3fcktmHPDzRZK/b810r63N2/cPfvJb0maU4DjweghRoJ/0WSDg65fii77SfMrNvMtpvZ9gaeC0DJGvmD33BvLX72tt7deyT1SLztB9pJI3v+Q5ImDrl+saTDjbUDoFUaCf82SZeZ2SVmdo6k2yVtLKctAM1W99t+d//BzB6Q9I6ksyStdvdPSusMQFPVPeqr68n4zA80XUu+5ANg5CL8QFCEHwiK8ANBEX4gKMIPBEX4gaAIPxAU4QeCIvxAUIQfCIrwA0ERfiAowg8ERfiBoAg/EBThB4Ii/EBQhB8IivADQRF+ICjCDwRF+IGgCD8QFOEHgiL8QFCEHwiK8ANBEX4gqLqX6JYkM9sv6RtJP0r6wd27ymgKrdPZ2Zms33PPPcn6o48+mqynVoE2Sy8m29fXl6w/9thjyXpvb2+yHl1D4c/c6O7HS3gcAC3E234gqEbD75LeNbMdZtZdRkMAWqPRt/3XufthM7tQ0ntmttfdPxh6h+wfBf5hANpMQ3t+dz+c/eyX1Cvp2mHu0+PuXfwxEGgvdYffzEab2XknL0uaKWl3WY0BaK5G3vaPk9SbjWtGSVrr7v8opSsATWepOWzpT2bWuicLZOzYsbm1hx9+OLntXXfdlayPGTMmWS+a1Tcy5y/63Tx48GCyfs011+TWjh8/c6fT7p5+YTOM+oCgCD8QFOEHgiL8QFCEHwiK8ANBMeobAYoOm128eHFurej/32aP244dO5asp3R0dCTrkydPTtb37NmTW7viiivqaWlEYNQHIInwA0ERfiAowg8ERfiBoAg/EBThB4Jizj8CbNu2LVmfOnVqbq3ROX9qVi5JN954Y7LeyKGz06ZNS9Y3b96crKf+t48aVcaJq9sTc34ASYQfCIrwA0ERfiAowg8ERfiBoAg/EBRz/jZw+eWXJ+tFc/4vv/wyt1Z0PH3RHP7BBx9M1hcuXJisL126NLd24MCB5LZFin53BwYGcmv33Xdfctuenp66emoHzPkBJBF+ICjCDwRF+IGgCD8QFOEHgiL8QFCFc34zWy3pFkn97n5ldtsFkl6XNFnSfkm3uftXhU/GnL8uRd8DSM3qG12Kuru7O1lftWpVsp5aJnvnzp3JbefNm5esr1u3LllP/W6PHz8+ue1IXsK7zDn/S5JmnXLbIknvu/tlkt7PrgMYQQrD7+4fSDpxys1zJK3JLq+RNLfkvgA0Wb2f+ce5+xFJyn5eWF5LAFqh6ScyM7NuSekPjgBart49/1EzmyBJ2c/+vDu6e4+7d7l7V53PBaAJ6g3/Rknzs8vzJb1ZTjsAWqUw/Gb2qqR/S5piZofM7A+SnpE0w8w+kzQjuw5gBCn8zO/ud+SUbiq5F+TYu3dvZc9ddD6Affv2Jeupcw0UnStg0aL0BLlozYFmfv/hTMA3/ICgCD8QFOEHgiL8QFCEHwiK8ANBnbnrFAcyffr03FrR4cBFo7y+vr5kfcqUKcn61q1bc2tjx45Nblt0uHlR77Nnz07Wo2PPDwRF+IGgCD8QFOEHgiL8QFCEHwiK8ANBMec/A9x55525tQULFiS3LTostoZTuyfrqVl+I4fkStLKlSuT9aJTg0fHnh8IivADQRF+ICjCDwRF+IGgCD8QFOEHgmLOf4YrmtNXuf2WLVuS2z700EPJOnP8xrDnB4Ii/EBQhB8IivADQRF+ICjCDwRF+IGgCuf8ZrZa0i2S+t39yuy2JyUtkHTyxOmPuPumZjWJtLVr1+bWOjs7k9t2dHQk60Xn/R89enSynvL4448n68zxm6uWPf9LkmYNc/vf3P2q7D+CD4wwheF39w8knWhBLwBaqJHP/A+Y2UdmttrMzi+tIwAtUW/4V0m6VNJVko5IWpZ3RzPrNrPtZra9zucC0AR1hd/dj7r7j+4+IOkFSdcm7tvj7l3u3lVvkwDKV1f4zWzCkKvzJO0upx0ArVLLqO9VSTdI6jCzQ5KekHSDmV0lySXtl3RvE3sE0ATW6PHap/VkZq17MpSiaM7/1FNPJetz587Nre3atSu57ezZs5P1ovP6R+Xu6QURMnzDDwiK8ANBEX4gKMIPBEX4gaAIPxAUo74apZaaPnbsWG4turfffju3dvPNNye3LTp19/Lly+vq6UzHqA9AEuEHgiL8QFCEHwiK8ANBEX4gKMIPBMUS3Znp06cn68uW5Z6pTHv37k1ue/fdd9fV05lgyZIlubWZM2cmt50yZUrZ7WAI9vxAUIQfCIrwA0ERfiAowg8ERfiBoAg/EFSYOX/qeHxJeu6555L1/v7+3FrkOX7REt3PP/98bs2spsPO0STs+YGgCD8QFOEHgiL8QFCEHwiK8ANBEX4gqMI5v5lNlPSypPGSBiT1uPsKM7tA0uuSJkvaL+k2d/+qea02Zt68ecl60bHjmzdvLrOdEaNoie7169cn66nXtWjNiKLzJKAxtez5f5D0J3f/laTfSLrfzH4taZGk9939MknvZ9cBjBCF4Xf3I+6+M7v8jaQ+SRdJmiNpTXa3NZLmNqtJAOU7rc/8ZjZZ0tWStkoa5+5HpMF/ICRdWHZzAJqn5u/2m9m5ktZLWujuX9f6vWwz65bUXV97AJqlpj2/mZ2tweC/4u4bspuPmtmErD5B0rBHvrh7j7t3uXtXGQ0DKEdh+G1wF/+ipD53f3ZIaaOk+dnl+ZLeLL89AM1SuES3mU2TtEXSxxoc9UnSIxr83P+GpEmSDki61d1PFDxWZUt0F42s+vr6kvU9e/bk1p5++umGHnvHjh3JepHOzs7c2vXXX5/ctmgEOndu+u+4RR//Ur9fK1asSG5btEQ3hlfrEt2Fn/nd/V+S8h7sptNpCkD74Bt+QFCEHwiK8ANBEX4gKMIPBEX4gaAK5/ylPlmFc/4i69atS9ZT8+5GZt2StGvXrmS9yKRJk3JrY8aMSW7baO9F26eW6F65cmVy2+PHjyfrGF6tc372/EBQhB8IivADQRF+ICjCDwRF+IGgCD8QFHP+TNES3ps2bcqtdXWlT1I0MDCQrDdz1l607XfffZesF50+e+nSpcl6b29vso7yMecHkET4gaAIPxAU4QeCIvxAUIQfCIrwA0Ex569RR0dHbm3x4sUNPXZ3d3o1sw0bNiTrjRz3XnTufJbJHnmY8wNIIvxAUIQfCIrwA0ERfiAowg8ERfiBoArn/GY2UdLLksZLGpDU4+4rzOxJSQskHcvu+oi75x/0rpE95wdGilrn/LWEf4KkCe6+08zOk7RD0lxJt0n61t3/WmtThB9ovlrDP6qGBzoi6Uh2+Rsz65N0UWPtAajaaX3mN7PJkq6WtDW76QEz+8jMVpvZ+TnbdJvZdjPb3lCnAEpV83f7zexcSZslLXH3DWY2TtJxSS5psQY/Gvy+4DF42w80WWmf+SXJzM6W9Jakd9z92WHqkyW95e5XFjwO4QearLQDe2zw1LAvSuobGvzsD4EnzZO0+3SbBFCdWv7aP03SFkkfa3DUJ0mPSLpD0lUafNu/X9K92R8HU4/Fnh9oslLf9peF8APNx/H8AJIIPxAU4QeCIvxAUIQfCIrwA0ERfiAowg8ERfiBoAg/EBThB4Ii/EBQhB8IivADQRWewLNkxyX9Z8j1juy2dtSuvbVrXxK91avM3jprvWNLj+f/2ZObbXf3rsoaSGjX3tq1L4ne6lVVb7ztB4Ii/EBQVYe/p+LnT2nX3tq1L4ne6lVJb5V+5gdQnar3/AAqUkn4zWyWme0zs8/NbFEVPeQxs/1m9rGZfVj1EmPZMmj9ZrZ7yG0XmNl7ZvZZ9nPYZdIq6u1JM/tv9tp9aGa/rai3iWb2TzPrM7NPzOyP2e2VvnaJvip53Vr+tt/MzpL0qaQZkg5J2ibpDnff09JGcpjZfkld7l75TNjMpkv6VtLLJ1dDMrO/SDrh7s9k/3Ce7+5/bpPentRprtzcpN7yVpb+nSp87cpc8boMVez5r5X0ubt/4e7fS3pN0pwK+mh77v6BpBOn3DxH0prs8hoN/vK0XE5vbcHdj7j7zuzyN5JOrixd6WuX6KsSVYT/IkkHh1w/pPZa8tslvWtmO8ysu+pmhjHu5MpI2c8LK+7nVIUrN7fSKStLt81rV8+K12WrIvzDrSbSTiOH69x9qqTZku7P3t6iNqskXarBZdyOSFpWZTPZytLrJS1096+r7GWoYfqq5HWrIvyHJE0ccv1iSYcr6GNY7n44+9kvqVeDH1PaydGTi6RmP/sr7uf/3P2ou//o7gOSXlCFr122svR6Sa+4+4bs5spfu+H6qup1qyL82yRdZmaXmNk5km6XtLGCPn7GzEZnf4iRmY2WNFPtt/rwRknzs8vzJb1ZYS8/0S4rN+etLK2KX7t2W/G6ki/5ZKOM5ZLOkrTa3Ze0vIlhmNkvNbi3lwaPeFxbZW9m9qqkGzR41NdRSU9I+rukNyRNknRA0q3u3vI/vOX0doNOc+XmJvWWt7L0VlX42pW54nUp/fANPyAmvuEHBEX4gaAIPxAU4QeCIvxAUIQfCIrwA0ERfiCo/wGTnJDl40xJsQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAADTlJREFUeJzt3W+IXfWdx/HPx9gEsVUTgmlIk7UbdNkqYtdhWEiJkWpxl2Lsg0qDD6IsjQ+qtBJko6gN6koQ26YBKSQkNEJrWmyjeSC2Ia7YlSUYJUTT2EbKbDObMGlNpUaQZDLffTAnyzTOPffm3nPuueP3/YIw957v+fPlTj7zO3fOufNzRAhAPhc03QCAZhB+ICnCDyRF+IGkCD+QFOEHkiL8QFKEH0iK8ANJXdjPg9nmdkKgZhHhTtbraeS3fYvt39l+1/a6XvYFoL/c7b39tmdJ+r2kmyWNSnpd0qqI+G3JNoz8QM36MfIPS3o3Iv4QEack7ZC0sof9AeijXsK/SNKRKc9Hi2V/w/Ya2/ts7+vhWAAq1ssv/KY7tfjYaX1EbJa0WeK0HxgkvYz8o5IWT3n+OUlHe2sHQL/0Ev7XJV1p+/O2Z0v6hqRd1bQFoG5dn/ZHxLjteyT9StIsSdsi4mBlnQGoVdeX+ro6GO/5gdr15SYfADMX4QeSIvxAUoQfSIrwA0kRfiApwg8kRfiBpAg/kBThB5Ii/EBShB9IivADSRF+ICnCDyRF+IGkCD+QFOEHkiL8QFKEH0iK8ANJEX4gKcIPJEX4gaQIP5AU4QeSIvxAUoQfSIrwA0l1PUW3JNkekfSBpDOSxiNiqIqmANSvp/AXboyIP1ewHwB9xGk/kFSv4Q9Jv7b9hu01VTQEoD96Pe1fFhFHbV8uabftdyLi1akrFD8U+MEADBhHRDU7stdLOhkRT5WsU83BALQUEe5kva5P+21fbPszZx9L+oqkt7vdH4D+6uW0f4GknbbP7uenEfFSJV0BqF1lp/0dHYzT/q7Mnj27tL5nz56WtWXLlpVuW/zwbun9998vrV977bWl9SNHjpTWUb3aT/sBzGyEH0iK8ANJEX4gKcIPJEX4gaSq+FQfetTuUt7WrVtL6+0u55V5/vnnS+sbNmworR89erTrY9dtwYIFLWtjY2N97GQwMfIDSRF+ICnCDyRF+IGkCD+QFOEHkiL8QFJc5x8Aa9euLa3fcccdXe/76aefLq3ff//9pfWPPvqo62PX7amnWv7RKEnSXXfd1bL22GOPlW67cePGrnqaSRj5gaQIP5AU4QeSIvxAUoQfSIrwA0kRfiAprvP3wdVXX11af+ihh3ra/8mTJ1vW7rvvvtJtx8fHezp2nYaGymd8v/POO0vrc+fOrbCbTx5GfiApwg8kRfiBpAg/kBThB5Ii/EBShB9Iqu11ftvbJH1V0vGIuKZYNk/SzyRdIWlE0u0R8Zf62pzZ1q1bV1q/6KKLSuvtrsXfeuutXW87yNr9rYF58+aV1k+fPt2y1m6+ggw6Gfl/LOmWc5atk7QnIq6UtKd4DmAGaRv+iHhV0olzFq+UtL14vF3SbRX3BaBm3b7nXxARxySp+Hp5dS0B6Ifa7+23vUbSmrqPA+D8dDvyj9leKEnF1+OtVoyIzRExFBHln9IA0Ffdhn+XpNXF49WSXqimHQD90jb8tp+V9N+S/sH2qO1/k7RB0s22D0u6uXgOYAZp+54/Ila1KH254l4+sa6//vqetn/ppZdK66+88krX+541a1Zpffbs2V3vu52lS5eW1m+44Yae9v/cc8+1rI2MjPS0708C7vADkiL8QFKEH0iK8ANJEX4gKcIPJMWf7p4B5syZ0/W2w8PDpfXHH3+8tH7TTTd1fey6jY2NldafeOKJPnUyMzHyA0kRfiApwg8kRfiBpAg/kBThB5Ii/EBSXOfvgyeffLK0vm3bttL6jTfeWFp/+eWXW9aWL19euu0FF8zcn/9btmwprR88eLBPncxMM/c7D6AnhB9IivADSRF+ICnCDyRF+IGkCD+QFNf5+2DJkiU9bX/hheXfphUrVnS9771795bWd+7cWVpftGhRaf3ee+897546tW/fvtr2nQEjP5AU4QeSIvxAUoQfSIrwA0kRfiApwg8k1fY6v+1tkr4q6XhEXFMsWy/pm5L+VKz2YES8WFeTM127z+ufOnWqtmPv2LGjtH7kyJHS+pkzZ0rrDzzwwHn31KnXXnuttP7ii/yX60UnI/+PJd0yzfIfRMR1xT++C8AM0zb8EfGqpBN96AVAH/Xynv8e2wdsb7M9t7KOAPRFt+H/kaSlkq6TdEzS91qtaHuN7X22uREbGCBdhT8ixiLiTERMSNoiqeVskBGxOSKGImKo2yYBVK+r8NteOOXp1yS9XU07APqlk0t9z0paIWm+7VFJ35W0wvZ1kkLSiKS7a+wRQA3ahj8iVk2zeGsNvXxijY6OltY3bNjQp06q9+GHH9a2702bNpXWx8fHazt2BtzhByRF+IGkCD+QFOEHkiL8QFKEH0iKP92NnrT7yG+ZiYmJ0vrhw4e73jfaY+QHkiL8QFKEH0iK8ANJEX4gKcIPJEX4gaS4zo+e3H1393/KYffu3aX1/fv3d71vtMfIDyRF+IGkCD+QFOEHkiL8QFKEH0iK8ANJcZ0fpS699NLS+iWXXNL1vjdu3Nj1tugdIz+QFOEHkiL8QFKEH0iK8ANJEX4gKcIPJNX2Or/txZKekfRZSROSNkfED23Pk/QzSVdIGpF0e0T8pb5W0YTh4eHS+pIlS0rrp0+fbll77733uuoJ1ehk5B+XtDYi/lHSP0v6lu0vSFonaU9EXClpT/EcwAzRNvwRcSwi3iwefyDpkKRFklZK2l6stl3SbXU1CaB65/We3/YVkr4oaa+kBRFxTJr8ASHp8qqbA1Cfju/tt/1pSb+Q9J2I+KvtTrdbI2lNd+0BqEtHI7/tT2ky+D+JiF8Wi8dsLyzqCyUdn27biNgcEUMRMVRFwwCq0Tb8nhzit0o6FBHfn1LaJWl18Xi1pBeqbw9AXRwR5SvYX5L0G0lvafJSnyQ9qMn3/T+XtETSHyV9PSJOtNlX+cEwcN55553S+lVXXVVaP3Gi9X+J+fPnd9UTykVER+/J277nj4j/ktRqZ18+n6YADA7u8AOSIvxAUoQfSIrwA0kRfiApwg8kxZ/uRqk5c+b0tP2BAwcq6gRVY+QHkiL8QFKEH0iK8ANJEX4gKcIPJEX4gaS4zo9anTlzpukW0AIjP5AU4QeSIvxAUoQfSIrwA0kRfiApwg8kxXV+1Gr58uUta4888kjpto8++mjV7WAKRn4gKcIPJEX4gaQIP5AU4QeSIvxAUoQfSKrtdX7biyU9I+mzkiYkbY6IH9peL+mbkv5UrPpgRLxYV6NoxqZNm0rrDz/8cGn9sssua1mbmJjoqidUo5ObfMYlrY2IN21/RtIbtncXtR9ExFP1tQegLm3DHxHHJB0rHn9g+5CkRXU3BqBe5/We3/YVkr4oaW+x6B7bB2xvsz23xTZrbO+zva+nTgFUquPw2/60pF9I+k5E/FXSjyQtlXSdJs8MvjfddhGxOSKGImKogn4BVKSj8Nv+lCaD/5OI+KUkRcRYRJyJiAlJWyQN19cmgKq1Db9tS9oq6VBEfH/K8oVTVvuapLerbw9AXRwR5SvYX5L0G0lvafJSnyQ9KGmVJk/5Q9KIpLuLXw6W7av8YAB6FhHuZL224a8S4Qfq12n4ucMPSIrwA0kRfiApwg8kRfiBpAg/kBThB5Ii/EBShB9IivADSRF+ICnCDyRF+IGkCD+QVL+n6P6zpP+Z8nx+sWwQDWpvg9qXRG/dqrK3v+t0xb5+nv9jB7f3Derf9hvU3ga1L4neutVUb5z2A0kRfiCppsO/ueHjlxnU3ga1L4neutVIb42+5wfQnKZHfgANaST8tm+x/Tvb79pe10QPrdgesf2W7f1NTzFWTIN23PbbU5bNs73b9uHi67TTpDXU23rb/1u8dvtt/2tDvS22/Z+2D9k+aPvbxfJGX7uSvhp53fp+2m97lqTfS7pZ0qik1yWtiojf9rWRFmyPSBqKiMavCdteLumkpGci4ppi2ZOSTkTEhuIH59yI+PcB6W29pJNNz9xcTCizcOrM0pJuk3SnGnztSvq6XQ28bk2M/MOS3o2IP0TEKUk7JK1soI+BFxGvSjpxzuKVkrYXj7dr8j9P37XobSBExLGIeLN4/IGkszNLN/ralfTViCbCv0jSkSnPRzVYU36HpF/bfsP2mqabmcaCszMjFV8vb7ifc7WdubmfzplZemBeu25mvK5aE+GfbjaRQbrksCwi/knSv0j6VnF6i850NHNzv0wzs/RA6HbG66o1Ef5RSYunPP+cpKMN9DGtiDhafD0uaacGb/bhsbOTpBZfjzfcz/8bpJmbp5tZWgPw2g3SjNdNhP91SVfa/rzt2ZK+IWlXA318jO2Li1/EyPbFkr6iwZt9eJek1cXj1ZJeaLCXvzEoMze3mllaDb92gzbjdSM3+RSXMjZKmiVpW0T8R9+bmIbtv9fkaC9NfuLxp032ZvtZSSs0+amvMUnflfS8pJ9LWiLpj5K+HhF9/8Vbi95W6Dxnbq6pt1YzS+9Vg69dlTNeV9IPd/gBOXGHH5AU4QeSIvxAUoQfSIrwA0kRfiApwg8kRfiBpP4PsLbHmY6NcN0AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainDatas = mnist.train.images\n",
    "trainLabels = mnist.train.labels\n",
    "testDatas = mnist.test.images\n",
    "testLabels = mnist.test.labels\n",
    "#显示图看下\n",
    "plt.figure()\n",
    "for i in range(3):\n",
    "    plt.imshow(trainDatas[i].reshape(28,28),'gray')\n",
    "    plt.pause(0.0000001)\n",
    "plt.show()\n",
    "#更多本数据集操作见\n",
    "#https://blog.csdn.net/panrenlong/article/details/81736754"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#数据初始化\n",
    "n_input = 784\n",
    "n_output = 10\n",
    "#定义两层conv卷积，两层全连接层fc1，fc2\n",
    "weights = {\n",
    "    'wc1':tf.Variable(tf.truncated_normal([3,3,1,64],stddev=0.1)),\n",
    "    'wc2':tf.Variable(tf.truncated_normal([3,3,64,128],stddev=0.1)),\n",
    "    'wd1':tf.Variable(tf.truncated_normal([7*7*128,1024],stddev=0.1)),\n",
    "    'wd2':tf.Variable(tf.truncated_normal([1024,n_output],stddev=0.1))\n",
    "}\n",
    "#参数解释wc1 [3,3,1,64]#filter的高h,宽w,深度1为灰度图RGB通道一条，输出64个特征图\n",
    "#wc2[3,3,64,128] #filter高，宽，深64（前一层传过来的64个），输出特征图128个\n",
    "\n",
    "#定义偏置项\n",
    "biases = {\n",
    "    'bc1':tf.Variable(tf.random_normal([64],stddev=0.1)),\n",
    "    'bc2':tf.Variable(tf.random_normal([128],stddev=0.1)),\n",
    "    'bd1':tf.Variable(tf.random_normal([1024],stddev=0.1)),\n",
    "    'bd2':tf.Variable(tf.random_normal([n_output],stddev=0.1))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#卷积加池化操作函数\n",
    "def conv_basic(_input,_w,_b,_keepratio):\n",
    "    #前向传播\n",
    "    #数据预处理,将输入的数据变成tensorflow需要的四维格式\n",
    "    _input_r = tf.reshape(_input,shape=[-1,28,28,1])\n",
    "    #参数：batchsize，没有的话设1，-1 tf会根据后三项自动算\n",
    "        #h高度，w宽度，chanel RGB 通道1灰度\n",
    "    \n",
    "    #用conv2d进行卷积，代入参数\n",
    "    _conv1 = tf.nn.conv2d(_input_r,_w['wc1'],strides=[1,1,1,1],padding='SAME')\n",
    "    #参数：_w['wc1']为上面定义的字典取值第一层w\n",
    "        #strides 滑动步长 要求四维格式[输入,h,w,chanel]主要 改中间两个h w\n",
    "        #padding='SAME' 滑到边缘有空值时补0\n",
    "    \n",
    "    #给上面卷积层加relu激活函数，转非线性\n",
    "    _conv1 = tf.nn.relu(tf.nn.bias_add(_conv1,_b['bc1']))\n",
    "    #relu里面是一个wx+b操作\n",
    "    \n",
    "    #加池化层Max-pooling\n",
    "    _pool1 = tf.nn.max_pool(_conv1,ksize=[1,2,2,1],strides=[1,2,2,1],padding='SAME')\n",
    "    #参数：ksize = [1,2,2,1]#2*2窗口\n",
    "        #strides 滑动步长\n",
    "    \n",
    "    #加一层dropout，防过拟和（通过弃用不重要特征的方式）\n",
    "    _pool_dr1 = tf.nn.dropout(_pool1,_keepratio)\n",
    "    #_keepratio杀死比例\n",
    "    \n",
    "    #再定义一层卷积\n",
    "    _conv2 = tf.nn.conv2d(_pool_dr1,_w['wc2'],strides=[1,1,1,1],padding='SAME')\n",
    "    #加relu激活函数\n",
    "    _conv2 = tf.nn.relu(tf.nn.bias_add(_conv2,_b['bc2']))\n",
    "    \n",
    "    #二层池化层\n",
    "    _pool2 = tf.nn.max_pool(_conv2,ksize=[1,2,2,1],strides=[1,2,2,1],padding='SAME')\n",
    "    #加dropout\n",
    "    _pool_dr2 = tf.nn.dropout(_pool2,_keepratio)\n",
    "    \n",
    "    #拼接整理特征图\n",
    "    _dense1 = tf.reshape(_pool_dr2,[-1,_w['wd1'].get_shape().as_list()[0]])\n",
    "    #_w['wd1']用上面字典中定义好的骨架\n",
    "    \n",
    "    #全连接一层fc1\n",
    "    _fc1 = tf.nn.relu(tf.add(tf.matmul(_dense1,_w['wd1']),_b['bd1']))\n",
    "    #与上一层输入x即 _dense1特征图，执行wx+b操作，再relu\n",
    "    #加dropout\n",
    "    _fc_dr1 = tf.nn.dropout(_fc1,_keepratio)\n",
    "                      \n",
    "    #第二层全连接层fc2，这里用_out表示了\n",
    "    _out = tf.add(tf.matmul(_fc_dr1,_w['wd2']),_b['bd2'])\n",
    "    #wx+b,累加\n",
    "    \n",
    "    #组合上面信息返回值\n",
    "    out = {\n",
    "        'input_r':_input_r,\n",
    "        'conv1':_conv1,\n",
    "        'pool1':_pool1,\n",
    "        'pool_dr1':_pool_dr1,\n",
    "        'conv2':_conv2,\n",
    "        'pool2':_pool2,\n",
    "        'pool_dr2':_pool_dr2,\n",
    "        'dense1':_dense1,\n",
    "        'fc1':_fc1,\n",
    "        'fc_dr1':_fc_dr1,\n",
    "        'out':_out\n",
    "    }\n",
    "    \n",
    "    return out\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#处理tf相关操作\n",
    "a = tf.Variable(tf.random_normal([3,3,1,64],stddev=0.1))\n",
    "#print(a)\n",
    "a = tf.Print(a,[a],\"a:\")\n",
    "#初始化tf变量\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32,[None,n_input])\n",
    "y = tf.placeholder(tf.float32,[None,n_output])\n",
    "keepratio = tf.placeholder(tf.float32)\n",
    "\n",
    "_pred = conv_basic(x,weights,biases,keepratio)['out']\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=_pred,labels=y))\n",
    "#`softmax_cross_entropy_with_logits`参数格式 (labels=..., logits=..., ...)\n",
    "optm = tf.train.AdamOptimizer(learning_rate=0.001).minimize(cost)\n",
    "_corr = tf.equal(tf.argmax(_pred,1),tf.argmax(y,1))\n",
    "#比较预测值与真实例相等则正确\n",
    "accr = tf.reduce_mean(tf.cast(_corr,tf.float32))\n",
    "\n",
    "#tf变量初始化\n",
    "init = tf.global_variables_initializer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 1 次迭代，共 15 次，平均损失: 5.714303398 \n",
      "训练精确度：0.125000000\n",
      "第 2 次迭代，共 15 次，平均损失: 2.583076143 \n",
      "训练精确度：0.062500000\n",
      "第 3 次迭代，共 15 次，平均损失: 2.250749660 \n",
      "训练精确度：0.125000000\n",
      "第 4 次迭代，共 15 次，平均损失: 2.132968116 \n",
      "训练精确度：0.500000000\n",
      "第 5 次迭代，共 15 次，平均损失: 2.019559205 \n",
      "训练精确度：0.562500000\n",
      "第 6 次迭代，共 15 次，平均损失: 1.710370076 \n",
      "训练精确度：0.750000000\n",
      "第 7 次迭代，共 15 次，平均损失: 1.180212092 \n",
      "训练精确度：0.750000000\n",
      "第 8 次迭代，共 15 次，平均损失: 0.928154027 \n",
      "训练精确度：0.687500000\n",
      "第 9 次迭代，共 15 次，平均损失: 0.692349124 \n",
      "训练精确度：0.937500000\n",
      "第 10 次迭代，共 15 次，平均损失: 0.717520785 \n",
      "训练精确度：0.812500000\n",
      "第 11 次迭代，共 15 次，平均损失: 0.619076169 \n",
      "训练精确度：0.812500000\n",
      "第 12 次迭代，共 15 次，平均损失: 0.580044615 \n",
      "训练精确度：0.812500000\n",
      "第 13 次迭代，共 15 次，平均损失: 0.650347844 \n",
      "训练精确度：0.937500000\n",
      "第 14 次迭代，共 15 次，平均损失: 0.590193996 \n",
      "训练精确度：0.875000000\n",
      "第 15 次迭代，共 15 次，平均损失: 0.562588316 \n",
      "训练精确度：0.875000000\n"
     ]
    }
   ],
   "source": [
    "#迭代执行上面函数\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "training_epochs = 15\n",
    "batch_size = 16\n",
    "display_step = 1\n",
    "\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0\n",
    "    total_batch = 10\n",
    "    for i in range(total_batch):\n",
    "        batch_xs,batch_ys = mnist.train.next_batch(batch_size)\n",
    "        #【训练数据是从这里传进去的，mnist.train 自动划分了x和y】\n",
    "        sess.run(optm,feed_dict={x:batch_xs,y:batch_ys,keepratio:0.7})\n",
    "        avg_cost += sess.run(cost,feed_dict={x:batch_xs,y:batch_ys,keepratio:1.})/total_batch\n",
    "        \n",
    "    if epoch % display_step == 0:\n",
    "        print(\"第 %s 次迭代，共 %s 次，平均损失: %.9f \" % (epoch+1,training_epochs,avg_cost))\n",
    "        train_acc = sess.run(accr,feed_dict={x:batch_xs,y:batch_ys,keepratio:1.})\n",
    "        print(\"训练精确度：%.9f\" % train_acc)\n",
    "        #显示小数点位数%.3f 3位，注意前面不是%s.3f\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.1 0.2 0.3]\n",
      " [2.1 2.2 2.3]\n",
      " [3.1 3.2 3.3]\n",
      " [1.1 1.2 1.3]]\n",
      "Tensor(\"embedding_lookup:0\", shape=(4, 3), dtype=float64)\n",
      "==================\n",
      "[[[0.1 0.2 0.3]\n",
      "  [2.1 2.2 2.3]\n",
      "  [3.1 3.2 3.3]\n",
      "  [1.1 1.2 1.3]]\n",
      "\n",
      " [[4.1 4.2 4.3]\n",
      "  [0.1 0.2 0.3]\n",
      "  [2.1 2.2 2.3]\n",
      "  [2.1 2.2 2.3]]]\n",
      "Tensor(\"embedding_lookup_1:0\", shape=(2, 4, 3), dtype=float64)\n"
     ]
    }
   ],
   "source": [
    "# -*- coding= utf-8 -*-\n",
    "#embedding_lookup用法，通过embedding_lookup用法来实现窗口扫描文本\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "a = [[0.1, 0.2, 0.3], [1.1, 1.2, 1.3], [2.1, 2.2, 2.3], [3.1, 3.2, 3.3], [4.1, 4.2, 4.3]]\n",
    "a = np.asarray(a)\n",
    "idx1 = tf.Variable([0, 2, 3, 1], tf.int32)\n",
    "idx2 = tf.Variable([[0, 2, 3, 1], [4, 0, 2, 2]], tf.int32)\n",
    "out1 = tf.nn.embedding_lookup(a, idx1)\n",
    "out2 = tf.nn.embedding_lookup(a, idx2)\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    print( sess.run(out1))\n",
    "    print(out1)\n",
    "    print('==================')\n",
    "    print(sess.run(out2))\n",
    "    print(out2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Text-CNN,即cnn对文本的处理\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "#文本卷积函数\n",
    "class TextCNN(object):\n",
    "    \"\"\"\n",
    "    cnn对文本的处理\n",
    "    输入层, 卷积层, 池化层，softmax连接层.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "      self, sequence_length, num_classes, vocab_size,\n",
    "      embedding_size, filter_sizes, num_filters, l2_reg_lambda=0.0):\n",
    "\n",
    "        # 初始化：sequence_length为词的长度\n",
    "        self.input_x = tf.placeholder(tf.int32, [None, sequence_length], name=\"input_x\")\n",
    "        self.input_y = tf.placeholder(tf.float32, [None, num_classes], name=\"input_y\")\n",
    "        self.dropout_keep_prob = tf.placeholder(tf.float32, name=\"dropout_keep_prob\")\n",
    "\n",
    "        # l2正则惩罚项\n",
    "        l2_loss = tf.constant(0.0)\n",
    "\n",
    "        # 输入层\n",
    "        with tf.device('/cpu:0'), tf.name_scope(\"embedding\"):\n",
    "            self.W = tf.Variable(\n",
    "                tf.random_uniform([vocab_size, embedding_size], -1.0, 1.0),\n",
    "                name=\"W\")\n",
    "            self.embedded_chars = tf.nn.embedding_lookup(self.W, self.input_x)\n",
    "            self.embedded_chars_expanded = tf.expand_dims(self.embedded_chars, -1)\n",
    "\n",
    "        #为每个filter 卷积和 建 卷积层、池化层\n",
    "        pooled_outputs = []\n",
    "        for i, filter_size in enumerate(filter_sizes):\n",
    "            with tf.name_scope(\"conv-maxpool-%s\" % filter_size):\n",
    "                # 建卷积层，通过tf的conv2d开始卷积\n",
    "                filter_shape = [filter_size, embedding_size, 1, num_filters]\n",
    "                W = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name=\"W\")\n",
    "                b = tf.Variable(tf.constant(0.1, shape=[num_filters]), name=\"b\")\n",
    "                conv = tf.nn.conv2d(\n",
    "                    self.embedded_chars_expanded,\n",
    "                    W,\n",
    "                    strides=[1, 1, 1, 1],\n",
    "                    padding=\"VALID\",\n",
    "                    name=\"conv\")\n",
    "                # 加激活函数\n",
    "                h = tf.nn.relu(tf.nn.bias_add(conv, b), name=\"relu\")\n",
    "                # 加池化层\n",
    "                pooled = tf.nn.max_pool(\n",
    "                    h,\n",
    "                    ksize=[1, sequence_length - filter_size + 1, 1, 1],\n",
    "                    strides=[1, 1, 1, 1],\n",
    "                    padding='VALID',\n",
    "                    name=\"pool\")\n",
    "                pooled_outputs.append(pooled)\n",
    "\n",
    "        # 组合特征\n",
    "        num_filters_total = num_filters * len(filter_sizes)\n",
    "        self.h_pool = tf.concat(pooled_outputs, 3)\n",
    "        self.h_pool_flat = tf.reshape(self.h_pool, [-1, num_filters_total])\n",
    "\n",
    "        # 填加dropout防止过拟和\n",
    "        with tf.name_scope(\"dropout\"):\n",
    "            self.h_drop = tf.nn.dropout(self.h_pool_flat, self.dropout_keep_prob)\n",
    "\n",
    "        # 预测\n",
    "        with tf.name_scope(\"output\"):\n",
    "            W = tf.get_variable(\n",
    "                \"W\",\n",
    "                shape=[num_filters_total, num_classes],\n",
    "                initializer=tf.contrib.layers.xavier_initializer())\n",
    "            b = tf.Variable(tf.constant(0.1, shape=[num_classes]), name=\"b\")\n",
    "            l2_loss += tf.nn.l2_loss(W)\n",
    "            l2_loss += tf.nn.l2_loss(b)\n",
    "            self.scores = tf.nn.xw_plus_b(self.h_drop, W, b, name=\"scores\")\n",
    "            self.predictions = tf.argmax(self.scores, 1, name=\"predictions\")\n",
    "\n",
    "        # 计算损失值\n",
    "        with tf.name_scope(\"loss\"):\n",
    "            losses = tf.nn.softmax_cross_entropy_with_logits(logits=self.scores, labels=self.input_y)\n",
    "            self.loss = tf.reduce_mean(losses) + l2_reg_lambda * l2_loss\n",
    "\n",
    "        # 计算精确度\n",
    "        with tf.name_scope(\"accuracy\"):\n",
    "            correct_predictions = tf.equal(self.predictions, tf.argmax(self.input_y, 1))\n",
    "            self.accuracy = tf.reduce_mean(tf.cast(correct_predictions, \"float\"), name=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "#读取数据\n",
    "import numpy as np\n",
    "import re\n",
    "import itertools\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "def clean_str(string):\n",
    "    \"\"\"\n",
    "    Tokenization/string cleaning for all datasets except for SST.\n",
    "    Original taken from https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py\n",
    "    \"\"\"\n",
    "    # 清理数据替换掉无词义的符号\n",
    "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n",
    "    string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
    "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
    "    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
    "    string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
    "    string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
    "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
    "    string = re.sub(r\",\", \" , \", string)\n",
    "    string = re.sub(r\"!\", \" ! \", string)\n",
    "    string = re.sub(r\"\\(\", \" \\( \", string)\n",
    "    string = re.sub(r\"\\)\", \" \\) \", string)\n",
    "    string = re.sub(r\"\\?\", \" \\? \", string)\n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "    return string.strip().lower()\n",
    "\n",
    "\n",
    "def load_data_and_labels(positive_data_file, negative_data_file):\n",
    "    \"\"\"\n",
    "    Loads MR polarity data from files, splits the data into words and generates labels.\n",
    "    Returns split sentences and labels.\n",
    "    \"\"\"\n",
    "    # Load data from files\n",
    "\n",
    "\n",
    "    positive = open(positive_data_file, \"rb\").read().decode('utf-8')\n",
    "    negative = open(negative_data_file, \"rb\").read().decode('utf-8')\n",
    "\n",
    "    # 按回车分割样本\n",
    "    positive_examples = positive.split('\\n')[:-1]\n",
    "    negative_examples = negative.split('\\n')[:-1]\n",
    "\n",
    "    # 去空格\n",
    "    positive_examples = [s.strip() for s in positive_examples]\n",
    "    negative_examples = [s.strip() for s in negative_examples]\n",
    "\n",
    "    #positive_examples = list(open(positive_data_file, \"rb\").read().decode('utf-8'))\n",
    "    #positive_examples = [s.strip() for s in positive_examples]\n",
    "    #negative_examples = list(open(negative_data_file, \"rb\").read().decode('utf-8'))\n",
    "    #negative_examples = [s.strip() for s in negative_examples]\n",
    "    # Split by words\n",
    "    x_text = positive_examples + negative_examples\n",
    "    x_text = [clean_str(sent) for sent in x_text] # 字符过滤，实现函数见clean_str()\n",
    "    # Generate labels\n",
    "    positive_labels = [[0, 1] for _ in positive_examples]\n",
    "    negative_labels = [[1, 0] for _ in negative_examples]\n",
    "    y = np.concatenate([positive_labels, negative_labels], 0) # 将两种label连在一起\n",
    "    return [x_text, y]\n",
    "\n",
    "# 创建batch迭代模块\n",
    "def batch_iter(data, batch_size, num_epochs, shuffle=True): # shuffle=True洗牌\n",
    "    \"\"\"\n",
    "    Generates a batch iterator for a dataset.\n",
    "    \"\"\"\n",
    "    # 每次只输出shuffled_data[start_index:end_index]这么多\n",
    "    data = np.array(data)\n",
    "    data_size = len(data)\n",
    "    num_batches_per_epoch = int((len(data)-1)/batch_size) + 1 # 每一个epoch有多少个batch_size\n",
    "    for epoch in range(num_epochs):\n",
    "        # Shuffle the data at each epoch\n",
    "        if shuffle:\n",
    "            shuffle_indices = np.random.permutation(np.arange(data_size)) # 洗牌\n",
    "            shuffled_data = data[shuffle_indices]\n",
    "        else:\n",
    "            shuffled_data = data\n",
    "        for batch_num in range(num_batches_per_epoch):\n",
    "            start_index = batch_num * batch_size # 当前batch的索引开始\n",
    "            end_index = min((batch_num + 1) * batch_size, data_size) # 判断下一个batch是不是超过最后一个数据了\n",
    "            yield shuffled_data[start_index:end_index]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#训练迭代\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "from tensorflow.contrib import learn\n",
    "\n",
    "# Parameters\n",
    "# ==================================================\n",
    "\n",
    "# Data loading params\n",
    "# 语料文件路径定义\n",
    "tf.flags.DEFINE_float(\"dev_sample_percentage\", .1, \"Percentage of the training data to use for validation\")\n",
    "tf.flags.DEFINE_string(\"positive_data_file\", \"./data/rt-polaritydata/rt-polarity.pos\", \"Data source for the positive data.\")\n",
    "tf.flags.DEFINE_string(\"negative_data_file\", \"./data/rt-polaritydata/rt-polarity.neg\", \"Data source for the negative data.\")\n",
    "\n",
    "# Model Hyperparameters\n",
    "# 定义网络超参数\n",
    "tf.flags.DEFINE_integer(\"embedding_dim\", 128, \"Dimensionality of character embedding (default: 128)\")\n",
    "tf.flags.DEFINE_string(\"filter_sizes\", \"3,4,5\", \"Comma-separated filter sizes (default: '3,4,5')\")\n",
    "tf.flags.DEFINE_integer(\"num_filters\", 128, \"Number of filters per filter size (default: 128)\")\n",
    "tf.flags.DEFINE_float(\"dropout_keep_prob\", 0.5, \"Dropout keep probability (default: 0.5)\")\n",
    "tf.flags.DEFINE_float(\"l2_reg_lambda\", 0.0, \"L2 regularization lambda (default: 0.0)\")\n",
    "\n",
    "# Training parameters\n",
    "# 训练参数\n",
    "tf.flags.DEFINE_integer(\"batch_size\", 32, \"Batch Size (default: 32)\")\n",
    "tf.flags.DEFINE_integer(\"num_epochs\", 200, \"Number of training epochs (default: 200)\") # 总训练次数\n",
    "tf.flags.DEFINE_integer(\"evaluate_every\", 100, \"Evaluate model on dev set after this many steps (default: 100)\") # 每训练100次测试一下\n",
    "tf.flags.DEFINE_integer(\"checkpoint_every\", 100, \"Save model after this many steps (default: 100)\") # 保存一次模型\n",
    "tf.flags.DEFINE_integer(\"num_checkpoints\", 5, \"Number of checkpoints to store (default: 5)\")\n",
    "# Misc Parameters\n",
    "tf.flags.DEFINE_boolean(\"allow_soft_placement\", True, \"Allow device soft device placement\") # 加上一个布尔类型的参数，要不要自动分配\n",
    "tf.flags.DEFINE_boolean(\"log_device_placement\", False, \"Log placement of ops on devices\") # 加上一个布尔类型的参数，要不要打印日志\n",
    "\n",
    "# 打印一下相关初始参数\n",
    "FLAGS = tf.flags.FLAGS\n",
    "FLAGS._parse_flags()\n",
    "print(\"\\nParameters:\")\n",
    "for attr, value in sorted(FLAGS.__flags.items()):\n",
    "    print(\"{}={}\".format(attr.upper(), value))\n",
    "print(\"\")\n",
    "\n",
    "\n",
    "# Data Preparation\n",
    "# ==================================================\n",
    "\n",
    "# Load data\n",
    "print(\"Loading data...\")\n",
    "x_text, y = load_data_and_labels(FLAGS.positive_data_file, FLAGS.negative_data_file)\n",
    "\n",
    "# Build vocabulary\n",
    "max_document_length = max([len(x.split(\" \")) for x in x_text]) # 计算最长邮件\n",
    "vocab_processor = learn.preprocessing.VocabularyProcessor(max_document_length) # tensorflow提供的工具，将数据填充为最大长度，默认0填充\n",
    "x = np.array(list(vocab_processor.fit_transform(x_text)))\n",
    "\n",
    "# Randomly shuffle data\n",
    "# 数据洗牌\n",
    "np.random.seed(10)\n",
    "# np.arange生成随机序列\n",
    "shuffle_indices = np.random.permutation(np.arange(len(y)))\n",
    "x_shuffled = x[shuffle_indices]\n",
    "y_shuffled = y[shuffle_indices]\n",
    "\n",
    "# 将数据按训练train和测试dev分块\n",
    "# Split train/test set\n",
    "# TODO: This is very crude, should use cross-validation\n",
    "dev_sample_index = -1 * int(FLAGS.dev_sample_percentage * float(len(y)))\n",
    "x_train, x_dev = x_shuffled[:dev_sample_index], x_shuffled[dev_sample_index:]\n",
    "y_train, y_dev = y_shuffled[:dev_sample_index], y_shuffled[dev_sample_index:]\n",
    "print(\"Vocabulary Size: {:d}\".format(len(vocab_processor.vocabulary_)))\n",
    "print(\"Train/Dev split: {:d}/{:d}\".format(len(y_train), len(y_dev))) # 打印切分的比例\n",
    "\n",
    "\n",
    "# Training\n",
    "# ==================================================\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "    session_conf = tf.ConfigProto(\n",
    "        allow_soft_placement=FLAGS.allow_soft_placement,\n",
    "        log_device_placement=FLAGS.log_device_placement)\n",
    "    sess = tf.Session(config=session_conf)\n",
    "    with sess.as_default():\n",
    "        # 卷积池化网络导入\n",
    "        cnn = TextCNN(\n",
    "            sequence_length=x_train.shape[1],\n",
    "            num_classes=y_train.shape[1], # 分几类\n",
    "            vocab_size=len(vocab_processor.vocabulary_),\n",
    "            embedding_size=FLAGS.embedding_dim,\n",
    "            filter_sizes=list(map(int, FLAGS.filter_sizes.split(\",\"))), # 上面定义的filter_sizes拿过来，\"3,4,5\"按\",\"分割\n",
    "            num_filters=FLAGS.num_filters, # 一共有几个filter\n",
    "            l2_reg_lambda=FLAGS.l2_reg_lambda) # l2正则化项\n",
    "\n",
    "        # Define Training procedure\n",
    "        global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "        optimizer = tf.train.AdamOptimizer(1e-3) # 定义优化器\n",
    "        grads_and_vars = optimizer.compute_gradients(cnn.loss)\n",
    "        train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step)\n",
    "\n",
    "        # Keep track of gradient values and sparsity (optional)\n",
    "        grad_summaries = []\n",
    "        for g, v in grads_and_vars:\n",
    "            if g is not None:\n",
    "                grad_hist_summary = tf.summary.histogram(\"{}/grad/hist\".format(v.name), g)\n",
    "                sparsity_summary = tf.summary.scalar(\"{}/grad/sparsity\".format(v.name), tf.nn.zero_fraction(g))\n",
    "                grad_summaries.append(grad_hist_summary)\n",
    "                grad_summaries.append(sparsity_summary)\n",
    "        grad_summaries_merged = tf.summary.merge(grad_summaries)\n",
    "\n",
    "        # Output directory for models and summaries\n",
    "        timestamp = str(int(time.time()))\n",
    "        out_dir = os.path.abspath(os.path.join(os.path.curdir, \"runs\", timestamp))\n",
    "        print(\"Writing to {}\\n\".format(out_dir))\n",
    "\n",
    "        # Summaries for loss and accuracy\n",
    "        # 损失函数和准确率的参数保存\n",
    "        loss_summary = tf.summary.scalar(\"loss\", cnn.loss)\n",
    "        acc_summary = tf.summary.scalar(\"accuracy\", cnn.accuracy)\n",
    "\n",
    "        # Train Summaries\n",
    "        # 训练数据保存\n",
    "        train_summary_op = tf.summary.merge([loss_summary, acc_summary, grad_summaries_merged])\n",
    "        train_summary_dir = os.path.join(out_dir, \"summaries\", \"train\")\n",
    "        train_summary_writer = tf.summary.FileWriter(train_summary_dir, sess.graph)\n",
    "\n",
    "        # Dev summaries\n",
    "        # 测试数据保存\n",
    "        dev_summary_op = tf.summary.merge([loss_summary, acc_summary])\n",
    "        dev_summary_dir = os.path.join(out_dir, \"summaries\", \"dev\")\n",
    "        dev_summary_writer = tf.summary.FileWriter(dev_summary_dir, sess.graph)\n",
    "\n",
    "        # Checkpoint directory. Tensorflow assumes this directory already exists so we need to create it\n",
    "        checkpoint_dir = os.path.abspath(os.path.join(out_dir, \"checkpoints\"))\n",
    "        checkpoint_prefix = os.path.join(checkpoint_dir, \"model\")\n",
    "        if not os.path.exists(checkpoint_dir):\n",
    "            os.makedirs(checkpoint_dir)\n",
    "\n",
    "        saver = tf.train.Saver(tf.global_variables(), max_to_keep=FLAGS.num_checkpoints) # 前面定义好参数num_checkpoints\n",
    "\n",
    "        # Write vocabulary\n",
    "        vocab_processor.save(os.path.join(out_dir, \"vocab\"))\n",
    "\n",
    "        # Initialize all variables\n",
    "        sess.run(tf.global_variables_initializer()) # 初始化所有变量\n",
    "\n",
    "        # 定义训练函数\n",
    "        def train_step(x_batch, y_batch):\n",
    "            \"\"\"\n",
    "            A single training step\n",
    "            \"\"\"\n",
    "            feed_dict = {\n",
    "              cnn.input_x: x_batch,\n",
    "              cnn.input_y: y_batch,\n",
    "              cnn.dropout_keep_prob: FLAGS.dropout_keep_prob # 参数在前面有定义\n",
    "            }\n",
    "            _, step, summaries, loss, accuracy = sess.run(\n",
    "                [train_op, global_step, train_summary_op, cnn.loss, cnn.accuracy], feed_dict)\n",
    "            time_str = datetime.datetime.now().isoformat() # 取当前时间，python的函数\n",
    "            print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n",
    "            train_summary_writer.add_summary(summaries, step)\n",
    "\n",
    "        # 定义测试函数\n",
    "        def dev_step(x_batch, y_batch, writer=None):\n",
    "            \"\"\"\n",
    "            Evaluates model on a dev set\n",
    "            \"\"\"\n",
    "            feed_dict = {\n",
    "              cnn.input_x: x_batch,\n",
    "              cnn.input_y: y_batch,\n",
    "              cnn.dropout_keep_prob: 1.0 # 神经元全部保留\n",
    "            }\n",
    "            step, summaries, loss, accuracy = sess.run(\n",
    "                [global_step, dev_summary_op, cnn.loss, cnn.accuracy], feed_dict)\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n",
    "            if writer:\n",
    "                writer.add_summary(summaries, step)\n",
    "\n",
    "        # Generate batches\n",
    "        batches = batch_iter(list(zip(x_train, y_train)), FLAGS.batch_size, FLAGS.num_epochs)\n",
    "        # Training loop. For each batch...\n",
    "        # 训练部分\n",
    "        for batch in batches:\n",
    "            x_batch, y_batch = zip(*batch) # 按batch把数据拿进来\n",
    "            train_step(x_batch, y_batch)\n",
    "            current_step = tf.train.global_step(sess, global_step) # 将Session和global_step值传进来\n",
    "            if current_step % FLAGS.evaluate_every == 0: # 每FLAGS.evaluate_every次每100执行一次测试\n",
    "                print(\"\\nEvaluation:\")\n",
    "                dev_step(x_dev, y_dev, writer=dev_summary_writer)\n",
    "                print(\"\")\n",
    "            if current_step % FLAGS.checkpoint_every == 0: # 每checkpoint_every次执行一次保存模型\n",
    "                path = saver.save(sess, './', global_step=current_step) # 定义模型保存路径\n",
    "                print(\"Saved model checkpoint to {}\\n\".format(path))\n",
    "\n",
    "#【待调试】"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
