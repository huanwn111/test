{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 循环和递归神经网络 Task8\n",
    "\n",
    "* RNN的结构。循环神经网络的提出背景、优缺点。着重学习RNN的反向传播、RNN出现的问题（梯度问题、长期依赖问题）、BPTT算法。\n",
    "* 双向RNN\n",
    "* 递归神经网络\n",
    "* LSTM、GRU的结构、提出背景、优缺点。\n",
    "* 针对梯度消失（LSTM等其他门控RNN）、梯度爆炸（梯度截断）的解决方案。\n",
    "* Memory Network（自选）\n",
    "* Text-RNN的原理。\n",
    "* 利用Text-RNN模型来进行文本分类。\n",
    "* Recurrent Convolutional Neural Networks（RCNN）原理。\n",
    "* 利用RCNN模型来进行文本分类。\n",
    "* 参考：[一份详细的LSTM和GRU图解](https://www.atyun.com/30234.html) [Tensorflow实战(1): 实现深层循环神经网络](https://zhuanlan.zhihu.com/p/37070414) [lstm](https://x-algo.cn/index.php/2017/01/13/1609/) [RCNN kreas](https://github.com/airalcorn2/Recurrent-Convolutional-Neural-Network-Text-Classifier) [RCNN tf](https://github.com/zhangfazhan/TextRCNN) [RCNN tf 推荐](https://github.com/roomylee/rcnn-text-classification)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs:===> (<tf.Tensor 'bidirectional_rnn/fw/fw/transpose_1:0' shape=(?, 5, 100) dtype=float32>, <tf.Tensor 'ReverseV2:0' shape=(?, 5, 100) dtype=float32>)\n",
      "loss: 2.8903913 acc: 0.125 label: [1 0 1 1 1 2 1 1] prediction: [2 2 2 2 2 2 2 2]\n",
      "loss: 2.721138 acc: 0.125 label: [1 0 1 1 1 2 1 1] prediction: [2 2 2 2 2 2 2 2]\n",
      "loss: 2.5252109 acc: 0.125 label: [1 0 1 1 1 2 1 1] prediction: [2 2 2 2 2 2 2 2]\n",
      "loss: 2.2736619 acc: 0.75 label: [1 0 1 1 1 2 1 1] prediction: [1 1 1 1 1 1 1 1]\n",
      "loss: 1.9544692 acc: 0.75 label: [1 0 1 1 1 2 1 1] prediction: [1 1 1 1 1 1 1 1]\n",
      "loss: 1.6184561 acc: 0.75 label: [1 0 1 1 1 2 1 1] prediction: [1 1 1 1 1 1 1 1]\n",
      "loss: 1.4595925 acc: 0.75 label: [1 0 1 1 1 2 1 1] prediction: [1 1 1 1 1 1 1 1]\n",
      "loss: 1.5684565 acc: 0.75 label: [1 0 1 1 1 2 1 1] prediction: [1 1 1 1 1 1 1 1]\n",
      "loss: 1.618169 acc: 0.75 label: [1 0 1 1 1 2 1 1] prediction: [1 1 1 1 1 1 1 1]\n",
      "loss: 1.5419416 acc: 0.75 label: [1 0 1 1 1 2 1 1] prediction: [1 1 1 1 1 1 1 1]\n",
      "loss: 1.4079881 acc: 0.75 label: [1 0 1 1 1 2 1 1] prediction: [1 1 1 1 1 1 1 1]\n",
      "loss: 1.2910342 acc: 0.75 label: [1 0 1 1 1 2 1 1] prediction: [1 1 1 1 1 1 1 1]\n",
      "loss: 1.2477609 acc: 0.75 label: [1 0 1 1 1 2 1 1] prediction: [1 1 1 1 1 1 1 1]\n",
      "loss: 1.2795731 acc: 0.75 label: [1 0 1 1 1 2 1 1] prediction: [1 1 1 1 1 1 1 1]\n",
      "loss: 1.3238435 acc: 0.75 label: [1 0 1 1 1 2 1 1] prediction: [1 1 1 1 1 1 1 1]\n",
      "loss: 1.3283986 acc: 0.75 label: [1 0 1 1 1 2 1 1] prediction: [1 1 1 1 1 1 1 1]\n",
      "loss: 1.2918348 acc: 0.75 label: [1 0 1 1 1 2 1 1] prediction: [1 1 1 1 1 1 1 1]\n",
      "loss: 1.237546 acc: 0.75 label: [1 0 1 1 1 2 1 1] prediction: [1 1 1 1 1 1 1 1]\n",
      "loss: 1.1921664 acc: 0.75 label: [1 0 1 1 1 2 1 1] prediction: [1 1 1 1 1 1 1 1]\n",
      "loss: 1.1721861 acc: 0.75 label: [1 0 1 1 1 2 1 1] prediction: [1 1 1 1 1 1 1 1]\n",
      "loss: 1.176256 acc: 0.75 label: [1 0 1 1 1 2 1 1] prediction: [1 1 1 1 1 1 1 1]\n",
      "loss: 1.1893454 acc: 0.75 label: [1 0 1 1 1 2 1 1] prediction: [1 1 1 1 1 1 1 1]\n",
      "loss: 1.195636 acc: 0.75 label: [1 0 1 1 1 2 1 1] prediction: [1 1 1 1 1 1 1 1]\n",
      "loss: 1.1878754 acc: 0.75 label: [1 0 1 1 1 2 1 1] prediction: [1 1 1 1 1 1 1 1]\n",
      "loss: 1.1683178 acc: 0.75 label: [1 0 1 1 1 2 1 1] prediction: [1 1 1 1 1 1 1 1]\n",
      "loss: 1.1450067 acc: 0.75 label: [1 0 1 1 1 2 1 1] prediction: [1 1 1 1 1 1 1 1]\n",
      "loss: 1.1268485 acc: 0.75 label: [1 0 1 1 1 2 1 1] prediction: [1 1 1 1 1 1 1 1]\n",
      "loss: 1.1189839 acc: 0.75 label: [1 0 1 1 1 2 1 1] prediction: [1 1 1 1 1 1 1 1]\n",
      "loss: 1.1201329 acc: 0.75 label: [1 0 1 1 1 2 1 1] prediction: [1 1 1 1 1 1 1 1]\n",
      "loss: 1.1238625 acc: 0.75 label: [1 0 1 1 1 2 1 1] prediction: [1 1 1 1 1 1 1 1]\n",
      "loss: 1.1233084 acc: 0.75 label: [1 0 1 1 1 2 1 1] prediction: [1 1 1 1 1 1 1 1]\n",
      "loss: 1.115623 acc: 0.75 label: [1 0 1 1 1 2 1 1] prediction: [1 1 1 1 1 1 1 1]\n",
      "loss: 1.1029158 acc: 0.75 label: [1 0 1 1 1 2 1 1] prediction: [1 1 1 1 1 1 1 1]\n",
      "loss: 1.0899689 acc: 0.75 label: [1 0 1 1 1 2 1 1] prediction: [1 1 1 1 1 1 1 1]\n",
      "loss: 1.080903 acc: 0.75 label: [1 0 1 1 1 2 1 1] prediction: [1 1 1 1 1 1 1 1]\n",
      "loss: 1.0768697 acc: 0.75 label: [1 0 1 1 1 2 1 1] prediction: [1 1 1 1 1 1 1 1]\n",
      "loss: 1.0759284 acc: 0.75 label: [1 0 1 1 1 2 1 1] prediction: [1 1 1 1 1 1 1 1]\n",
      "loss: 1.0748041 acc: 0.75 label: [1 0 1 1 1 2 1 1] prediction: [1 1 1 1 1 1 1 1]\n",
      "loss: 1.0710661 acc: 0.75 label: [1 0 1 1 1 2 1 1] prediction: [1 1 1 1 1 1 1 1]\n",
      "loss: 1.0643307 acc: 0.75 label: [1 0 1 1 1 2 1 1] prediction: [1 1 1 1 1 1 1 1]\n",
      "loss: 1.0560737 acc: 0.75 label: [1 0 1 1 1 2 1 1] prediction: [1 1 1 1 1 1 1 1]\n",
      "loss: 1.0484929 acc: 0.75 label: [1 0 1 1 1 2 1 1] prediction: [1 1 1 1 1 1 1 1]\n",
      "loss: 1.0431213 acc: 0.75 label: [1 0 1 1 1 2 1 1] prediction: [1 1 1 1 1 1 1 1]\n",
      "loss: 1.0399861 acc: 0.75 label: [1 0 1 1 1 2 1 1] prediction: [1 1 1 1 1 1 1 1]\n",
      "loss: 1.037771 acc: 0.75 label: [1 0 1 1 1 2 1 1] prediction: [1 1 1 1 1 1 1 1]\n",
      "loss: 1.034841 acc: 0.75 label: [1 0 1 1 1 2 1 1] prediction: [1 1 1 1 1 1 1 1]\n",
      "loss: 1.030349 acc: 0.75 label: [1 0 1 1 1 2 1 1] prediction: [1 1 1 1 1 1 1 1]\n",
      "loss: 1.0246623 acc: 0.75 label: [1 0 1 1 1 2 1 1] prediction: [1 1 1 1 1 1 1 1]\n",
      "loss: 1.0189185 acc: 0.75 label: [1 0 1 1 1 2 1 1] prediction: [1 1 1 1 1 1 1 1]\n",
      "loss: 1.0141504 acc: 0.75 label: [1 0 1 1 1 2 1 1] prediction: [1 1 1 1 1 1 1 1]\n",
      "loss: 1.0106272 acc: 0.75 label: [1 0 1 1 1 2 1 1] prediction: [1 1 1 1 1 1 1 1]\n",
      "loss: 1.0078081 acc: 0.75 label: [1 0 1 1 1 2 1 1] prediction: [1 1 1 1 1 1 1 1]\n",
      "loss: 1.0048356 acc: 0.75 label: [1 0 1 1 1 2 1 1] prediction: [1 1 1 1 1 1 1 1]\n",
      "loss: 1.0011536 acc: 0.75 label: [1 0 1 1 1 2 1 1] prediction: [1 1 1 1 1 1 1 1]\n",
      "loss: 0.9968182 acc: 0.75 label: [1 0 1 1 1 2 1 1] prediction: [1 1 1 1 1 1 1 1]\n",
      "loss: 0.9923602 acc: 0.75 label: [1 0 1 1 1 2 1 1] prediction: [1 1 1 1 1 1 1 1]\n",
      "loss: 0.98835635 acc: 0.75 label: [1 0 1 1 1 2 1 1] prediction: [1 1 1 1 1 1 1 1]\n",
      "loss: 0.9850286 acc: 0.75 label: [1 0 1 1 1 2 1 1] prediction: [1 1 1 1 1 1 1 1]\n",
      "loss: 0.9821347 acc: 0.75 label: [1 0 1 1 1 2 1 1] prediction: [1 1 1 1 1 1 1 1]\n",
      "loss: 0.9792042 acc: 0.75 label: [1 0 1 1 1 2 1 1] prediction: [1 1 1 1 1 1 1 1]\n",
      "loss: 0.9759131 acc: 0.75 label: [1 0 1 1 1 2 1 1] prediction: [1 1 1 1 1 1 1 1]\n",
      "loss: 0.9722955 acc: 0.75 label: [1 0 1 1 1 2 1 1] prediction: [1 1 1 1 1 1 1 1]\n",
      "loss: 0.968665 acc: 0.75 label: [1 0 1 1 1 2 1 1] prediction: [1 1 1 1 1 1 1 1]\n",
      "loss: 0.9653324 acc: 0.75 label: [1 0 1 1 1 2 1 1] prediction: [1 1 1 1 1 1 1 1]\n",
      "loss: 0.96237576 acc: 0.75 label: [1 0 1 1 1 2 1 1] prediction: [1 1 1 1 1 1 1 1]\n",
      "loss: 0.95961934 acc: 0.75 label: [1 0 1 1 1 2 1 1] prediction: [1 1 1 1 1 1 1 1]\n",
      "loss: 0.95680636 acc: 0.75 label: [1 0 1 1 1 2 1 1] prediction: [1 1 1 1 1 1 1 1]\n",
      "loss: 0.95380497 acc: 0.75 label: [1 0 1 1 1 2 1 1] prediction: [1 1 1 1 1 1 1 1]\n",
      "loss: 0.9506885 acc: 0.75 label: [1 0 1 1 1 2 1 1] prediction: [1 1 1 1 1 1 1 1]\n",
      "loss: 0.9476455 acc: 0.75 label: [1 0 1 1 1 2 1 1] prediction: [1 1 1 1 1 1 1 1]\n",
      "loss: 0.94481564 acc: 0.75 label: [1 0 1 1 1 2 1 1] prediction: [1 1 1 1 1 1 1 1]\n",
      "loss: 0.9421832 acc: 0.75 label: [1 0 1 1 1 2 1 1] prediction: [1 1 1 1 1 1 1 1]\n",
      "loss: 0.9396154 acc: 0.75 label: [1 0 1 1 1 2 1 1] prediction: [1 1 1 1 1 1 1 1]\n",
      "loss: 0.9369836 acc: 0.75 label: [1 0 1 1 1 2 1 1] prediction: [1 1 1 1 1 1 1 1]\n",
      "loss: 0.934271 acc: 0.75 label: [1 0 1 1 1 2 1 1] prediction: [1 1 1 1 1 1 1 1]\n",
      "loss: 0.93156534 acc: 0.75 label: [1 0 1 1 1 2 1 1] prediction: [1 1 1 1 1 1 1 1]\n",
      "loss: 0.9289706 acc: 0.75 label: [1 0 1 1 1 2 1 1] prediction: [1 1 1 1 1 1 1 1]\n",
      "loss: 0.9265147 acc: 0.75 label: [1 0 1 1 1 2 1 1] prediction: [1 1 1 1 1 1 1 1]\n",
      "loss: 0.9241383 acc: 0.75 label: [1 0 1 1 1 2 1 1] prediction: [1 1 1 1 1 1 1 1]\n",
      "loss: 0.9217589 acc: 0.75 label: [1 0 1 1 1 2 1 1] prediction: [1 1 1 1 1 1 1 1]\n",
      "loss: 0.9193449 acc: 0.75 label: [1 0 1 1 1 2 1 1] prediction: [1 1 1 1 1 1 1 1]\n",
      "loss: 0.9169327 acc: 0.75 label: [1 0 1 1 1 2 1 1] prediction: [1 1 1 1 1 1 1 1]\n",
      "loss: 0.9145852 acc: 0.75 label: [1 0 1 1 1 2 1 1] prediction: [1 1 1 1 1 1 1 1]\n",
      "loss: 0.9123305 acc: 0.75 label: [1 0 1 1 1 2 1 1] prediction: [1 1 1 1 1 1 1 1]\n",
      "loss: 0.9101421 acc: 0.75 label: [1 0 1 1 1 2 1 1] prediction: [1 1 1 1 1 1 1 1]\n",
      "loss: 0.9079705 acc: 0.75 label: [1 0 1 1 1 2 1 1] prediction: [1 1 1 1 1 1 1 1]\n",
      "loss: 0.90579146 acc: 0.75 label: [1 0 1 1 1 2 1 1] prediction: [1 1 1 1 1 1 1 1]\n",
      "loss: 0.9036238 acc: 0.75 label: [1 0 1 1 1 2 1 1] prediction: [1 1 1 1 1 1 1 1]\n",
      "loss: 0.9015035 acc: 0.75 label: [1 0 1 1 1 2 1 1] prediction: [1 1 1 1 1 1 1 1]\n",
      "loss: 0.8994482 acc: 0.75 label: [1 0 1 1 1 2 1 1] prediction: [1 1 1 1 1 1 1 1]\n",
      "loss: 0.89744204 acc: 0.75 label: [1 0 1 1 1 2 1 1] prediction: [1 1 1 1 1 1 1 1]\n",
      "loss: 0.8954561 acc: 0.75 label: [1 0 1 1 1 2 1 1] prediction: [1 1 1 1 1 1 1 1]\n",
      "loss: 0.8934778 acc: 0.75 label: [1 0 1 1 1 2 1 1] prediction: [1 1 1 1 1 1 1 1]\n",
      "loss: 0.89151853 acc: 0.75 label: [1 0 1 1 1 2 1 1] prediction: [1 1 1 1 1 1 1 1]\n",
      "loss: 0.8895997 acc: 0.75 label: [1 0 1 1 1 2 1 1] prediction: [1 1 1 1 1 1 1 1]\n",
      "loss: 0.8877284 acc: 0.75 label: [1 0 1 1 1 2 1 1] prediction: [1 1 1 1 1 1 1 1]\n",
      "loss: 0.88589346 acc: 0.75 label: [1 0 1 1 1 2 1 1] prediction: [1 1 1 1 1 1 1 1]\n",
      "loss: 0.8840782 acc: 0.75 label: [1 0 1 1 1 2 1 1] prediction: [1 1 1 1 1 1 1 1]\n",
      "loss: 0.88227767 acc: 0.75 label: [1 0 1 1 1 2 1 1] prediction: [1 1 1 1 1 1 1 1]\n",
      "loss: 0.88050103 acc: 0.75 label: [1 0 1 1 1 2 1 1] prediction: [1 1 1 1 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "* 利用Text-RNN模型来进行文本分类。\n",
    "'''\n",
    "# -*- coding: utf-8 -*-\n",
    "#TextRNN: 1. embeddding输入层, 2.Bi-LSTM 层, 3.concat output连接输出层, 4.FC 全连接层, 5.softmax\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "import numpy as np\n",
    "\n",
    "class TextRNN:\n",
    "    def __init__(self,num_classes, learning_rate, batch_size, decay_steps, decay_rate,sequence_length,\n",
    "                 vocab_size,embed_size,is_training,initializer=tf.random_normal_initializer(stddev=0.1)):\n",
    "       \n",
    "        # 设置超参数\n",
    "        self.num_classes = num_classes\n",
    "        self.batch_size = batch_size\n",
    "        self.sequence_length=sequence_length\n",
    "        self.vocab_size=vocab_size\n",
    "        self.embed_size=embed_size\n",
    "        self.hidden_size=embed_size\n",
    "        self.is_training=is_training\n",
    "        self.learning_rate=learning_rate\n",
    "        self.initializer=initializer\n",
    "        self.num_sampled=20\n",
    "\n",
    "        # 初始化x,label,dropout\n",
    "        self.input_x = tf.placeholder(tf.int32, [None, self.sequence_length], name=\"input_x\")  # X\n",
    "        self.input_y = tf.placeholder(tf.int32,[None], name=\"input_y\")  # y [None,num_classes]\n",
    "        self.dropout_keep_prob=tf.placeholder(tf.float32,name=\"dropout_keep_prob\")\n",
    "\n",
    "        self.global_step = tf.Variable(0, trainable=False, name=\"Global_Step\")\n",
    "        self.epoch_step=tf.Variable(0,trainable=False,name=\"Epoch_Step\")\n",
    "        self.epoch_increment=tf.assign(self.epoch_step,tf.add(self.epoch_step,tf.constant(1)))\n",
    "        self.decay_steps, self.decay_rate = decay_steps, decay_rate\n",
    "\n",
    "        self.instantiate_weights()\n",
    "        self.logits = self.inference() #[None, self.label_size]. main computation graph is here.\n",
    "        if not is_training:\n",
    "            return\n",
    "        self.loss_val = self.loss() #-->self.loss_nce()\n",
    "        self.train_op = self.train()\n",
    "        self.predictions = tf.argmax(self.logits, axis=1, name=\"predictions\")  # shape:[None,]\n",
    "        correct_prediction = tf.equal(tf.cast(self.predictions,tf.int32), self.input_y) #tf.argmax(self.logits, 1)-->[batch_size]\n",
    "        self.accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32), name=\"Accuracy\") # shape=()\n",
    "    def instantiate_weights(self):\n",
    "        \n",
    "        with tf.name_scope(\"embedding\"): # embedding matrix\n",
    "            self.Embedding = tf.get_variable(\"Embedding\",shape=[self.vocab_size, self.embed_size],initializer=self.initializer) #[vocab_size,embed_size] tf.random_uniform([self.vocab_size, self.embed_size],-1.0,1.0)\n",
    "            self.W_projection = tf.get_variable(\"W_projection\",shape=[self.hidden_size*2, self.num_classes],initializer=self.initializer) #[embed_size,label_size]\n",
    "            self.b_projection = tf.get_variable(\"b_projection\",shape=[self.num_classes])       #[label_size]\n",
    "\n",
    "    def inference(self):\n",
    "       \n",
    "        #1.【提取输入句中的词向量用tf的embedding_lookup】\n",
    "      \n",
    "        self.embedded_words = tf.nn.embedding_lookup(self.Embedding,self.input_x) #shape:[None,sentence_length,embed_size]\n",
    "        \n",
    "        #2. Bi-lstm 层\n",
    "        # 定义 lstm 单元，取lstm单元输出【BasicLSTMCell】\n",
    "        lstm_fw_cell=rnn.BasicLSTMCell(self.hidden_size) #前向单元\n",
    "        lstm_bw_cell=rnn.BasicLSTMCell(self.hidden_size) #后向单元\n",
    "        if self.dropout_keep_prob is not None:\n",
    "            lstm_fw_cell=rnn.DropoutWrapper(lstm_fw_cell,output_keep_prob=self.dropout_keep_prob)\n",
    "            lstm_bw_cell=rnn.DropoutWrapper(lstm_bw_cell,output_keep_prob=self.dropout_keep_prob)\n",
    "        # bidirectional_dynamic_rnn: input: [batch_size, max_time, input_size]\n",
    "        #                            output: A tuple (outputs, output_states)\n",
    "        #                                    where:outputs: A tuple (output_fw, output_bw) containing the forward and the backward rnn output `Tensor`.\n",
    "        outputs,_=tf.nn.bidirectional_dynamic_rnn(lstm_fw_cell,lstm_bw_cell,self.embedded_words,dtype=tf.float32) #[batch_size,sequence_length,hidden_size] #creates a dynamic bidirectional recurrent neural network\n",
    "        print(\"outputs:===>\",outputs) #outputs:(<tf.Tensor 'bidirectional_rnn/fw/fw/transpose:0' shape=(?, 5, 100) dtype=float32>, <tf.Tensor 'ReverseV2:0' shape=(?, 5, 100) dtype=float32>))\n",
    "        output_rnn=tf.concat(outputs,axis=2) #[batch_size,sequence_length,hidden_size*2]\n",
    "\n",
    "        #3. 第二层 LSTM \n",
    "        rnn_cell=rnn.BasicLSTMCell(self.hidden_size*2)\n",
    "        if self.dropout_keep_prob is not None:\n",
    "            rnn_cell=rnn.DropoutWrapper(rnn_cell,output_keep_prob=self.dropout_keep_prob)\n",
    "        _,final_state_c_h=tf.nn.dynamic_rnn(rnn_cell,output_rnn,dtype=tf.float32)\n",
    "        final_state=final_state_c_h[1]\n",
    "\n",
    "        #4 .FC全连接层\n",
    "        output=tf.layers.dense(final_state,self.hidden_size*2,activation=tf.nn.tanh)\n",
    "        \n",
    "        #5. wx+b\n",
    "        with tf.name_scope(\"output\"): #inputs: A `Tensor` of shape `[batch_size, dim]`.  The forward activations of the input network.\n",
    "            logits = tf.matmul(output, self.W_projection) + self.b_projection  # [batch_size,num_classes]\n",
    "        return logits\n",
    "\n",
    "    #损失函数\n",
    "    def loss(self,l2_lambda=0.0001):\n",
    "        with tf.name_scope(\"loss\"):\n",
    "            #input: `logits` and `labels` must have the same shape `[batch_size, num_classes]`\n",
    "            #output: A 1-D `Tensor` of length `batch_size` of the same type as `logits` with the softmax cross entropy loss.\n",
    "            losses = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=self.input_y, logits=self.logits);#sigmoid_cross_entropy_with_logits.#losses=tf.nn.softmax_cross_entropy_with_logits(labels=self.input_y,logits=self.logits)\n",
    "            #print(\"1.sparse_softmax_cross_entropy_with_logits.losses:\",losses) # shape=(?,)\n",
    "            loss=tf.reduce_mean(losses)#print(\"2.loss.loss:\", loss) #shape=()\n",
    "            l2_losses = tf.add_n([tf.nn.l2_loss(v) for v in tf.trainable_variables() if 'bias' not in v.name]) * l2_lambda\n",
    "            loss=loss+l2_losses\n",
    "        return loss\n",
    "    \n",
    "    #NCE损失\n",
    "    def loss_nce(self,l2_lambda=0.0001): #0.0001-->0.001\n",
    "        \"\"\"calculate loss using (NCE)cross entropy here\"\"\"\n",
    "        # Compute the average NCE loss for the batch.\n",
    "        # tf.nce_loss automatically draws a new sample of the negative labels each\n",
    "        # time we evaluate the loss.\n",
    "        if self.is_training: #training\n",
    "            #labels=tf.reshape(self.input_y,[-1])               #[batch_size,1]------>[batch_size,]\n",
    "            labels=tf.expand_dims(self.input_y,1)                   #[batch_size,]----->[batch_size,1]\n",
    "            loss = tf.reduce_mean( #inputs: A `Tensor` of shape `[batch_size, dim]`.  The forward activations of the input network.\n",
    "                tf.nn.nce_loss(weights=tf.transpose(self.W_projection),#[hidden_size*2, num_classes]--->[num_classes,hidden_size*2]. nce_weights:A `Tensor` of shape `[num_classes, dim].O.K.\n",
    "                               biases=self.b_projection,                 #[label_size]. nce_biases:A `Tensor` of shape `[num_classes]`.\n",
    "                               labels=labels,                 #[batch_size,1]. train_labels, # A `Tensor` of type `int64` and shape `[batch_size,num_true]`. The target classes.\n",
    "                               inputs=self.output_rnn_last,# [batch_size,hidden_size*2] #A `Tensor` of shape `[batch_size, dim]`.  The forward activations of the input network.\n",
    "                               num_sampled=self.num_sampled,  #scalar. 100\n",
    "                               num_classes=self.num_classes,partition_strategy=\"div\"))  #scalar. 1999\n",
    "        l2_losses = tf.add_n([tf.nn.l2_loss(v) for v in tf.trainable_variables() if 'bias' not in v.name]) * l2_lambda\n",
    "        loss = loss + l2_losses\n",
    "        return loss\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"based on the loss, use SGD to update parameter\"\"\"\n",
    "        learning_rate = tf.train.exponential_decay(self.learning_rate, self.global_step, self.decay_steps,self.decay_rate, staircase=True)\n",
    "        train_op = tf.contrib.layers.optimize_loss(self.loss_val, global_step=self.global_step,learning_rate=learning_rate, optimizer=\"Adam\")\n",
    "        return train_op\n",
    "\n",
    "#分类测试 \n",
    "def test():\n",
    "\n",
    "    num_classes=10\n",
    "    learning_rate=0.001\n",
    "    batch_size=8\n",
    "    decay_steps=1000\n",
    "    decay_rate=0.9\n",
    "    sequence_length=5\n",
    "    vocab_size=10000\n",
    "    embed_size=100\n",
    "    is_training=True\n",
    "    dropout_keep_prob=1#0.5\n",
    "    textRNN=TextRNN(num_classes, learning_rate, batch_size, decay_steps, decay_rate,sequence_length,vocab_size,embed_size,is_training)\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        for i in range(100):\n",
    "            input_x=np.zeros((batch_size,sequence_length)) #[None, self.sequence_length]\n",
    "            input_y=input_y=np.array([1,0,1,1,1,2,1,1]) #np.zeros((batch_size),dtype=np.int32) #[None, self.sequence_length]\n",
    "            loss,acc,predict,_=sess.run([textRNN.loss_val,textRNN.accuracy,textRNN.predictions,textRNN.train_op],feed_dict={textRNN.input_x:input_x,textRNN.input_y:input_y,textRNN.dropout_keep_prob:dropout_keep_prob})\n",
    "            print(\"loss:\",loss,\"acc:\",acc,\"label:\",input_y,\"prediction:\",predict)\n",
    "test()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "* 利用RCNN模型来进行文本分类。\n",
    "'''\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "#RCNN实现\n",
    "class TextRCNN:\n",
    "    def __init__(self, sequence_length, num_classes, vocab_size, word_embedding_size, context_embedding_size,\n",
    "                 cell_type, hidden_size, l2_reg_lambda=0.0):\n",
    "        # 初始化输入x,y,  dropout\n",
    "        self.input_text = tf.placeholder(tf.int32, shape=[None, sequence_length], name='input_text')\n",
    "        self.input_y = tf.placeholder(tf.float32, shape=[None, num_classes], name='input_y')\n",
    "        self.dropout_keep_prob = tf.placeholder(tf.float32, name='dropout_keep_prob')\n",
    "\n",
    "        l2_loss = tf.constant(0.0)\n",
    "        text_length = self._length(self.input_text)\n",
    "\n",
    "        # 输入层【用embedding_lookup】\n",
    "        with tf.device('/cpu:0'), tf.name_scope(\"embedding\"):\n",
    "            self.W_text = tf.Variable(tf.random_uniform([vocab_size, word_embedding_size], -1.0, 1.0), name=\"W_text\")\n",
    "            self.embedded_chars = tf.nn.embedding_lookup(self.W_text, self.input_text)\n",
    "\n",
    "        # 【双向（左、右）循环结构】【使用tf的bidirectional_dynamic_rnn】\n",
    "        with tf.name_scope(\"bi-rnn\"):\n",
    "            fw_cell = self._get_cell(context_embedding_size, cell_type)\n",
    "            fw_cell = tf.nn.rnn_cell.DropoutWrapper(fw_cell, output_keep_prob=self.dropout_keep_prob)\n",
    "            bw_cell = self._get_cell(context_embedding_size, cell_type)\n",
    "            bw_cell = tf.nn.rnn_cell.DropoutWrapper(bw_cell, output_keep_prob=self.dropout_keep_prob)\n",
    "            (self.output_fw, self.output_bw), states = tf.nn.bidirectional_dynamic_rnn(cell_fw=fw_cell,\n",
    "                                                                                       cell_bw=bw_cell,\n",
    "\n",
    "        with tf.name_scope(\"context\"):\n",
    "            shape = [tf.shape(self.output_fw)[0], 1, tf.shape(self.output_fw)[2]]\n",
    "            self.c_left = tf.concat([tf.zeros(shape), self.output_fw[:, :-1]], axis=1, name=\"context_left\")\n",
    "            self.c_right = tf.concat([self.output_bw[:, 1:], tf.zeros(shape)], axis=1, name=\"context_right\")\n",
    "\n",
    "        with tf.name_scope(\"word-representation\"):\n",
    "            self.x = tf.concat([self.c_left, self.embedded_chars, self.c_right], axis=2, name=\"x\")\n",
    "            embedding_size = 2*context_embedding_size + word_embedding_size\n",
    "\n",
    "        with tf.name_scope(\"text-representation\"):\n",
    "            W2 = tf.Variable(tf.random_uniform([embedding_size, hidden_size], -1.0, 1.0), name=\"W2\")\n",
    "            b2 = tf.Variable(tf.constant(0.1, shape=[hidden_size]), name=\"b2\")\n",
    "            self.y2 = tf.tanh(tf.einsum('aij,jk->aik', self.x, W2) + b2)\n",
    "\n",
    "        with tf.name_scope(\"max-pooling\"):\n",
    "            self.y3 = tf.reduce_max(self.y2, axis=1)\n",
    "\n",
    "        with tf.name_scope(\"output\"):\n",
    "            W4 = tf.get_variable(\"W4\", shape=[hidden_size, num_classes], initializer=tf.contrib.layers.xavier_initializer())\n",
    "            b4 = tf.Variable(tf.constant(0.1, shape=[num_classes]), name=\"b4\")\n",
    "            l2_loss += tf.nn.l2_loss(W4)\n",
    "            l2_loss += tf.nn.l2_loss(b4)\n",
    "            self.logits = tf.nn.xw_plus_b(self.y3, W4, b4, name=\"logits\")\n",
    "            self.predictions = tf.argmax(self.logits, 1, name=\"predictions\")\n",
    "\n",
    "        # 计算平均交叉熵损失\n",
    "        with tf.name_scope(\"loss\"):\n",
    "            losses = tf.nn.softmax_cross_entropy_with_logits(logits=self.logits, labels=self.input_y)\n",
    "            self.loss = tf.reduce_mean(losses) + l2_reg_lambda * l2_loss\n",
    "\n",
    "        # 精确度\n",
    "        with tf.name_scope(\"accuracy\"):\n",
    "            correct_predictions = tf.equal(self.predictions, tf.argmax(self.input_y, axis=1))\n",
    "            self.accuracy = tf.reduce_mean(tf.cast(correct_predictions, tf.float32), name=\"accuracy\")\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_cell(hidden_size, cell_type):\n",
    "        if cell_type == \"vanilla\":\n",
    "            return tf.nn.rnn_cell.BasicRNNCell(hidden_size)\n",
    "        elif cell_type == \"lstm\":\n",
    "            return tf.nn.rnn_cell.BasicLSTMCell(hidden_size)\n",
    "        elif cell_type == \"gru\":\n",
    "            return tf.nn.rnn_cell.GRUCell(hidden_size)\n",
    "        else:\n",
    "            print(\"ERROR: '\" + cell_type + \"' is a wrong cell type !!!\")\n",
    "            return None\n",
    "\n",
    "    # 句子长度\n",
    "    @staticmethod\n",
    "    def _length(seq):\n",
    "        relevant = tf.sign(tf.abs(seq))\n",
    "        length = tf.reduce_sum(relevant, reduction_indices=1)\n",
    "        length = tf.cast(length, tf.int32)\n",
    "        return length\n",
    "\n",
    "    # 提取每个句子最后单元的输出\n",
    "    # 如：) The movie is good -> length = 4\n",
    "    #     output = [ [1.314, -3.32, ..., 0.98]\n",
    "    #                [0.287, -0.50, ..., 1.55]\n",
    "    #                [2.194, -2.12, ..., 0.63]\n",
    "    #                [1.938, -1.88, ..., 1.31]\n",
    "    #                [  0.0,   0.0, ...,  0.0]\n",
    "    #                ...\n",
    "    #                [  0.0,   0.0, ...,  0.0] ]\n",
    "    #     需要单元的第4个输出来提出它\n",
    "    @staticmethod\n",
    "    def last_relevant(seq, length):\n",
    "        batch_size = tf.shape(seq)[0]\n",
    "        max_length = int(seq.get_shape()[1])\n",
    "        input_size = int(seq.get_shape()[2])\n",
    "        index = tf.range(0, batch_size) * max_length + (length - 1)\n",
    "        flat = tf.reshape(seq, [-1, input_size])\n",
    "        return tf.gather(flat, index)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "RNN Recurrent（回路） Neural Network 循环神经网络。\n",
    "\n",
    "RNN的结构。循环神经网络的提出背景、优缺点。着重学习RNN的反向传播、RNN出现的问题（梯度问题、长期依赖问题）、BPTT算法。\n",
    "\n",
    "RNN的结构：\n",
    "传统神经网络有输入层、一个或多个隐藏层、输出层；\n",
    "相比之下，RNN循环神经网络，在隐层里多了一步递归，\n",
    "将结果输出一下层后，中间信息保存在记忆单元c中，\n",
    "将本次记忆的信息加在下一次传播过来的wx里，一起传给下一层。\n",
    "记忆单元c在整个传播过程中也是跟着每步更新的。\n",
    "\n",
    "提出背景：\n",
    "是为了解决前后传递的数据之间无关联的问题，\n",
    "比如NLP里，输入一行文字预测下一词时，我们前面如果有相关上下文背景会提高预测精度，减少运算。\n",
    "\n",
    "优点：\n",
    "RNN结构让机器学到了语序和上下文关联。比如做一个二制进加法操作，\n",
    "机器加本位的同时可以用前一位的记忆，学习到进位法则，自己学会运算。\n",
    "\n",
    "RNN出现的问题、缺点：\n",
    "记忆单元累积到最终输出时可能是大量的,有长期依赖问题，\n",
    "这些内容有一些距离远的失效信息增加了信息冗余；\n",
    "且在反向传播时，每一个记忆分支都要依次算梯度，增加计算复杂度，同时会出现梯度消失，\n",
    "之后的权重都变为，以后不会再被用算到；\n",
    "过多很小数累乘的结果本身也是约等于的，也能导致模型不能继续下去。\n",
    "\n",
    "RNN的反向传播：\n",
    "反身传播与传统神经网络一样，从结果值中取到值逐层逆向卷积算下降梯度，求导，取最小值，\n",
    "不同的是除了和正常传递层计算wx+b以外，还要和有存储信息的，记忆单元分支算梯度，\n",
    "合在一起链式法则向前回传。\n",
    "\n",
    "BPTT算法：\n",
    "Back Propagation Trough Time 基于时间的反向传播算法，\n",
    "\n",
    "双向RNN：\n",
    "双向RNN认为输出不仅依赖于序列之前的元素，也跟它之后的元素有关，\n",
    "这在序列挖掘中也是很常见的。\n",
    "\n",
    "递归神经网络：\n",
    "recursive neural network 递归神经网络是树状阶层拓扑结构，\n",
    "是RNN循环神经网络的升级，递归神经网络的每个父节点如果只有一个子节点时，\n",
    "则其结构等价全连接的循环神经网络。\n",
    "递归神经网络可以引入门控机制，设置记忆信息的阈值取舍，学习长距离依赖。\n",
    "灵活的拓扑还有权重共享，适用于包含结构关系的机器学习、深度学习任务，\n",
    "在NLP领域有重要应用。\n",
    "\n",
    "LSTM：\n",
    "Long Short Term Memory，可以解决上面出现的过远的信息长期依赖问题。又能学到学习到长期依赖关系。\n",
    "普通的RNN是每个模块内都加了一个tanh记忆层。\n",
    "LSTM每个循环的模块内又有4层结构:3个sigmoid层（forget，input，output），1个tanh层。\n",
    "sigmoid层用于做门单元，通过阈值控制参数来决定什么样的信息保留，超过多久之前的信息不保留之类的。\n",
    "\n",
    "GRU：\n",
    "Gated Recurrent Unit，是LSTM的一个变体。\n",
    "GRU只有两个门（update和reset），LSTM有三个门（forget，input，output）。\n",
    "LSTM的input gate负责控制new memory，输入信息，\n",
    "forget gate负责控制上一轮的memory，长期信息，\n",
    "output gate对前两者的激活信息进行控制，输出h，当前隐藏层信息。\n",
    "GRU的reset gate直接在new memory处对上一层的隐层信息进行处理，控制h(t-1)的信息存在，\n",
    "所以这里update gate是对输入信息的控制得到当前层隐层状态。\n",
    "GRU 参数相对少更容易收敛，但是在数据集较大的情况下，LSTM性能更好。\n",
    "LSTM还有许多变体，但不管是何种变体,，重点在于额外的门控机制是如何设计，用以控制梯度信息传播从而缓解梯度消失现象。\n",
    "\n",
    "针对梯度消失（LSTM等其他门控RNN）、梯度爆炸（梯度截断）的解决方案：\n",
    "梯度消失的解决方案：\n",
    "可以用LSTM门控RNN缓解梯度消失，控制信息长期依赖；\n",
    "初始化矩阵时不用随机矩阵，用单位矩阵；\n",
    "激活函数用Relu，PReLU；\n",
    "\n",
    "梯度爆炸（梯度截断）的解决方案：\n",
    "梯度爆炸是在断涯式数据处求梯度不稳定的现象。\n",
    "RNN中可以用梯度裁剪gradient clipping避免，\n",
    "在迭代中各权重的梯度平方和如果大于某个阈值，为了避免权重的变化值太大，\n",
    "求一个缩放因子（阈值/平方和），将所有梯度乘以这个因子；\n",
    "添加正则项。\n",
    "\n",
    "Memory Network：\n",
    "Memory Network，记忆网络出现之前，大多数机器学习的模型都缺乏可以读取和写入外部知识的组件，\n",
    "例如，给定一系列事实或故事，然后要求回答关于该主题的问题。\n",
    "RNN虽然可以被训练在阅读了一串文字之后用来预测下一个输出，但记忆太小，不能精确记住过去的事实。\n",
    "一个Memory Network由一个记忆数组m（一个向量的数组或者一个字符串数组，index by i）\n",
    "和四个组件（输入I，泛化G，输出O，回答R）组成。\n",
    "I :（输入特征映射） - 将输入转换为记忆网络内部特征的表示。给定输入x，可以是字符、单词、句子等不同的粒度，通过I(x)得到记忆网络内部的特征。\n",
    "G :（更新记忆） - 使用新的输入更新记忆数组m。 mi=G(mi,I(x),m)\n",
    "O：（输出） - 在记忆数组m更新完以后，就可以将输入和记忆单元联系起来，根据输入选择与之相关的记忆单元。 o=O(I(x),m)\n",
    "R：（输出回答） - 得到了输入编码向量I(x)，记忆数组m和需要的支持事实，就可以根据问题来得到需要的答案了。\n",
    "\n",
    "参考：https://www.cnblogs.com/zhangchaoyang/articles/6684906.html\n",
    "https://blog.csdn.net/sinat_33741547/article/details/82821782\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Text-RNN的原理。\n",
    "基于RNN的文本分类模型。\n",
    "TextCNN擅长捕获短的序列信息，\n",
    "TextRNN擅长捕获更长的序列信息。\n",
    "具体到文本分类任务中，BiLSTM可以理解为可以捕获变长且双向的N-Gram信息。\n",
    "文本分类也可以理解为一种特殊的Seq2Seq模型\n",
    "Seq2Seq模型的标配是引入了注意力机制[Attention]能够很好的给出每个词对结果的贡献程度，\n",
    "注意力机制的引入，可以在某种程度上提高深度学习文本分类模型的可解释性。\n",
    "TextRNN的结构非常灵活，可以任意改变。\n",
    "比如把LSTM单元替换为GRU单元，把双向改为单向，添加dropout或BatchNormalization以及再多堆叠一层等等。\n",
    "TextRNN在文本分类任务上的效果非常好，与TextCNN不相上下，但RNN的训练速度相对偏慢，一般2层就已经足够多了。\n",
    "\n",
    "Recurrent Convolutional Neural Networks（RCNN）原理:\n",
    "双向循环神经网络，RCNN可以较均匀的利用单词的上下文信息，\n",
    "既可以缓解在RNN中后面的单词比前面的单词影响力更大的缺点，\n",
    "也不需要像CNN一样需要通过窗口大小来设定对上下文的依赖长度。\n",
    "每一个单词的embedding方式主要有3个部分concat组成：\n",
    "left context ;\n",
    "单词本身的embedding;\n",
    "righ context，\n",
    "w代表单词。\n",
    "单词本身的embedding，使用了Skip-gram的方法进行预训练。\n",
    "工作流程：\n",
    "假定单词w先经过1层双向LSTM，\n",
    "该词的左侧的词正向输入进去得到一个词向量，\n",
    "该词的右侧反向输入进去得到一个词向量。\n",
    "再结合该词的词向量，生成一个 1 * 3k 的向量。\n",
    "再经过全连接层，tanh为非线性函数，得到y2。\n",
    "再经过最大池化层，得出最大化向量y3.\n",
    "再经过全连接层，sigmod为非线性函数，得到最终的多分类。\n",
    "\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
