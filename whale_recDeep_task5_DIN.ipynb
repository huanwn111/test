{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==深度推荐DIN==\n",
    "#一、背景\n",
    "# Deep Interest Network(DIIN) 2018年 阿里巴巴提出。\n",
    "# 模型基于业务的观察，从实际应用的角度进行改进，相比于之前很多“学术风”的深度模型， 该模型更加具有业务气息。\n",
    "# 注重用户的历史行为特征（历史购买过的商品或者类别信息）。\n",
    "# 推荐任务一般有着差不多的固定处理套路，就是大量稀疏特征先经过embedding层， 转成低维稠密的，然后进行拼接，最后喂入到多层神经网络中去。\n",
    "# 蛮力拼接，然后就各种交叉等。无法表达用户【广泛的兴趣】。\n",
    "# 例如，假设广告中的商品是键盘，用户历史商品中有鼠标， 化妆品， T-shirt和洗面奶， 鼠标这个商品embedding对预测“键盘”广告的点击率的重要程度应该大于后面的那三个。\n",
    "# 之前的那些深度学习模型，是没法很好的去表达出用户这广泛多样的兴趣的，如果想表达的准确些， 那么就得加大隐向量的维度，让每个特征的信息更加丰富，增加计算量。\n",
    "# DIN的巧妙之处在于--【用户的历史行为商品这块重点考虑与当前商品广告的一个关联性。】\n",
    "# 如何做到的？\n",
    "# “注意力”的思想，\n",
    "# 把注意力引入到了模型，设计了一个\"local activation unit\"结构，\n",
    "# 利用候选商品和历史问题商品之间的相关性计算出权重，这个就代表了对于当前商品广告的预测，用户历史行为的各个商品的重要程度大小。\n",
    "# 是一个加入了注意力权重的深度学习网络。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 二、DIN模型结构与原理\n",
    "\n",
    "# 前言，一般模型的数据集和特征表示：\n",
    "# 2.1 特征表示\n",
    "# 一般是通过one-hot的形式对特征编码， 转成系数的二值特征的形式，\n",
    "# DIN里有一个visted_cate_ids，也就是用户的历史商品列表， 对于某个用户来讲，这个值是个多值型的特征， 而且还要知道这个特征的长度不一样长，也就是用户购买的历史商品个数不一样多，\n",
    "# 这个特征，一般是用到multi-hot编码，也就是可能不止1个1了，有哪个商品，对应位置就是1，\n",
    "# 再喂入模型，\n",
    "# 注意一点 就是上面的特征里面没有任何的交互组合，也就是没有做特征交叉。这个交互信息交给后面的神经网络去学习。\n",
    "# 2.2 基线模型\n",
    "# base 模型，就是上面提到过的Embedding&MLP的形式，\n",
    "# 在这个基础上添加一个新结构(注意力网络)来学习当前候选广告与用户历史行为特征的相关性，从而动态捕捉用户的兴趣。\n",
    "# 基准模型的结构，前面也一直用这个基准， 分为三大模块：Embedding layer，Pooling & Concat layer和MLP，\n",
    "# 1、Embedding layer：\n",
    "# 作用是把高维稀疏的输入转成低维稠密向量， 每个离散特征下面都会对应着一个embedding词典，\n",
    "# 其他离散特征也是同理，只不过上面那个multi-hot编码的那个，会得到一个embedding向量的列表，因为他开始的那个multi-hot向量不止有一个是1，这样乘以embedding矩阵，就会得到一个列表了。通过这个层，上面的输入特征都可以拿到相应的稠密embedding向量了。\n",
    "# //就是embedding矩阵，直接把embedding矩阵中one-hot向量为1的那个位置的embedding向量拿出来。 这样就得到了稀疏特征的稠密向量了。\n",
    "# 2、pooling layer and Concat layer：\n",
    "# pooling层的作用是将用户的历史行为embedding这个最终变成一个定长的向量，因为每个用户历史购买的商品数是不一样的， 也就是每个用户multi-hot中1的个数不一致，这样经过embedding层，得到的用户历史行为embedding的个数不一样多，也就是上面的embedding列表不一样长， 那么这样的话，每个用户的历史行为特征拼起来就不一样长了。\n",
    "# 公式里的K表示对应历史特种组里面用户购买过的商品数量，也就是历史embedding的数量，看上面图里面的user behaviors系列。\n",
    "#  Concat layer层的作用就是拼接了，就是把这所有的特征embedding向量，如果再有连续特征的话也算上，从特征维度拼接整合，作为MLP的输入。\n",
    "# 3、MLP：这个就是普通的全连接，用了学习特征之间的各种交互。\n",
    "# 4、Loss: 由于这里是点击率预测任务， 二分类的问题，所以这里的损失函数用的负的log对数似然\n",
    "    \n",
    "# Base模型的问题：\n",
    "# 用户的历史行为特征和当前的候选广告特征在全都拼起来给神经网络之前，是一点交互的过程都没有， 而拼起来之后给神经网络，虽然是有了交互了，但是原来的一些信息，比如，每个历史商品的信息会丢失了一部分，因为这个与当前候选广告商品交互的是池化后的历史特征embedding， 这个embedding是综合了所有的历史商品信息， 这个通过我们前面的分析，对于预测当前广告点击率，并不是所有历史商品都有用，综合所有的商品信息反而会增加一些噪声性的信息，可以联想上面举得那个键盘鼠标的例子，如果加上了各种洗面奶，衣服啥的反而会起到反作用。其次就是这样综合起来，已经没法再看出到底用户历史行为中的哪个商品与当前商品比较相关，也就是丢失了历史行为中各个商品对当前预测的重要性程度。最后一点就是如果所有用户浏览过的历史行为商品，最后都通过embedding和pooling转换成了固定长度的embedding，这样会限制模型学习用户的多样化兴趣。\n",
    "\n",
    "# 改进思路？-DIN\n",
    "# 第一个就是加大embedding的维度，增加之前各个商品的表达能力，这样即使综合起来，embedding的表达能力也会加强， 能够蕴涵用户的兴趣信息，但是这个在大规模的真实推荐场景计算量超级大，不可取。\n",
    "# 另外一个思路就是在【当前候选广告和用户的历史行为之间引入注意力的机制】，这样在预测当前广告是否点击的时候，让模型【更关注于与当前广告相关的那些用户历史产品，也就是说与当前商品更加相关的历史行为更能促进用户的点击行为。】\n",
    "# 第二个思路就是DIN的改进，通过给定一个候选广告，然后去注意与该广告相关的局部兴趣的表示来模拟此过程。\n",
    "# DIN不会通过使用同一向量来表达所有用户的不同兴趣，而是通过考虑历史行为的相关性来自适应地计算用户兴趣的表示向量（对于给的广告）。 该表示向量随不同广告而变化。\n",
    "\n",
    "# 2.3 DIN模型架构\n",
    "\n",
    "# 它依然是采用了基模型的结构，只不过是在这个的基础上加了一个注意力机制来学习用户兴趣与当前候选广告间的关联程度，\n",
    "# 就是引入local activation unit， 能够根据用户历史行为特征和当前广告的相关性给用户历史行为特征embedding进行加权。\n",
    "# local activation unit这里面是一个前馈神经网络，输入是用户历史行为商品和当前的候选商品， ， 输出是它俩之间的相关性， 这个相关性相当于每个历史商品的权重，把这个权重与原来的历史行为embedding相乘求和就得到了用户的兴趣表示，\n",
    "# ，输入除了历史行为向量和候选广告向量外，还加了一个它俩的外积操作，作者说这里是有利于模型相关性建模的显性知识。\n",
    "# 注意，就是这里的权重加和不是1， 准确的说这里不是权重， 而是直接算的相关性的那种分数作为了权重，也就是平时的那种scores(softmax之前的那个值)，这个是为了保留用户的兴趣强度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#三、DIN实现\n",
    "# 代码解释：\n",
    "# 不同的用户这种历史行为特征长度会不一样， 但是我们的神经网络是要求序列等长的，所以这种情况我们一般会按照最长的序列进行padding的操作(不够长的填0)， 而到具体层上进行运算的时候，会用mask掩码的方式标记出这些填充的位置，好保证计算的准确性。 \n",
    "# DIN模型的输入特征大致上分为了三类： Dense(连续型), Sparse(离散型), VarlenSparse(变长离散型)，也就是指的上面的历史行为数据。\n",
    "# 处理的方式不同：\n",
    "# Dense型特征：由于是数值型了，这里为每个这样的特征建立Input层接收这种输入， 然后拼接起来先放着，等离散的那边处理好之后，和离散的拼接起来进DNN。\n",
    "# Sparse型特征：为离散型特征建立Input层接收输入，然后需要先通过embedding层转成低维稠密向量，然后拼接起来放着，等变长离散那边处理好之后， 一块拼起来进DNN， 但是这里面要注意有个特征的embedding向量还得拿出来用，就是候选商品的embedding向量，这个还得和后面的计算相关性，对历史行为序列加权。\n",
    "# VarlenSparse型特征：这个一般指的用户的历史行为特征，变长数据， 首先会进行padding操作成等长， 然后建立Input层接收输入，然后通过embedding层得到各自历史行为的embedding向量， 拿着这些向量与上面的候选商品embedding向量进入AttentionPoolingLayer去对这些历史行为特征加权合并，最后得到输出。\n",
    "# 通过上面的三种处理， 就得到了处理好的连续特征，离散特征和变长离散特征， 接下来把这三种特征拼接，进DNN网络，得到最后的输出结果即可。\n",
    "\n",
    "# DIN网络搭建\n",
    "def DIN(feature_columns, behavior_feature_list, behavior_seq_feature_list):\n",
    "    \"\"\"\n",
    "    这里搭建DIN网络，有了上面的各个模块，这里直接拼起来\n",
    "    :param feature_columns: A list. 里面的每个元素是namedtuple(元组的一种扩展类型，同时支持序号和属性名访问组件)类型，表示的是数据的特征封装版\n",
    "    :param behavior_feature_list: A list. 用户的候选行为列表\n",
    "    :param behavior_seq_feature_list: A list. 用户的历史行为列表\n",
    "    \"\"\"\n",
    "    # 构建Input层并将Input层转成列表作为模型的输入\n",
    "    input_layer_dict = build_input_layers(feature_columns)\n",
    "    input_layers = list(input_layer_dict.values())\n",
    "    \n",
    "    # 筛选出特征中的sparse和Dense特征， 后面要单独处理\n",
    "    sparse_feature_columns = list(filter(lambda x: isinstance(x, SparseFeat), feature_columns))\n",
    "    dense_feature_columns = list(filter(lambda x: isinstance(x, DenseFeat), feature_columns))\n",
    "    \n",
    "    # 获取Dense Input\n",
    "    dnn_dense_input = []\n",
    "    for fc in dense_feature_columns:\n",
    "        dnn_dense_input.append(input_layer_dict[fc.name])\n",
    "    \n",
    "    # 将所有的dense特征拼接\n",
    "    dnn_dense_input = concat_input_list(dnn_dense_input)   # (None, dense_fea_nums)\n",
    "    \n",
    "    # 构建embedding字典\n",
    "    embedding_layer_dict = build_embedding_layers(feature_columns, input_layer_dict)\n",
    "\n",
    "    # 离散的这些特特征embedding之后，然后拼接，然后直接作为全连接层Dense的输入，所以需要进行Flatten\n",
    "    dnn_sparse_embed_input = concat_embedding_list(sparse_feature_columns, input_layer_dict, embedding_layer_dict, flatten=True)\n",
    "    \n",
    "    # 将所有的sparse特征embedding特征拼接\n",
    "    dnn_sparse_input = concat_input_list(dnn_sparse_embed_input)   # (None, sparse_fea_nums*embed_dim)\n",
    "    \n",
    "    # 获取当前行为特征的embedding， 这里有可能有多个行为产生了行为列表，所以需要列表将其放在一起\n",
    "    query_embed_list = embedding_lookup(behavior_feature_list, input_layer_dict, embedding_layer_dict)\n",
    "    \n",
    "    # 获取历史行为的embedding， 这里有可能有多个行为产生了行为列表，所以需要列表将其放在一起\n",
    "    keys_embed_list = embedding_lookup(behavior_seq_feature_list, input_layer_dict, embedding_layer_dict)\n",
    "    # 使用注意力机制将历史行为的序列池化，得到用户的兴趣\n",
    "    dnn_seq_input_list = []\n",
    "    for i in range(len(keys_embed_list)):\n",
    "        seq_embed = AttentionPoolingLayer()([query_embed_list[i], keys_embed_list[i]])  # (None, embed_dim)\n",
    "        dnn_seq_input_list.append(seq_embed)\n",
    "    \n",
    "    # 将多个行为序列的embedding进行拼接\n",
    "    dnn_seq_input = concat_input_list(dnn_seq_input_list)  # (None, hist_len*embed_dim)\n",
    "    \n",
    "    # 将dense特征，sparse特征， 即通过注意力机制加权的序列特征拼接起来\n",
    "    dnn_input = Concatenate(axis=1)([dnn_dense_input, dnn_sparse_input, dnn_seq_input]) # (None, dense_fea_num+sparse_fea_nums*embed_dim+hist_len*embed_dim)\n",
    "    \n",
    "    # 获取最终的DNN的预测值\n",
    "    dnn_logits = get_dnn_logits(dnn_input, activation='prelu')\n",
    "    \n",
    "    model = Model(inputs=input_layers, outputs=dnn_logits)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 四、思考\n",
    "# DIN模型在工业上的应用还是比较广泛的， 大家可以自由去通过查资料看一下具体实践当中这个模型是怎么用的？ 有什么问题？比如行为序列的制作是否合理， 如果时间间隔比较长的话应不应该分一下段？ 再比如注意力机制那里能不能改成别的计算注意力的方式会好点？(我们也知道注意力机制的方式可不仅DNN这一种)， 再比如注意力权重那里该不该加softmax？ 这些其实都是可以值的思考探索的一些问题，根据实际的业务场景，大家也可以总结一些更加有意思的工业上应用该模型的技巧和tricks，欢迎一块讨论和分享。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
