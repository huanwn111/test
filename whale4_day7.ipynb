{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'''\n",
    "Task6 简单神经网络  \n",
    "\n",
    "1. 文本表示：从one-hot到word2vec。\n",
    "1.1 词袋模型：离散、高维、稀疏。\n",
    "1.2 分布式表示：连续、低维、稠密。word2vec词向量原理并实践，用来表示文本。\n",
    "\n",
    "2. 走进FastText\n",
    "2.1 FastText的原理。\n",
    "2.2 利用FastText模型进行文本分类。\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "1. 文本表示：从one-hot到word2vec。\n",
    "one-hot:\n",
    "one-hot独热向量是把文字等符号用数字表示的一种编码，如表示特征是否出现用分别用1 0 。\n",
    "比如我们在几行分好词的文本找出不重复（可以用set去重）的词做特征，分别看每行中是否有这个词，\n",
    "有，则该特征列标1；没有，则标0。\n",
    "它可以表示词集向量。\n",
    "比如数据预处理时，将非数值列特征二次提取特征变成one-hot编码向量化，可以进行训练。\n",
    "\n",
    "1.1 词袋模型：离散、高维、稀疏。\n",
    "【词袋模型】：\n",
    "BOW，bag of words,将文本看作是词的集合，忽略词序、语法、句法。\n",
    "文本特征提取有两个非常重要的模型：词集模型，词袋模型。\n",
    "两者对比：\n",
    "   词集模型：由词构成的集合，集合每个元素只有一个，即，词是唯一不重复的。\n",
    "   词袋模型：在词集基础上，如果一个词多次出现，统计它的次数（频数）。\n",
    "   它在词集基础上增加了频率的维度，\n",
    "   词集关注有没有，词袋还关注有几个。\n",
    "   对一篇文章进行特征化，最常见的方式就是词袋。\n",
    "\n",
    "'''\n",
    "#图片也可以表式为词袋模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "   对分好词的分词对象进行词袋化处理的方法：\n",
    "'''\n",
    "\n",
    "#引库： \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "#实例化对象 ：\n",
    "vec  = CountVectorizer()\n",
    "\n",
    "text = [\n",
    "   '这 里 是 分 好 词 的 文本',\n",
    "   '注意 每段 文字 内 是 空格 间隔 的 格式',\n",
    "   '所以 中文 要 先 分词 ，可以 用 jieba 分词'\n",
    "]\n",
    "#传入上面定义的要分的文本进行分词：\n",
    "x = vec.fit_transform(text)#【fit_trasform有返回值，fit无返回值，结果携带在vec里里】\n",
    "\n",
    "#词袋化这就完成了\n",
    "\n",
    "#打印词数据：\n",
    "print(x)#\n",
    "'''\n",
    "(0, 6) 1 # 前面三位是：第几行样本，特征编号，索引 \n",
    "(1, 7) 1\n",
    "(1,11) 1\n",
    "(1,10) 1\n",
    "……\n",
    "'''\n",
    "print(x.toarray())#以数组的方式显示\n",
    "'''\n",
    "[[0 0 0 0 0 0 1 0 0 0 0 0] #第一行样本，编号索引为6号的特征在这行，标1，其它不在，标0。\n",
    " [0 0 0 0 0 1 0 1 1 1 1 1] #第二行样本\n",
    " [1 1 2 1 1 0 0 0 0 0 0 0]]\n",
    " #【注意：这里的数字也是一种one-hot编码。即用散点的不相关的向量单个的表示一个特征】\n",
    "'''\n",
    "#显示查看特征名称\n",
    "vec.vocabulary_#字内格式，特征名称和其编号\n",
    "'''\n",
    "{'文本': 6,'注意': 9,'每段': 8,'文字': 5,'空格': 10,'间隔': 11,\n",
    "'格式': 7,'所以': 4,'中文': 1,'分词': 2,'可以': 3,'jieba': 0}\n",
    "'''\n",
    "#vec.get_feature_names()#数组格式，只有词（特征），没有编号\n",
    "\n",
    "'''\n",
    "   还有一种特征提取方法TF-IDF (词频与逆向文件频率)\n",
    "   常和词袋模型搭配使用:\n",
    "'''\n",
    "#它可以把词袋模型的0 1 2单纯次数这种整数表式的矩阵 处理成带有权重区分的显示方式\n",
    "#使用TfidfTransformer 实例化 fit数据即可\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "trans = TfidfTransformer()\n",
    "tfidf = trans.fit_transform(x)\n",
    "print(tfidf.toarray())\n",
    "'''\n",
    "[[0.         0.         0.         0.         0.         0.\n",
    "  1.         0.         0.         0.         0.         0.        ]\n",
    " [0.         0.         0.         0.         0.         0.40824829\n",
    "  0.         0.40824829 0.40824829 0.40824829 0.40824829 0.40824829]\n",
    " [0.35355339 0.35355339 0.70710678 0.35355339 0.35355339 0.\n",
    "  0.         0.         0.         0.         0.         0.        ]]\n",
    "#【虽然带了权重，但这种编码仍然是one-hot编码，\n",
    "#词袋模型就是用one-hot编码表示词向量的，词与词之间不考虑语序、上下文，散点高维稀疏的】\n",
    "'''\n",
    "#上面 CountVectorizer 统计词频 和 TfidfTransformer 统计权重词频 \n",
    "#可以两步合一步，用TfidfVectorizer代替，一步完成从文字到权重词频向量的转换"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "离散、高维、稀疏：\n",
    "\n",
    "词袋的特征词是互不相干，忽略语序的，无意义的独立点，不连续的，离散状态；\n",
    "词袋向量的维度等于词典的维度，词典中词的个数，\n",
    "（即所处理文本中不重复的词的个数，特征列数，\n",
    "例：上文中矩阵的列12列对应12个不重复特征词）\n",
    "所以词袋向量的特点是高维，可能会出现很多的特征词，很多维；\n",
    "稀疏矩阵：为0的元素远远少于非0的，即有特征出现的点散布的。\n",
    "稠密矩阵：与上面相反，为0的元素多，非0的少，即，有意义的点稠密的集中在一起。\n",
    "词袋模型因为常用词特征虽然也不少，是高维的，但组成的句子、篇章是更巨大量的。\n",
    "所以词袋模型抽取的特征词对应的样本中出现的次数很少有不存在为0的情况，它有稀疏性。\n",
    "高维稀疏性的好处是，方便区分（体量大，都是分散的，在里会找东西好找），\n",
    "常用于场景识别，回环检测。\n",
    "\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "1.2 分布式表示：连续、低维、稠密。word2vec词向量原理并实践，用来表示文本。\n",
    "\n",
    "NNLM模型（Nerual Network Language Model 神经网络模型）：\n",
    "\n",
    "NNLM可以把Onehot表示的离散、高维、稀疏向量矩阵投射成连续的、低维、稠密矩阵，\n",
    "和一般的神经网络不一样，它在输入层后面加了一个投影层C层，后面还是正常的多个隐藏层、输出层\n",
    "转换原理：输入层输入的是One-hot独热向量代表的词矩阵（很多维，比如上万级十万级），\n",
    "在投影层设定一个低维的矩阵（比如500维，即500列），与输入层传过来的矩阵做矩阵乘（列行对乘再相加），\n",
    "经过这样结果就变为低维（500维）矩阵了，这一操作相当于对前面的高维矩阵的切割，\n",
    "结果是每位值来说原来是0的位，0乘任何矩阵行结果都是0，就过滤掉，\n",
    "变稠密、低维向量继续下一层传播，经过激活函数（比如sigmoid softmax）\n",
    "把离散值转为连续的概率值，最终输出属于每种分类情况下的得分概率，\n",
    "再将结果回传，反正传播，用梯度下降迭代使损失最小，达到优化C层词向量矩阵的目的，\n",
    "#注意这里和一般的神经网络不同，它们要的是结果，比如用最优的C参代入最终再求一个精度高的分类结果，\n",
    "#nlp这里要的就是这个C，优化后的词库里的词向量。\n",
    "\n",
    "word2vec词向量原理:\n",
    "word2vec(word to vector)，是一种将词转为词向量的神经网络模型，\n",
    "它的原理借鉴了上面的nnlm，\n",
    "但简化了：\n",
    "输入层输入的直接使用低维稠密向量（传入单个词的词）；\n",
    "投影层省去了矩阵乘，简化为累加操作（把词向量累加）；\n",
    "中间层也随时有反向传播回传优化；\n",
    "采用双向上下文窗口（类似于滑动广告屏），每次只看要观察的词前后的词（即和它关系最近的，有关系的），\n",
    "用周围的词定义自已，不看上下文（所以不用大量传入One-hot高维稀疏矩阵）；\n",
    "\n",
    "word2vec优点：\n",
    "计算简单，高效，度量词相似性很好（它学习到了词之间的关系，内在含义），应用广泛。\n",
    "\n",
    "word2vec两种模型：\n",
    "CBOW(Continuous Bag-of-Words 连续性词袋)：\n",
    "这种是用周围的词推断下一个可能的词（即根据当前词周围的词预测其出现的概率）。\n",
    "Skip-gram：这种是从中间词推断它周围可能的词（算周围词出现的概率）。\n",
    "\n",
    "word2vec的优化：\n",
    "也借鉴了哈夫曼树，\n",
    "只计算非叶子节点词的贡献，不计算整个输出层，计算量下降成树的深度\n",
    "判断权重最大的离根节点最近排列的筛选思路。\n",
    "\n",
    "负例采样:\n",
    "比如哈夫曼树是通过只是走正例的分支来简化直达路径，\n",
    "负例采样的思想是负例也采，采到以后判断是负例则用1减去负例，即为正例，提高了计算效率。\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['今天', '是', '星期三'], ['明天', '是', '星期四'], ['后天', '是', '星期五'], ['自然语言处理', '是', '人工智能', '深度学习', '的', '一个', '分支'], ['图象识别', '是', '人工智能', '深度学习', '的', '一个', '分支']]\n",
      "Word2Vec(vocab=14, size=100, alpha=0.025)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G:\\Users\\Administrator\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:25: DeprecationWarning: Call to deprecated `similarity` (Method will be removed in 4.0.0, use self.wv.similarity() instead).\n",
      "G:\\Users\\Administrator\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:26: DeprecationWarning: Call to deprecated `similarity` (Method will be removed in 4.0.0, use self.wv.similarity() instead).\n",
      "G:\\Users\\Administrator\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:27: DeprecationWarning: Call to deprecated `similarity` (Method will be removed in 4.0.0, use self.wv.similarity() instead).\n",
      "G:\\Users\\Administrator\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:29: DeprecationWarning: Call to deprecated `similarity` (Method will be removed in 4.0.0, use self.wv.similarity() instead).\n",
      "G:\\Users\\Administrator\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:30: DeprecationWarning: Call to deprecated `similarity` (Method will be removed in 4.0.0, use self.wv.similarity() instead).\n",
      "G:\\Users\\Administrator\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:31: DeprecationWarning: Call to deprecated `similarity` (Method will be removed in 4.0.0, use self.wv.similarity() instead).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.11737931"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#实践word2vec，用来表示文本\n",
    "#用到gensim库 pip install gensim\n",
    "from gensim.models import word2vec\n",
    "#word2vec小写\n",
    "\n",
    "#模拟一个语料库，不同语料有些相关词，有些不相关。以空格分好的句子，原始中文语料可用jieba分\n",
    "corpus = [\"今天 是 星期三\",\n",
    "          \"明天 是 星期四\",\n",
    "         \"后天 是 星期五\",\n",
    "         \"自然语言处理 是 人工智能 深度学习 的 一个 分支\",\n",
    "         \"图象识别 是 人工智能 深度学习 的 一个 分支\"]\n",
    "texts = [tt.split() for tt in corpus]\n",
    "#把句子分成【单个词列表】，\n",
    "#因为word2vec是传入的单个词向量，滑动窗口式输入，非词袋批量输入\n",
    "print(texts)\n",
    "\n",
    "#建模\n",
    "#实例化word2vec模型 输入词\n",
    "model = word2vec.Word2Vec(texts,min_count=1)\n",
    "#参数：输入词列表；min_count过滤低频词；size神经网络层数，默认100\n",
    "print(model)\n",
    "\n",
    "#查看词语相似度\n",
    "#相关度高的词（周围出现相似词句）\n",
    "model.similarity('今天','明天')#-0.15154815\n",
    "model.similarity('今天','后天')#-0.14681846\n",
    "model.similarity('星期五','星期三')#0.11737931\n",
    "model.similarity('自然语言处理','图象识别')#0.11322209\n",
    "#相关度低的词（周围词分布毫不相关的）\n",
    "model.similarity('今天','图象识别')#0.032949924\n",
    "model.similarity('分支','星期三')#-0.0065055043\n",
    "\n",
    "#【由上面得分可知，机器根据相同句周围的词，学习到了这些词之间的关系远近\n",
    "#满有意思的吧！～～】\n",
    "#实际训练模型时可使用维基百科语料丰富"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "2.走进FastText \n",
    "2.1 FastText的原理\n",
    "\n",
    "fastText 是 Facebook 16年开发的用于高效学习单词呈现和语句分类的开源库。\n",
    "它是word2vec的延伸，借鉴了Glove。\n",
    "Glove原理：通过算共现概率来细化词与词之间的关联。\n",
    "共现概率算法：即一个词出现在它周围词的10词以内算共现一次。\n",
    "共现概率公式：Pij = Xij/Xi，即词j在词i的上下文中出现的次数占词i总上下文次数的比值。\n",
    "举例说明：solid出现在ice旁边的概率/solid出现在steam附近的概率。比值越小越相关。\n",
    "即借助第三词k来来探究i,j两词的关系。\n",
    "再比如通过man - woman可知 son - ? 对daughter.\n",
    "通过这个关联，找到关系最近的词。\n",
    "fastText借鉴了上面思想，\n",
    "改进了word2vec 在负例采样的skip-gram（由中心词测周围词）模型中，\n",
    "将每个中心词视为子词的集合。并学习子词的词向量。\n",
    "比如where一词，找到它包含的的3-6个子词组成一个合集 \"<wh\",\"whe\",\"ere\", \"re>\"\n",
    "<>代表前后辍。\n",
    "这样做就学到了词辍的意义比如un代表否定，cardio代表心脏相关，\n",
    "细化到子词的学习，不会把有关系的同词根的两个词当成不相关的。\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "2.2 利用FastText模型进行文本分类。\n",
    "'''\n",
    "#【\n",
    "#pip install fasttext失败\n",
    "#下载fasttext-0.8.22-cp36-cp36m-win_amd64.whl\n",
    "#下载地址https://www.lfd.uci.edu/~gohlke/pythonlibs/#fasttext\n",
    "#放到anaconda下，如：G:\\Users\\Administrator\\Anaconda3\\Lib\\site-packages\n",
    "#打开anaconda的prompt\n",
    "#输入pip install G:\\Users\\Administrator\\Anaconda3\\Lib\\site-packages\\fasttext-0.8.22-cp36-cp36m-win_amd64.whl\n",
    "#】\n",
    "# _*_coding:utf-8 _*_\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "import fastText.FastText as ff\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(103369, 0.9214851647979568, 0.9214851647979568)\n"
     ]
    }
   ],
   "source": [
    "#训练模型\n",
    "classifier = ff.train_supervised(\"news_fasttext_train.txt\")\n",
    "model2 = classifier.save_model('news_fasttext_model.model') # 保存模型\n",
    "test2 = classifier.test('news_fasttext_test.txt') # 输出测试结果\n",
    "classifier.get_labels() # 输出标签\n",
    "pre = classifier.predict('文本') #输出改文本的预测结果\n",
    "print(test2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#【上面是自己安装fasttext库，也可以使用gensim库的fastText】\n",
    "#下面看下使用\n",
    "from gensim.models import FastText\n",
    "from gensim.test.utils import common_texts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['human', 'interface', 'computer'], ['survey', 'user', 'computer', 'system', 'response', 'time'], ['eps', 'user', 'interface', 'system'], ['system', 'human', 'system', 'eps'], ['user', 'response', 'time'], ['trees'], ['graph', 'trees'], ['graph', 'minors', 'trees'], ['graph', 'minors', 'survey']]\n"
     ]
    }
   ],
   "source": [
    "#打印库自带的数据看下格式，是列表格式\n",
    "print(common_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-03-13 20:13:07,113 : INFO : resetting layer weights\n",
      "2019-03-13 20:13:07,115 : INFO : Total number of ngrams is 0\n",
      "2019-03-13 20:13:07,117 : INFO : collecting all words and their counts\n",
      "2019-03-13 20:13:07,119 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2019-03-13 20:13:07,120 : INFO : collected 14 word types from a corpus of 23 raw words and 5 sentences\n",
      "2019-03-13 20:13:07,121 : INFO : Loading a fresh vocabulary\n",
      "2019-03-13 20:13:07,123 : INFO : effective_min_count=1 retains 14 unique words (100% of original 14, drops 0)\n",
      "2019-03-13 20:13:07,124 : INFO : effective_min_count=1 leaves 23 word corpus (100% of original 23, drops 0)\n",
      "2019-03-13 20:13:07,125 : INFO : deleting the raw counts dictionary of 14 items\n",
      "2019-03-13 20:13:07,127 : INFO : sample=0.001 downsamples 14 most-common words\n",
      "2019-03-13 20:13:07,128 : INFO : downsampling leaves estimated 2 word corpus (12.8% of prior 23)\n",
      "2019-03-13 20:13:07,129 : INFO : estimated required memory for 14 words, 81 buckets and 4 dimensions: 10080 bytes\n",
      "2019-03-13 20:13:07,131 : INFO : resetting layer weights\n",
      "2019-03-13 20:13:07,134 : INFO : Total number of ngrams is 81\n",
      "2019-03-13 20:13:07,135 : INFO : training model with 3 workers on 14 vocabulary and 4 features, using sg=0 hs=0 sample=0.001 negative=5 window=3\n",
      "2019-03-13 20:13:07,163 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-03-13 20:13:07,166 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-13 20:13:07,167 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-13 20:13:07,167 : INFO : EPOCH - 1 : training on 23 raw words (2 effective words) took 0.0s, 355 effective words/s\n",
      "2019-03-13 20:13:07,176 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-03-13 20:13:07,180 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-13 20:13:07,180 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-13 20:13:07,181 : INFO : EPOCH - 2 : training on 23 raw words (2 effective words) took 0.0s, 344 effective words/s\n",
      "2019-03-13 20:13:07,191 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-03-13 20:13:07,194 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-13 20:13:07,203 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-13 20:13:07,207 : INFO : EPOCH - 3 : training on 23 raw words (3 effective words) took 0.0s, 176 effective words/s\n",
      "2019-03-13 20:13:07,215 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-03-13 20:13:07,218 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-13 20:13:07,219 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-13 20:13:07,223 : INFO : EPOCH - 4 : training on 23 raw words (5 effective words) took 0.0s, 619 effective words/s\n",
      "2019-03-13 20:13:07,230 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-03-13 20:13:07,232 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-13 20:13:07,233 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-13 20:13:07,234 : INFO : EPOCH - 5 : training on 23 raw words (4 effective words) took 0.0s, 905 effective words/s\n",
      "2019-03-13 20:13:07,244 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-03-13 20:13:07,247 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-13 20:13:07,248 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-13 20:13:07,249 : INFO : EPOCH - 6 : training on 23 raw words (5 effective words) took 0.0s, 885 effective words/s\n",
      "2019-03-13 20:13:07,261 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-03-13 20:13:07,265 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-13 20:13:07,266 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-13 20:13:07,267 : INFO : EPOCH - 7 : training on 23 raw words (4 effective words) took 0.0s, 486 effective words/s\n",
      "2019-03-13 20:13:07,276 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-03-13 20:13:07,280 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-13 20:13:07,281 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-13 20:13:07,282 : INFO : EPOCH - 8 : training on 23 raw words (1 effective words) took 0.0s, 160 effective words/s\n",
      "2019-03-13 20:13:07,293 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-03-13 20:13:07,298 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-13 20:13:07,299 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-13 20:13:07,300 : INFO : EPOCH - 9 : training on 23 raw words (3 effective words) took 0.0s, 321 effective words/s\n",
      "2019-03-13 20:13:07,310 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-03-13 20:13:07,314 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-13 20:13:07,315 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-13 20:13:07,316 : INFO : EPOCH - 10 : training on 23 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2019-03-13 20:13:07,317 : INFO : training on a 230 raw words (29 effective words) took 0.2s, 161 effective words/s\n",
      "2019-03-13 20:13:07,318 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#建模\n",
    "#定义自己的词典，需要是列表格式，不是词袋模型，因为fastText是输入词，滑动窗口式的\n",
    "corpus3 = [[\"今天\",\"是\",\"星期三\"],\n",
    "          [\"明天\",\"是\",\"星期四\"],\n",
    "         [\"后天\",\"是\",\"星期五\"],\n",
    "         [\"自然语言处理\",\"是\",\"人工智能\",\"深度学习\",\"的\",\"一个\",\"分支\"],\n",
    "         [\"图象识别\",\"是\",\"人工智能\",\"深度学习\",\"的\",\"一个\",\"分支\"]]\n",
    "#实例化FastText\n",
    "model3 = FastText(size=4,window=3,min_count=1)\n",
    "#size=4神经网络层数\n",
    "\n",
    "#建词典\n",
    "model3.build_vocab(sentences = corpus3)\n",
    "#训练\n",
    "model3.train(sentences=corpus3,\n",
    "            total_examples = len(corpus3),\n",
    "            epochs = 10)\n",
    "#上面三行可以写成一行\n",
    "#model2 = FastText(size=4, window=3, min_count=1, sentences=corpus3, iter=10)\n",
    "\n",
    "#model建好，后面就可以用wv使用model里的函数了\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('人工智能', 0.6834825873374939), ('的', 0.6742437481880188), ('星期三', 0.5137643218040466), ('星期五', 0.408088743686676), ('一个', 0.20406867563724518), ('分支', 0.2009875774383545), ('星期四', 0.13015753030776978), ('后天', 0.08627131581306458), ('深度学习', 0.014041736721992493), ('明天', -0.0069428980350494385)]\n"
     ]
    }
   ],
   "source": [
    "#查看相似度\n",
    "similar = model3.wv.most_similar(positive=['自然语言处理'])\n",
    "print(similar)\n",
    "#结果可知\"自然语言处理\"和\"人工智能\"最接近的词是0.6834825873374939\n",
    "#最不接近的是明天得分-0.0069428980350494385"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('是', 0.9093168377876282), ('星期四', 0.7687274217605591), ('一个', 0.5691813826560974), ('星期三', 0.30468249320983887), ('分支', 0.26344534754753113), ('人工智能', 0.1935146450996399), ('的', 0.10026012361049652), ('星期五', -0.36623483896255493), ('图象识别', -0.3735969662666321), ('自然语言处理', -0.47564271092414856)]\n",
      "--------\n",
      "[('后天', 0.7321161031723022), ('深度学习', 0.711909294128418), ('星期五', 0.15931332111358643), ('分支', -0.04213597625494003), ('人工智能', -0.13502061367034912), ('星期三', -0.16559240221977234), ('的', -0.23315072059631348), ('明天', -0.2549751400947571), ('今天', -0.3092395067214966), ('一个', -0.7150505781173706)]\n"
     ]
    }
   ],
   "source": [
    "#相关度高的词（周围出现相似词句）\n",
    "print(model3.wv.most_similar(['今天','明天']))\n",
    "#'是', 0.9093168377876282 '星期四', 0.7687274217605591\n",
    "#对比上面word2vec\n",
    "#model.similarity('今天','明天')#-0.15154815\n",
    "print(\"--------\")\n",
    "print(model3.wv.most_similar(['自然语言处理','图象识别']))\n",
    "#('后天', 0.7321161031723022), ('深度学习', 0.711909294128418)\n",
    "#model3.similarity('自然语言处理','图象识别')#0.11322209\n",
    "\n",
    "#【由结果可知fastText可以通过中间词，找到和它相关的词\n",
    "#根据得分分类】\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#更多gensim fastText 函数使用，\n",
    "#参考:https://radimrehurek.com/gensim/models/fasttext.html#module-gensim.models.fasttext\n",
    "#【后面可看下用day2的数据分类新闻看下】"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
