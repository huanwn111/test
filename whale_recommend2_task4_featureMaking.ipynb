{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==task04 特征工程(制作特征和标签， 转成监督学习问题。\n",
    "# 【做成有标签的监督学习是为了给召回的这些数据后面加LR排序分类，\n",
    "# 因为召回来是粗排，虽然能用score直接排序选，但策略太简单不可靠，\n",
    "# 所以再补加特征用LR等模型做一个精排，分类概率排序，\n",
    "# 要做分类就要做特征和标签，做符合模型需要的数据】)==\n",
    "# 先捋一下原始的给定数据， 哪些特征可以直接利用：\n",
    "# 1、文章的自身特征， category_id表示这文章的类型， created_at_ts表示文章建立的时间， 这个关系着文章的时效性， words_count是文章的字数， 一般字数太长我们不太喜欢点击, 也不排除有人就喜欢读长文。\n",
    "# 2、文章的内容embedding特征， 这个召回的时候用过， 这里可以选择使用， 也可以选择不用， 也可以尝试其他类型的embedding特征， 比如W2V等\n",
    "# 3、用户的设备特征信息\n",
    "# 上面这些直接可以用的特征， 待做完特征工程之后， 直接就可以根据article_id或者是user_id把这些特征加入进去。\n",
    "# 但是我们需要【先基于召回的结果， 构造一些特征，然后制作标签，形成一个监督学习的数据集】。\n",
    "\n",
    "# 构造监督数据集的思路， 根据召回结果， \n",
    "# 我们会得到一个{user_id: [可能点击的文章列表]}形式的字典。 \n",
    "# 那么我们就可以对于每个用户， 每篇可能点击的文章构造一个监督测试集， \n",
    "# 比如对于用户user1， 假设得到的他的召回列表{user1: [item1, item2, item3]}， \n",
    "# 我们就可以得到三行数据(user1, item1), (user1, item2), (user1, item3)的形式， 这就是监督测试集时候的前两列特征。\n",
    "\n",
    "# 构造其它特征思路举例：结合用户的历史点击文章信息。 往往用户的最后一次点击会和其最后几次点击有很大的关联。\n",
    "# 所以对于每个候选文章， 做出与最后几次点击相关的特征如下：\n",
    "\n",
    "# 1、候选item与最后几次点击的相似性特征(embedding内积） — 这个直接关联用户历史行为\n",
    "# 2、候选item与最后几次点击的相似性特征的统计特征 — 统计特征可以减少一些波动和异常\n",
    "# 3、候选item与最后几次点击文章的字数差的特征 — 可以通过字数看用户偏好\n",
    "# 4、候选item与最后几次点击的文章建立的时间差特征 — 时间差特征可以看出该用户对于文章的实时性的偏好\n",
    "\n",
    "# 下面我们就实现上面的这些特征的制作， 下面的逻辑是这样：\n",
    "\n",
    "# 1、我们首先获得用户的最后一次点击操作和用户的历史点击， 这个基于我们的日志数据集做\n",
    "# 2、基于用户的历史行为制作特征， 这个会用到用户的历史点击表， 最后的召回列表， 文章的信息表和embedding向量\n",
    "# 3、制作标签， 形成最后的监督学习数据集\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import gc, os\n",
    "import logging\n",
    "import time\n",
    "import lightgbm as lgb\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 节省内存的一个函数\n",
    "# 减少内存\n",
    "def reduce_mem(df):\n",
    "    starttime = time.time()\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if pd.isnull(c_min) or pd.isnull(c_max):\n",
    "                continue\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('-- Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction),time spend:{:2.2f} min'.format(end_mem,\n",
    "                                                                                                           100*(start_mem-end_mem)/start_mem,\n",
    "                                                                                                           (time.time()-starttime)/60))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义数据路径\n",
    "data_path = './data/'\n",
    "save_path = './dataRs/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据读取\n",
    "# all_click_df指的是训练集\n",
    "# sample_user_nums 采样作为验证集的用户数量\n",
    "def trn_val_split(all_click_df, sample_user_nums):\n",
    "    all_click = all_click_df\n",
    "    all_user_ids = all_click.user_id.unique()\n",
    "    \n",
    "    # replace=True表示可以重复抽样，反之不可以\n",
    "    sample_user_ids = np.random.choice(all_user_ids, size=sample_user_nums, replace=False) \n",
    "    \n",
    "    click_val = all_click[all_click['user_id'].isin(sample_user_ids)]\n",
    "    click_trn = all_click[~all_click['user_id'].isin(sample_user_ids)]\n",
    "    \n",
    "    # 将验证集中的最后一次点击给抽取出来作为答案\n",
    "    click_val = click_val.sort_values(['user_id', 'click_timestamp'])\n",
    "    val_ans = click_val.groupby('user_id').tail(1)\n",
    "    \n",
    "    click_val = click_val.groupby('user_id').apply(lambda x: x[:-1]).reset_index(drop=True)\n",
    "    \n",
    "    # 去除val_ans中某些用户只有一个点击数据的情况，如果该用户只有一个点击数据，又被分到ans中，\n",
    "    # 那么训练集中就没有这个用户的点击数据，出现用户冷启动问题，给自己模型验证带来麻烦\n",
    "    val_ans = val_ans[val_ans.user_id.isin(click_val.user_id.unique())] # 保证答案中出现的用户再验证集中还有\n",
    "    click_val = click_val[click_val.user_id.isin(val_ans.user_id.unique())]\n",
    "    \n",
    "    return click_trn, click_val, val_ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取历史点击和最后一次点击\n",
    "# 获取当前数据的历史点击和最后一次点击\n",
    "def get_hist_and_last_click(all_click):\n",
    "    all_click = all_click.sort_values(by=['user_id', 'click_timestamp'])\n",
    "    click_last_df = all_click.groupby('user_id').tail(1)\n",
    "\n",
    "    # 如果用户只有一个点击，hist为空了，会导致训练的时候这个用户不可见，此时默认泄露一下\n",
    "    def hist_func(user_df):\n",
    "        if len(user_df) == 1:\n",
    "            return user_df\n",
    "        else:\n",
    "            return user_df[:-1]\n",
    "\n",
    "    click_hist_df = all_click.groupby('user_id').apply(hist_func).reset_index(drop=True)\n",
    "\n",
    "    return click_hist_df, click_last_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取训练、验证及测试集\n",
    "def get_trn_val_tst_data(data_path, offline=True):\n",
    "    if offline:\n",
    "        click_trn_data = pd.read_csv(data_path+'train_click_log.csv')  # 训练集用户点击日志\n",
    "        click_trn_data = reduce_mem(click_trn_data)\n",
    "        click_trn, click_val, val_ans = trn_val_split(all_click_df, sample_user_nums)\n",
    "    else:\n",
    "        click_trn = pd.read_csv(data_path+'train_click_log.csv')\n",
    "        click_trn = reduce_mem(click_trn)\n",
    "        click_val = None\n",
    "        val_ans = None\n",
    "    \n",
    "    click_tst = pd.read_csv(data_path+'testA_click_log.csv')\n",
    "    \n",
    "    return click_trn, click_val, click_tst, val_ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取召回列表\n",
    "# 返回多路召回列表或者单路召回\n",
    "def get_recall_list(save_path, single_recall_model=None, multi_recall=False):\n",
    "    if multi_recall:\n",
    "        return pickle.load(open(save_path + 'final_recall_items_dict.pkl', 'rb'))\n",
    "    \n",
    "    if single_recall_model == 'i2i_itemcf':\n",
    "        return pickle.load(open(save_path + 'itemcf_recall_dict.pkl', 'rb'))\n",
    "#     elif single_recall_model == 'i2i_emb_itemcf':\n",
    "#         return pickle.load(open(save_path + 'itemcf_emb_dict.pkl', 'rb'))\n",
    "#     elif single_recall_model == 'user_cf':\n",
    "#         return pickle.load(open(save_path + 'youtubednn_usercf_dict.pkl', 'rb'))\n",
    "#     elif single_recall_model == 'youtubednn':\n",
    "#         return pickle.load(open(save_path + 'youtube_u2i_dict.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取文章信息\n",
    "def get_article_info_df():\n",
    "    article_info_df = pd.read_csv(data_path + 'articles.csv')\n",
    "    article_info_df = reduce_mem(article_info_df)\n",
    "    \n",
    "    return article_info_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Mem. usage decreased to 23.34 Mb (69.4% reduction),time spend:0.00 min\n"
     ]
    }
   ],
   "source": [
    "# 读取数据\n",
    "# 这里offline的online的区别就是验证集是否为空\n",
    "click_trn, click_val, click_tst, val_ans = get_trn_val_tst_data(data_path, offline=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "click_trn_hist, click_trn_last = get_hist_and_last_click(click_trn)\n",
    "\n",
    "if click_val is not None:\n",
    "    click_val_hist, click_val_last = click_val, val_ans\n",
    "else:\n",
    "    click_val_hist, click_val_last = None, None\n",
    "    \n",
    "click_tst_hist = click_tst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将召回列表转换成df的形式\n",
    "def recall_dict_2_df(recall_list_dict):\n",
    "    df_row_list = [] # [user, item, score]\n",
    "    for user, recall_list in tqdm(recall_list_dict.items()):\n",
    "        for item, score in recall_list:\n",
    "            df_row_list.append([user, item, score])\n",
    "    \n",
    "    col_names = ['user_id', 'sim_item', 'score']\n",
    "    recall_list_df = pd.DataFrame(df_row_list, columns=col_names)\n",
    "    \n",
    "    return recall_list_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 召回数据打标签\n",
    "def get_rank_label_df(recall_list_df, label_df, is_test=False):\n",
    "    # 测试集是没有标签了，为了后面代码同一一些，这里直接给一个负数替代\n",
    "    if is_test:\n",
    "        recall_list_df['label'] = -1\n",
    "        return recall_list_df\n",
    "    \n",
    "    label_df = label_df.rename(columns={'click_article_id': 'sim_item'})\n",
    "    recall_list_df_ = recall_list_df.merge(label_df[['user_id', 'sim_item', 'click_timestamp']], \\\n",
    "                                               how='left', on=['user_id', 'sim_item'])\n",
    "    recall_list_df_['label'] = recall_list_df_['click_timestamp'].apply(lambda x: 0.0 if np.isnan(x) else 1.0)\n",
    "    del recall_list_df_['click_timestamp']\n",
    "    \n",
    "    return recall_list_df_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_recall_item_label_df(click_trn_hist, click_val_hist, click_tst_hist,click_trn_last, click_val_last, recall_list_df):\n",
    "    # 获取训练数据的召回列表\n",
    "    trn_user_items_df = recall_list_df[recall_list_df['user_id'].isin(click_trn_hist['user_id'].unique())]\n",
    "    # 训练数据打标签\n",
    "    trn_user_item_label_df = get_rank_label_df(trn_user_items_df, click_trn_last, is_test=False)\n",
    "    # 训练数据负采样【负采样先不做】\n",
    "#     trn_user_item_label_df = neg_sample_recall_data(trn_user_item_label_df)\n",
    "    \n",
    "    if click_val is not None:\n",
    "        val_user_items_df = recall_list_df[recall_list_df['user_id'].isin(click_val_hist['user_id'].unique())]\n",
    "        val_user_item_label_df = get_rank_label_df(val_user_items_df, click_val_last, is_test=False)\n",
    "#         val_user_item_label_df = neg_sample_recall_data(val_user_item_label_df)\n",
    "    else:\n",
    "        val_user_item_label_df = None\n",
    "        \n",
    "    # 测试数据不需要进行负采样，直接对所有的召回商品进行打-1标签\n",
    "    tst_user_items_df = recall_list_df[recall_list_df['user_id'].isin(click_tst_hist['user_id'].unique())]\n",
    "    tst_user_item_label_df = get_rank_label_df(tst_user_items_df, None, is_test=True)\n",
    "    \n",
    "    return trn_user_item_label_df, val_user_item_label_df, tst_user_item_label_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████| 250000/250000 [00:02<00:00, 94413.96it/s]\n"
     ]
    }
   ],
   "source": [
    "# 读取召回列表\n",
    "recall_list_dict = get_recall_list(save_path, single_recall_model='i2i_itemcf') # 这里只选择了单路召回的结果，也可以选择多路召回结果\n",
    "# 将召回数据转换成df\n",
    "recall_list_df = recall_dict_2_df(recall_list_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 给训练验证数据打标签，并负采样（这一部分时间比较久）\n",
    "trn_user_item_label_df, val_user_item_label_df, tst_user_item_label_df = get_user_recall_item_label_df(click_trn_hist, \n",
    "                                                                                                       click_val_hist, \n",
    "                                                                                                       click_tst_hist,\n",
    "                                                                                                       click_trn_last, \n",
    "                                                                                                       click_val_last, \n",
    "                                                                                                       recall_list_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将召回数据转换成字典\n",
    "# 将最终的召回的df数据转换成字典的形式做排序特征\n",
    "def make_tuple_func(group_df):\n",
    "    row_data = []\n",
    "    for name, row_df in group_df.iterrows():\n",
    "        row_data.append((row_df['sim_item'], row_df['score'], row_df['label']))\n",
    "    \n",
    "    return row_data\n",
    "\n",
    "trn_user_item_label_tuples = trn_user_item_label_df.groupby('user_id').apply(make_tuple_func).reset_index()\n",
    "trn_user_item_label_tuples_dict = dict(zip(trn_user_item_label_tuples['user_id'], trn_user_item_label_tuples[0]))\n",
    "\n",
    "if val_user_item_label_df is not None:\n",
    "    val_user_item_label_tuples = val_user_item_label_df.groupby('user_id').apply(make_tuple_func).reset_index()\n",
    "    val_user_item_label_tuples_dict = dict(zip(val_user_item_label_tuples['user_id'], val_user_item_label_tuples[0]))\n",
    "else:\n",
    "    val_user_item_label_tuples_dict = None\n",
    "    \n",
    "tst_user_item_label_tuples = tst_user_item_label_df.groupby('user_id').apply(make_tuple_func).reset_index()\n",
    "tst_user_item_label_tuples_dict = dict(zip(tst_user_item_label_tuples['user_id'], tst_user_item_label_tuples[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Mem. usage decreased to  5.56 Mb (50.0% reduction),time spend:0.00 min\n",
      "-- Mem. usage decreased to 46.65 Mb (62.5% reduction),time spend:0.00 min\n"
     ]
    }
   ],
   "source": [
    "# 读取文章特征\n",
    "articles =  pd.read_csv(data_path+'articles.csv')\n",
    "articles = reduce_mem(articles)\n",
    "\n",
    "# 日志数据，就是前面的所有数据\n",
    "if click_val is not None:\n",
    "    all_data = click_trn.append(click_val)\n",
    "all_data = click_trn.append(click_tst)\n",
    "all_data = reduce_mem(all_data)\n",
    "\n",
    "# 拼上文章信息\n",
    "all_data = all_data.merge(articles, left_on='click_article_id', right_on='article_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==【开始做特征：】="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 用户的设备习惯\n",
    "def device_fea(all_data, cols):\n",
    "    #参数：all_data数据集，cols用到的特征列\n",
    "    user_device_info = all_data[cols]\n",
    "    # 用众数（最多的数）来表示每个用户的设备信息\n",
    "#  groupby    value_counts() 以用户id分组，组内agg统计重复个数（众数）\n",
    "    user_device_info = user_device_info.groupby('user_id').agg(lambda x:x.value_counts().index[0]).reset_index()\n",
    "    print(user_device_info)\n",
    "    return user_device_info\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    user_id  click_environment  click_deviceGroup  click_os  click_country  \\\n",
      "0    198481                  4                  1        17              1   \n",
      "1    198563                  4                  1        17              1   \n",
      "2    198576                  4                  1        17              1   \n",
      "3    198579                  4                  3        20              1   \n",
      "4    198598                  4                  3        20              1   \n",
      "..      ...                ...                ...       ...            ...   \n",
      "95   199968                  4                  1        17              1   \n",
      "96   199973                  4                  1        17              1   \n",
      "97   199989                  4                  1        17              1   \n",
      "98   199991                  4                  1        17              1   \n",
      "99   199999                  4                  1        17              1   \n",
      "\n",
      "    click_region  click_referrer_type  \n",
      "0             16                    1  \n",
      "1              4                    1  \n",
      "2             16                    1  \n",
      "3             25                    1  \n",
      "4             21                    1  \n",
      "..           ...                  ...  \n",
      "95            25                    1  \n",
      "96            16                    2  \n",
      "97            10                    1  \n",
      "98             2                    2  \n",
      "99            13                    1  \n",
      "\n",
      "[100 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "# 设备特征\n",
    "device_cols = ['user_id', 'click_environment', 'click_deviceGroup', 'click_os', 'click_country', 'click_region', 'click_referrer_type']\n",
    "user_device_info = device_fea(all_data[:100], device_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#（时间较长，暂放弃这个特征,用上面100条数据代替）\n",
    "# user_device_info = device_fea(all_data, device_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 用户的主题爱好\n",
    "# 思路：先把用户点击的文章属于的主题转成一个列表， \n",
    "#后面再总的汇总的时候单独制作一个特征， 就是文章的主题如果属于这里面， 就是1， 否则就是0。\n",
    "\n",
    "def user_cat_hob_fea(all_data, cols):\n",
    "#    all_data: 数据集cols: 用到的特征列\n",
    "    user_category_hob_info = all_data[cols]\n",
    "    user_category_hob_info = user_category_hob_info.groupby('user_id').agg({list}).reset_index()\n",
    "    \n",
    "    #做一个df，加入新列，特征列表\n",
    "    user_cat_hob_info = pd.DataFrame()\n",
    "    user_cat_hob_info['user_id'] = user_category_hob_info['user_id']\n",
    "    user_cat_hob_info['cate_list'] = user_category_hob_info['category_id']\n",
    "    print(user_cat_hob_info)\n",
    "    return user_cat_hob_info\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    user_id cate_list\n",
      "0    198481     [281]\n",
      "1    198563     [281]\n",
      "2    198576     [281]\n",
      "3    198579     [281]\n",
      "4    198598     [281]\n",
      "..      ...       ...\n",
      "95   199968     [281]\n",
      "96   199973     [281]\n",
      "97   199989     [281]\n",
      "98   199991     [281]\n",
      "99   199999     [281]\n",
      "\n",
      "[100 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "user_category_hob_cols = ['user_id', 'category_id']\n",
    "user_cat_hob_info = user_cat_hob_fea(all_data[:100], user_category_hob_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 用户的字数偏好特征\n",
    "user_wcou_info = all_data[:100].groupby('user_id')['words_count'].agg('mean').reset_index()\n",
    "user_wcou_info.rename(columns={'words_count': 'words_hbo'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>words_hbo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>198481</td>\n",
       "      <td>173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>198563</td>\n",
       "      <td>173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>198576</td>\n",
       "      <td>173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>198579</td>\n",
       "      <td>173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>198598</td>\n",
       "      <td>173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>199968</td>\n",
       "      <td>173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>199973</td>\n",
       "      <td>173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>199989</td>\n",
       "      <td>173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>199991</td>\n",
       "      <td>173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>199999</td>\n",
       "      <td>173</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    user_id  words_hbo\n",
       "0    198481        173\n",
       "1    198563        173\n",
       "2    198576        173\n",
       "3    198579        173\n",
       "4    198598        173\n",
       "..      ...        ...\n",
       "95   199968        173\n",
       "96   199973        173\n",
       "97   199989        173\n",
       "98   199991        173\n",
       "99   199999        173\n",
       "\n",
       "[100 rows x 2 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_wcou_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 用户的信息特征合并保存\n",
    "user_info = pd.merge(user_device_info,user_cat_hob_info, on='user_id')#合并用户喜好特征、用户设备习惯特征\n",
    "user_info = user_info.merge(user_wcou_info, on='user_id')#合并文章字数特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#保存\n",
    "user_info.to_csv(save_path + 'user_info_100.csv', index=False)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==文章本身特征==\n",
    "# 下面基于data做历史相关的特征\n",
    "def create_feature(users_id, recall_list, click_hist_df,  articles_info, user_emb=None, N=1):#articles_emb此处未用\n",
    "    \"\"\"\n",
    "    基于用户的历史行为做相关特征\n",
    "    :param users_id: 用户id\n",
    "    :param recall_list: 对于每个用户召回的候选文章列表\n",
    "    :param click_hist_df: 用户的历史点击信息\n",
    "    :param articles_info: 文章信息\n",
    "    :param articles_emb: 文章的embedding向量, 这个可以用item_content_emb, item_w2v_emb, item_youtube_emb\n",
    "    :param user_emb: 用户的embedding向量， 这个是user_youtube_emb, 如果没有也可以不用， 但要注意如果要用的话， articles_emb就要用item_youtube_emb的形式， 这样维度才一样\n",
    "    :param N: 最近的N次点击  由于testA日志里面很多用户只存在一次历史点击， 所以为了不产生空值，默认是1\n",
    "    \"\"\"\n",
    "    \n",
    "    # 建立一个二维列表保存结果， 后面要转成DataFrame\n",
    "    all_user_feas = []\n",
    "    i = 0\n",
    "    for user_id in tqdm(users_id):\n",
    "        # 该用户的最后N次点击\n",
    "        hist_user_items = click_hist_df[click_hist_df['user_id']==user_id]['click_article_id'][-N:]\n",
    "        \n",
    "        # 遍历该用户的召回列表\n",
    "        for rank, (article_id, score, label) in enumerate(recall_list[user_id]):\n",
    "            # 该文章建立时间, 字数\n",
    "            a_create_time = articles_info[articles_info['article_id']==article_id]['created_at_ts'].values[0]\n",
    "            a_words_count = articles_info[articles_info['article_id']==article_id]['words_count'].values[0]\n",
    "            single_user_fea = [user_id, article_id]\n",
    "            # 计算与最后点击的商品的相似度的和， 最大值和最小值， 均值\n",
    "#             sim_fea = []\n",
    "            time_fea = []\n",
    "            word_fea = []\n",
    "            # 遍历用户的最后N次点击文章\n",
    "            for hist_item in hist_user_items:\n",
    "                b_create_time = articles_info[articles_info['article_id']==hist_item]['created_at_ts'].values[0]\n",
    "                b_words_count = articles_info[articles_info['article_id']==hist_item]['words_count'].values[0]\n",
    "                \n",
    "#                 sim_fea.append(np.dot(articles_emb[hist_item], articles_emb[article_id]))\n",
    "                time_fea.append(abs(a_create_time-b_create_time))\n",
    "                word_fea.append(abs(a_words_count-b_words_count))\n",
    "                \n",
    "#             single_user_fea.extend(sim_fea)      # 相似性特征\n",
    "            single_user_fea.extend(time_fea)    # 时间差特征\n",
    "            single_user_fea.extend(word_fea)    # 字数差特征\n",
    "#             single_user_fea.extend([max(sim_fea), min(sim_fea), sum(sim_fea), sum(sim_fea) / len(sim_fea)])  # 相似性的统计特征\n",
    "            \n",
    "            if user_emb:  # 如果用户向量有的话， 这里计算该召回文章与用户的相似性特征 \n",
    "                single_user_fea.append(np.dot(user_emb[user_id], articles_emb[article_id]))\n",
    "                \n",
    "            single_user_fea.extend([score, rank, label])    \n",
    "            # 加入到总的表中\n",
    "            all_user_feas.append(single_user_fea)\n",
    "    \n",
    "    # 定义列名\n",
    "    id_cols = ['user_id', 'click_article_id']\n",
    "    sim_cols = ['sim' + str(i) for i in range(N)]\n",
    "    time_cols = ['time_diff' + str(i) for i in range(N)]\n",
    "    word_cols = ['word_diff' + str(i) for i in range(N)]\n",
    "    sat_cols = ['sim_max', 'sim_min', 'sim_sum', 'sim_mean']\n",
    "    user_item_sim_cols = ['user_item_sim'] if user_emb else []\n",
    "    user_score_rank_label = ['score', 'rank', 'label']\n",
    "    cols = id_cols + sim_cols + time_cols + word_cols + sat_cols + user_item_sim_cols + user_score_rank_label\n",
    "            \n",
    "    # 转成DataFrame\n",
    "    df = pd.DataFrame( all_user_feas, columns=cols)\n",
    "    \n",
    "    return df\n",
    "#更多参考：http://datawhale.club/t/topic/201"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Mem. usage decreased to  5.56 Mb (50.0% reduction),time spend:0.00 min\n"
     ]
    }
   ],
   "source": [
    "article_info_df = get_article_info_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取训练验证及测试数据中召回列文章相关特征\n",
    "trn_user_item_feats_df = create_feature(trn_user_item_label_tuples_dict.keys(), trn_user_item_label_tuples_dict, \\\n",
    "                                            click_trn_hist, article_info_df)#item_content_emb_dict参没用向量\n",
    "\n",
    "if val_user_item_label_tuples_dict is not None:\n",
    "    val_user_item_feats_df = create_feature(val_user_item_label_tuples_dict.keys(), val_user_item_label_tuples_dict, \\\n",
    "                                                click_val_hist, article_info_df)\n",
    "else:\n",
    "    val_user_item_feats_df = None\n",
    "    \n",
    "tst_user_item_feats_df = create_feature(tst_user_item_label_tuples_dict.keys(), tst_user_item_label_tuples_dict, \\\n",
    "                                            click_tst_hist, article_info_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存备用\n",
    "trn_user_item_feats_df.to_csv(save_path + 'trn_user_item_feats_df.csv', index=False)\n",
    "\n",
    "if val_user_item_feats_df is not None:\n",
    "    val_user_item_feats_df.to_csv(save_path + 'val_user_item_feats_df.csv', index=False)\n",
    "\n",
    "tst_user_item_feats_df.to_csv(save_path + 'tst_user_item_feats_df.csv', index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==合并上面的用户行为特征==\n",
    "# 把用户信息直接读入进来\n",
    "user_info = pd.read_csv(save_path + 'user_info_100.csv')\n",
    "if os.path.exists(save_path + 'trn_user_item_feats_df.csv'):\n",
    "    trn_user_item_feats_df = pd.read_csv(save_path + 'trn_user_item_feats_df.csv')\n",
    "    \n",
    "if os.path.exists(save_path + 'tst_user_item_feats_df.csv'):\n",
    "    tst_user_item_feats_df = pd.read_csv(save_path + 'tst_user_item_feats_df.csv')\n",
    "\n",
    "if os.path.exists(save_path + 'val_user_item_feats_df.csv'):\n",
    "    val_user_item_feats_df = pd.read_csv(save_path + 'val_user_item_feats_df.csv')\n",
    "else:\n",
    "    val_user_item_feats_df = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 拼接\n",
    "trn_user_item_feats_df = trn_user_item_feats_df.merge(user_info, on='user_id', how='left')\n",
    "\n",
    "if val_user_item_feats_df is not None:\n",
    "    val_user_item_feats_df = val_user_item_feats_df.merge(user_info, on='user_id', how='left')\n",
    "else:\n",
    "    val_user_item_feats_df = None\n",
    "    \n",
    "tst_user_item_feats_df = tst_user_item_feats_df.merge(user_info, on='user_id',how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#文章特章读入\n",
    "articles =  pd.read_csv(data_path+'articles.csv')\n",
    "articles = reduce_mem(articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 拼上文章特征\n",
    "trn_user_item_feats_df = trn_user_item_feats_df.merge(articles, left_on='click_article_id', right_on='article_id')\n",
    "\n",
    "if val_user_item_feats_df is not None:\n",
    "    val_user_item_feats_df = val_user_item_feats_df.merge(articles, left_on='click_article_id', right_on='article_id')\n",
    "else:\n",
    "    val_user_item_feats_df = None\n",
    "\n",
    "tst_user_item_feats_df = tst_user_item_feats_df.merge(articles, left_on='click_article_id', right_on='article_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存特征\n",
    "# 训练验证特征\n",
    "trn_user_item_feats_df.to_csv(save_path + 'trn_user_item_feats_df.csv', index=False)\n",
    "if val_user_item_feats_df is not None:\n",
    "    val_user_item_feats_df.to_csv(save_path + 'val_user_item_feats_df.csv', index=False)\n",
    "tst_user_item_feats_df.to_csv(save_path + 'tst_user_item_feats_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#更多参数：http://datawhale.club/t/topic/201"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
