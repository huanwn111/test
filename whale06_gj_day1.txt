【Task1(2天)】随机森林算法梳理
【参考框架】欢迎有自己的框架
1. 集成学习概念
2. 个体学习器概念
3. boosting  bagging
4. 结合策略(平均法，投票法，学习法)
5. 随机森林思想
6. 随机森林的推广
7. 优缺点
8. sklearn参数
9.应用场景


1. 集成学习概念:

多个弱学习分类器算法结合，综合使用，通过取长补短加权等方式变强学习器.
如多棵决策树组成随机森林RF,其它算法还有AdaBoost、GBDT、XGBoost、LightGBM、CatBoost等。
类别：同质集成（同种类型个体学习器结合，这种个体学习器叫基学习器）；
异质集成（由不同类型学习算法组成，这种个体学习器叫组件学习器）
如何训练多个弱分类器以及如何组合多个弱分类器，常用的方法有boosting、bagging.

2. 个体学习器概念:
单一的机器学习算法，如单一的决策树算法

3. boosting  bagging:

boosting(译：增进 推进 提升) 
串行学习，先训练单个学习器，再根据学习结果调整后续训练样本分布，对做错的加关注，再调整的基础上训练下一个学习器。

bagging(译：打包)
并行学习，同时训练多个单一学习器，用同一批训练数据随机采样（有放回的，多次供多个学习器使用）进行分类训练，通过训练结果，根据各学习器训练效果采取加权微调，再一次采样训练时考虑之前的权重分配样本，直到训练到最终优质结果。最后选择最优的分类器，或取各分类器结果的平均做为最终的分类结果。


4. 结合策略(平均法，投票法，学习法)：

如3中所说，结合策略是指各个体学习器结合生成最终结果的方法。
一般结合方法分为：平均法，投票法，学习法。

平均法（数值型输出）：
简单平均：将各个体学习器结果求平均做为最终学习结果。

加权平均：将各学习器结果带各自的权重求平均做为最终学习结果。
该策略典型代表是AdaBoost算法。

//个体学习器性能相差大时一般用加权平均法

投票法（分类任务）：

绝对多数投票：取各分类器大多数（一半以上）分类结果做最终分类结果，如10个个体分类器，有8个分类一致属于A类，则取A这个结果做为最终的分类结果。

相对多数投票：同时有多个分类labels得最高票，则从中随机选一个做最终分类结果。

加权投票：
与加权平均类似，指采样过程中动态修正各个体分类器权值，最终求得最终分类结果。

学习法：将个体学习器的学习结果，不直接取结果，而是输入到一个学习器中，把结果进行再次训练学习，取结果。最终这个学习器叫次级学习器或元学习器（常见的有Stacking和Blending），前面的训练第一阶段数据的个体学习器叫初级学习器。
学习法的结合策略更为强大，训练大量数据时可用。

5. 随机森林思想:
利用随机方式将多个决策树组合成一个森林。它是bagging并行集成方法。
实现步骤有4个方面：

随机选样本：有放回随机采样得到不同子决策树的训练集，比如一个训练样本数量为N，有放回采样采N次，每次采一个，放回，继续采，这样会采到重复样本。这样构成的新训练集，给各个子决策树使用，有效利用有限样本，是一个洗牌作用，用这个新样本作训练集。

随机选特征：
单个决策树是通过比较计算最大增益的特征作为最优选特征，划分分类节点。
随机森林不是计算所有特征信息增益，而是从总特征中随机抽取一部分，计算这部分特征的增益，来选最优特征（属性），来确定分类切分节点。注意，特征随机是无放回的（可能因为本身多个树使用多次放回随机同一份数据训练，随机多个特征计算比较本来就已经是很复杂的，两步随机已经足够用，且大量数据多用多训越好，特征就那么多，只是相互比较选一个最优，没有必要再随机放回打乱，增加运算负担）。

构建决策树：用上面随机训练样本构建多个决策树，用随机选特征方法选各树特征切分树。

随机森林投票分类：
最终分类结果通过上面4中说的投票法，来选择绝对多数分类，或随机选并列最高分类之一，或加权随机选分类结果，做为最终森林的分类结果。

6. 随机森林的推广：

Extra Tree：
是随机森林RF的变种，原理同。
二者区别：
RF随机采样决定子决策树训练集。Extra Tree每个树都用了原始数据。
RF选特征切分分类时和传统决策树一个，基于信息增益、信息增益率、基尼系数等。
原则方法。Extra Tree比较极端，它是随机选一个特征。
由此，它分出的树会比RF庞大，但它的方差会减少（随机选择，排除人为干扰，使波动更平稳自然），所以泛化能力（适用更大众）比RF强。

TRTE（Totally Random Trees Embedding）：
TRTE （译：完全随机树嵌入矩阵）
它是将初始数据落在决策树中叶子节点的位置用向量编码表示，
比如数据落入树1第3节点表示为00100，落入2树1节点表示为10000，第3树5节点表示为00001
则这数据的特征映射码为（0,0,1,0,0,1,0,0,0,0,0,0,0,0,1）
通过这种数据转化方式，把低维数据变高维，再分类。
分类方法是通过把相似的特征码聚类到一起来完成。

TRTE的数据转化方式是非监督的。
转化后的高维数据可用于无监督分类，也可用有监督回归分类器分类。

但是要注意：这种转换数据方式也有可能转成低维的，比如词袋模型。
特征有2000个，可能做完TRTE后只剩几百个。（这和词袋模型有太多重复词有关，决策树比较特征切分过程相当于做了过滤）


IForest（Isolation Forest）：
译：隔离森林、孤立森林
它的思想是异常点检测：用一个很小的切分标准，很少的数据，只需把异常的区分出来，隔离即可。
所以，它和RF的区别如下：
IForest 随机采样时只需少量数据；
建决策树时划分特征随机选（随便挑一个），切分的阈值也是随机选一个；
深度max_depth较小（决策树的规模不大）；
判断异常的方法：通过算子节点的深度和平均深度比较，算异常概率。

7. 随机森林的优缺点：

RF优点：
训练可并行，大量样本训练时有速度优势；
随机选决策树划分特征列表，提高训练性能，即使是样本较高维时也一样；
决策树的节点可列出各特征的重要性权重百分比，得到特征重要性列表；
随机抽样，模型方差小，泛化能力强；
实现简单（随机抽样、随机选特征、建树、投票决策结果）
对部分特征丢失（本来就是随机选特征切分的，丢弃了一些了）不敏感；

RF缺点：
噪声（非决定性特征）过大的特征上，容易过拟合（即选了一个非常多权重，但并没有实际意义的特征做切分点，换其它常规测试集时不能泛化）；
取值较多的划分特征对RF的决策会产生更大的影响，可能影响模型效果（比如上面，噪声特征很大，但并不影响它的决策地位，这样的模型是没有意义的）。

8. sklearn参数
sklearn.ensemble.RandomForestClassifier(
	n_estimators=10,#【随机森林树的个数】
	criterion='gini', 
#采用什么标准衡量特征重要性，此处是基尼系数  
	max_depth=None, 
 #森林最大深度   
	min_samples_split=2, 
#树分裂的最小个数   
	min_samples_leaf=1,
  #【某叶节点上样本的最小数量（少于该值时该节点不再切分）】 
	min_weight_fraction_leaf=0.0,
#叶节点最小加权分数
	max_features='auto',
  #生成最佳森林时最大特征数  
	max_leaf_nodes=None, 
 #最大叶节点   
	min_impurity_decrease=0.0, 
    
	min_impurity_split=None,
    
	bootstrap=True, 
    
	oob_score=False, 
    
	n_jobs=1, 
    
	random_state=None,
 #随机种子
	verbose=0, 
    
	warm_start=False, 
    
	class_weight=None
)

9.随机森林应用场景：
适用于非连续性（非线性）特征分类；
算法bagging思想并行集成，算法容易处理高维数据（不需降维）；
需要大量训练数据时；
对分类准确性要求较高时；

其它分类方法应用场景比较：
参考：
https://blog.csdn.net/u010770184/article/details/53841743

