{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Task3 特征选择  \n",
    "1. TF-IDF原理。\n",
    "2. 文本矩阵化，使用词袋模型，以TF-IDF特征值为权重。（可以使用Python中TfidfTransformer库）\n",
    "3. 互信息的原理。\n",
    "4. 使用第二步生成的特征矩阵，利用互信息进行特征筛选。\n",
    "5. 参考\n",
    "文本挖掘预处理之TF-IDF：https://www.cnblogs.com/pinard/p/6693230.html\n",
    "使用不同的方法计算TF-IDF值：https://www.jianshu.com/p/f3b92124cd2b\n",
    "sklearn-点互信息和互信息：https://blog.csdn.net/u013710265/article/details/72848755\n",
    "如何进行特征选择（理论篇）机器学习你会遇到的“坑”：https://baijiahao.baidu.com/s?id=1604074325918456186&wfr=spider&for=pc\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "1. TF-IDF原理。\n",
    "词频-逆文档频（Term Frequency-Inverse DocumentFrequency），\n",
    "是衡量一个词的在一篇文章中的重性的一套标准。\n",
    "它与词频，也就是词在文章中出现的次数成正比。\n",
    "与词在整个与词在整个语料库中出现次数，也就是文档频，成反比，即逆文档频率\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==分到词,去停用词后==\n",
      "['今年', '汽车', '销售额', '有', '提高', '汽车', '拥有', '辆', '有', '提高', '但是', '道路', '拥堵', '也', '加重', '\\n', '中午', '去', '吃', '烤鸭', '要', '减肥', '还是', '少', '吃', '油腻', '多', '吃', '青菜', '健康', '饮食', '\\n', '最新', '上映', '影片', '获得', '最高', '票房', '但是', '有刷票', '嫌疑', '\\n', '人工智能', '飞速发展', '自然语言', '识别', '是', '人工智能', '一个', '分支', '机器', '学习', '是', '基础', '后面', '还有', '深度', '学习', '这些', '多', '是', '讲', '基础', '算法', '\\n']\n",
      "====以空格组合词成句,合成符合TF-IDF需要的训练格式===\n",
      "['今年 汽车 销售额 有 提高 汽车 拥有 辆 有 提高 但是 道路 拥堵 也 加重 ', ' 中午 去 吃 烤鸭 要 减肥 还是 少 吃 油腻 多 吃 青菜 健康 饮食 ', ' 最新 上映 影片 获得 最高 票房 但是 有刷票 嫌疑 ', ' 人工智能 飞速发展 自然语言 识别 是 人工智能 一个 分支 机器 学习 是 基础 后面 还有 深度 学习 这些 多 是 讲 基础 算法 ', '']\n",
      "==打印词频统计对象==\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None,\n",
      "        stop_words=[' ', '。', '，', '的', '。', '.', '了', '呀'],\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "==打印词频 括号里两位分别是 分类、词id（id可在下面信息对照） 右边值是词频==\n",
      "  (0, 9)\t1\n",
      "  (0, 15)\t1\n",
      "  (0, 34)\t1\n",
      "  (0, 5)\t1\n",
      "  (0, 16)\t1\n",
      "  (0, 17)\t2\n",
      "  (0, 35)\t1\n",
      "  (0, 22)\t2\n",
      "  (0, 4)\t1\n",
      "  (1, 38)\t1\n",
      "  (1, 6)\t1\n",
      "  (1, 36)\t1\n",
      "  (1, 23)\t1\n",
      "  (1, 31)\t1\n",
      "  (1, 7)\t1\n",
      "  (1, 25)\t1\n",
      "  (1, 2)\t1\n",
      "  (2, 12)\t1\n",
      "  (2, 20)\t1\n",
      "  (2, 26)\t1\n",
      "  (2, 19)\t1\n",
      "  (2, 29)\t1\n",
      "  (2, 14)\t1\n",
      "  (2, 1)\t1\n",
      "  (2, 18)\t1\n",
      "  (2, 5)\t1\n",
      "  (3, 27)\t1\n",
      "  (3, 33)\t1\n",
      "  (3, 24)\t1\n",
      "  (3, 32)\t1\n",
      "  (3, 10)\t1\n",
      "  (3, 11)\t2\n",
      "  (3, 13)\t2\n",
      "  (3, 21)\t1\n",
      "  (3, 8)\t1\n",
      "  (3, 0)\t1\n",
      "  (3, 30)\t1\n",
      "  (3, 28)\t1\n",
      "  (3, 37)\t1\n",
      "  (3, 3)\t2\n",
      "[[0 0 0 0 1 1 0 0 0 1 0 0 0 0 0 1 1 2 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 1 1\n",
      "  0 0 0]\n",
      " [0 0 1 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0 0 0 0\n",
      "  1 0 1]\n",
      " [0 1 0 0 0 1 0 0 0 0 0 0 1 0 1 0 0 0 1 1 1 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0\n",
      "  0 0 0]\n",
      " [1 0 0 2 0 0 0 0 1 0 1 2 0 2 0 0 0 0 0 0 0 1 0 0 1 0 0 1 1 0 1 0 1 1 0 0\n",
      "  0 1 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0]]\n",
      "==去重后的词,即特征==\n",
      "['一个', '上映', '中午', '人工智能', '今年', '但是', '健康', '减肥', '分支', '加重', '后面', '基础', '嫌疑', '学习', '影片', '拥堵', '拥有', '提高', '最新', '最高', '有刷票', '机器', '汽车', '油腻', '深度', '烤鸭', '票房', '算法', '自然语言', '获得', '识别', '还是', '还有', '这些', '道路', '销售额', '青菜', '飞速发展', '饮食']\n",
      "==每个单词对应的id值==\n",
      "{'今年': 4, '汽车': 22, '销售额': 35, '提高': 17, '拥有': 16, '但是': 5, '道路': 34, '拥堵': 15, '加重': 9, '中午': 2, '烤鸭': 25, '减肥': 7, '还是': 31, '油腻': 23, '青菜': 36, '健康': 6, '饮食': 38, '最新': 18, '上映': 1, '影片': 14, '获得': 29, '最高': 19, '票房': 26, '有刷票': 20, '嫌疑': 12, '人工智能': 3, '飞速发展': 37, '自然语言': 28, '识别': 30, '一个': 0, '分支': 8, '机器': 21, '学习': 13, '基础': 11, '后面': 10, '还有': 32, '深度': 24, '这些': 33, '算法': 27}\n",
      "==打印每类文本的TF-IDF权重==\n",
      "  (0, 4)\t0.2612568039321974\n",
      "  (0, 22)\t0.5225136078643948\n",
      "  (0, 35)\t0.2612568039321974\n",
      "  (0, 17)\t0.5225136078643948\n",
      "  (0, 16)\t0.2612568039321974\n",
      "  (0, 5)\t0.2107803444058925\n",
      "  (0, 34)\t0.2612568039321974\n",
      "  (0, 15)\t0.2612568039321974\n",
      "  (0, 9)\t0.2612568039321974\n",
      "  (1, 2)\t0.35355339059327373\n",
      "  (1, 25)\t0.35355339059327373\n",
      "  (1, 7)\t0.35355339059327373\n",
      "  (1, 31)\t0.35355339059327373\n",
      "  (1, 23)\t0.35355339059327373\n",
      "  (1, 36)\t0.35355339059327373\n",
      "  (1, 6)\t0.35355339059327373\n",
      "  (1, 38)\t0.35355339059327373\n",
      "  (2, 5)\t0.2743035641495426\n",
      "  (2, 18)\t0.339992197464673\n",
      "  (2, 1)\t0.339992197464673\n",
      "  (2, 14)\t0.339992197464673\n",
      "  (2, 29)\t0.339992197464673\n",
      "  (2, 19)\t0.339992197464673\n",
      "  (2, 26)\t0.339992197464673\n",
      "  (2, 20)\t0.339992197464673\n",
      "  (2, 12)\t0.339992197464673\n",
      "  (3, 3)\t0.4170288281141496\n",
      "  (3, 37)\t0.2085144140570748\n",
      "  (3, 28)\t0.2085144140570748\n",
      "  (3, 30)\t0.2085144140570748\n",
      "  (3, 0)\t0.2085144140570748\n",
      "  (3, 8)\t0.2085144140570748\n",
      "  (3, 21)\t0.2085144140570748\n",
      "  (3, 13)\t0.4170288281141496\n",
      "  (3, 11)\t0.4170288281141496\n",
      "  (3, 10)\t0.2085144140570748\n",
      "  (3, 32)\t0.2085144140570748\n",
      "  (3, 24)\t0.2085144140570748\n",
      "  (3, 33)\t0.2085144140570748\n",
      "  (3, 27)\t0.2085144140570748\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "2. 文本矩阵化，使用词袋模型，以TF-IDF特征值为权重。（可以使用Python中TfidfTransformer库）\n",
    "'''\n",
    "import jieba\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "#定义文本\n",
    "textAll=''\n",
    "\n",
    "#读文档\n",
    "with open(r'cutText2.txt','r') as fw:\n",
    "    lines = fw.readlines()\n",
    "#print(\"==打印从txt文件读取的文本==\")\n",
    "#print(lines)\n",
    "\n",
    "#拼接文本\n",
    "for line in lines:\n",
    "    textAll += line\n",
    "    \n",
    "#停用词表：\n",
    "stopWordsList = [' ','。','，','的','。','.','了','呀']\n",
    "\n",
    "#中文分词\n",
    "bagList = jieba.lcut(textAll,cut_all=False)\n",
    "#print(\"==分词==\")\n",
    "#print(bagList)\n",
    "#去停用词\n",
    "#分到词的列表\n",
    "wordsList_w = [w for w in bagList if w not in stopWordsList]\n",
    "print(\"==分到词,去停用词后==\")\n",
    "print(wordsList_w)\n",
    "#【去停用词，保留\\n换行符，把词再拼成句子用空格隔开，做成符合TF-IDF需要的训练格式。】\n",
    "#以空格组合词成句,合成符合TF-IDF需要的训练格式\n",
    "#join成字符串，再以\\n分隔存入list中，split\n",
    "wordsList_str = \" \".join(wordsList_w)\n",
    "wordsList = wordsList_str.split(\"\\n\")\n",
    "print(\"====以空格组合词成句,合成符合TF-IDF需要的训练格式===\")\n",
    "print(wordsList)\n",
    "\n",
    "\n",
    "#实例化CountVectorizer词频对象 \n",
    "#初始化词频统计对象\n",
    "vector = CountVectorizer(stop_words=stopWordsList)\n",
    "print(\"==打印词频统计对象==\")\n",
    "print(vector)\n",
    "#学习分词得到词频\n",
    "fitTsVector = vector.fit_transform(wordsList)\n",
    "#【注意：上面用分好词的词列表wordsList最后算出来TF-IDF值都是1？换成整段文字lines看下】\n",
    "#【这个要看下面中英文对照，中文也要传句子，但是是分好词的，\n",
    "#TF-IDF是看词在句中和在篇章中的权重综合。分在自己就没有比对了，当然会是1\n",
    "#所以上面做一个修改，把分好词的再以空格连接起来组成一组】\n",
    "#【但这样打印出来的get_feature_names特征词是句子】\n",
    "#fit_transform和fit一样，是把数据传进来学习，一个参数代表无监督学习，即没有结果，\n",
    "#找特征是一种无监督学习\n",
    "\n",
    "#换用以下英文文本做训练\n",
    "'''\n",
    "text=[\n",
    "    \"My name is Wang\",\n",
    "    \"I come to China to travel\",\n",
    "    \"This is a car polupar in China\",\n",
    "    \"I love tea and Apple \",\n",
    "    \"The work is to write some papers in science\"]\n",
    "'''\n",
    "'''\n",
    "text = [\n",
    "    '当地 时间 2017 15','日本 神奈川县 横须贺',\n",
    "      ' 东芝 国际 反应堆 报废 研究 开发 机构 IRID 共同开发 机器人 公开 亮相',\n",
    "      '这个 30 厘米 直径 13 厘米 水下 机器人 投放 福岛 第一 核电站 机组 反应堆 安全壳 底部 展开 调查'\n",
    "]\n",
    "#【注意以上中文和英文的格式对比，中文要传入fit的也必须是句子，只不过是分好词的句子。\n",
    "#看它的tf-idf重要性要看在句子中的？总之分成当个词一个一个逗开的那种，权重都是1。】\n",
    "'''\n",
    "\n",
    "#fitTsVector = vector.fit_transform(text)\n",
    "\n",
    "print(\"==打印词频 括号里两位分别是 分类、词id（id可在下面信息对照） 右边值是词频==\")\n",
    "#【也就是说，这一次Count后 fit 训练得到的是词频，传到下面TfidfTransformer的fit里面再训练就得到的是tf-idf\n",
    "#可以用TfidfVectorizer一步到位算得tf-idf】\n",
    "print(fitTsVector)\n",
    "# 每类文本的词频转array\n",
    "print(fitTsVector.toarray())\n",
    "# 去重后的词,即特征\n",
    "print(\"==去重后的词,即特征==\")\n",
    "print(vector.get_feature_names())\n",
    "# 每个单词对应的id值\n",
    "print(\"==每个单词对应的id值==\")\n",
    "print(vector.vocabulary_)\n",
    "\n",
    "\n",
    "#实例化TfidfTransformer TF-IDF计算类\n",
    "transformer = TfidfTransformer()\n",
    "#用TF-IDF计算器训练上面求得的词频结果，得到每个词的最终TF-IDF\n",
    "tfidfit = transformer.fit_transform(fitTsVector)\n",
    "print(\"==打印每类文本的TF-IDF权重==\")\n",
    "print(tfidfit)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 4)\t0.2612568039321974\n",
      "  (0, 22)\t0.5225136078643948\n",
      "  (0, 35)\t0.2612568039321974\n",
      "  (0, 17)\t0.5225136078643948\n",
      "  (0, 16)\t0.2612568039321974\n",
      "  (0, 5)\t0.2107803444058925\n",
      "  (0, 34)\t0.2612568039321974\n",
      "  (0, 15)\t0.2612568039321974\n",
      "  (0, 9)\t0.2612568039321974\n",
      "  (1, 2)\t0.35355339059327373\n",
      "  (1, 25)\t0.35355339059327373\n",
      "  (1, 7)\t0.35355339059327373\n",
      "  (1, 31)\t0.35355339059327373\n",
      "  (1, 23)\t0.35355339059327373\n",
      "  (1, 36)\t0.35355339059327373\n",
      "  (1, 6)\t0.35355339059327373\n",
      "  (1, 38)\t0.35355339059327373\n",
      "  (2, 5)\t0.2743035641495426\n",
      "  (2, 18)\t0.339992197464673\n",
      "  (2, 1)\t0.339992197464673\n",
      "  (2, 14)\t0.339992197464673\n",
      "  (2, 29)\t0.339992197464673\n",
      "  (2, 19)\t0.339992197464673\n",
      "  (2, 26)\t0.339992197464673\n",
      "  (2, 20)\t0.339992197464673\n",
      "  (2, 12)\t0.339992197464673\n",
      "  (3, 3)\t0.4170288281141496\n",
      "  (3, 37)\t0.2085144140570748\n",
      "  (3, 28)\t0.2085144140570748\n",
      "  (3, 30)\t0.2085144140570748\n",
      "  (3, 0)\t0.2085144140570748\n",
      "  (3, 8)\t0.2085144140570748\n",
      "  (3, 21)\t0.2085144140570748\n",
      "  (3, 13)\t0.4170288281141496\n",
      "  (3, 11)\t0.4170288281141496\n",
      "  (3, 10)\t0.2085144140570748\n",
      "  (3, 32)\t0.2085144140570748\n",
      "  (3, 24)\t0.2085144140570748\n",
      "  (3, 33)\t0.2085144140570748\n",
      "  (3, 27)\t0.2085144140570748\n",
      "[[0.         0.         0.         0.         0.2612568  0.21078034\n",
      "  0.         0.         0.         0.2612568  0.         0.\n",
      "  0.         0.         0.         0.2612568  0.2612568  0.52251361\n",
      "  0.         0.         0.         0.         0.52251361 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.2612568  0.2612568\n",
      "  0.         0.         0.        ]\n",
      " [0.         0.         0.35355339 0.         0.         0.\n",
      "  0.35355339 0.35355339 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.35355339\n",
      "  0.         0.35355339 0.         0.         0.         0.\n",
      "  0.         0.35355339 0.         0.         0.         0.\n",
      "  0.35355339 0.         0.35355339]\n",
      " [0.         0.3399922  0.         0.         0.         0.27430356\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.3399922  0.         0.3399922  0.         0.         0.\n",
      "  0.3399922  0.3399922  0.3399922  0.         0.         0.\n",
      "  0.         0.         0.3399922  0.         0.         0.3399922\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.        ]\n",
      " [0.20851441 0.         0.         0.41702883 0.         0.\n",
      "  0.         0.         0.20851441 0.         0.20851441 0.41702883\n",
      "  0.         0.41702883 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.20851441 0.         0.\n",
      "  0.20851441 0.         0.         0.20851441 0.20851441 0.\n",
      "  0.20851441 0.         0.20851441 0.20851441 0.         0.\n",
      "  0.         0.20851441 0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "#TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "'''\n",
    "\n",
    "text2=[\n",
    "    \"My name is Wang\",\n",
    "    \"I come to China to travel\",\n",
    "    \"This is a car polupar in China\",\n",
    "    \"I love tea and Apple \",\n",
    "    \"The work is to write some papers in science\"]\n",
    "'''\n",
    "#实例化 TfidfVectorizer 不用算词频，直接可以算出TF-IDF\n",
    "tfidf2 = TfidfVectorizer()\n",
    "rs = tfidf2.fit_transform(wordsList)\n",
    "print(rs)\n",
    "#打印看数值是一样的\n",
    "print(rs.toarray())#变数组\n",
    "#python公式计算TF-IDF参考：https://www.jianshu.com/p/f3b92124cd2b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "3. 互信息的原理。\n",
    "两个事件集合之间的相关性\n",
    "由于事件A发生与事件B发生相关联而提供的信息量。\n",
    "在处理分类问题提取特征的时候就可以用互信息来衡量某个特征和特定类别的相关性，如果信息量越大，那么特征和这个类别的相关性越大。\n",
    "公式：I(A,B) = log_2 P(B|A)/P(B)\n",
    "A，B的互信息 = log以2为底的 A发生情况下的B的概率除B的概率\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.         0.         0.         0.2612568  0.21078034\n",
      "  0.         0.         0.         0.2612568  0.         0.\n",
      "  0.         0.         0.         0.2612568  0.2612568  0.52251361\n",
      "  0.         0.         0.         0.         0.52251361 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.2612568  0.2612568\n",
      "  0.         0.         0.        ]\n",
      " [0.         0.         0.35355339 0.         0.         0.\n",
      "  0.35355339 0.35355339 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.35355339\n",
      "  0.         0.35355339 0.         0.         0.         0.\n",
      "  0.         0.35355339 0.         0.         0.         0.\n",
      "  0.35355339 0.         0.35355339]\n",
      " [0.         0.3399922  0.         0.         0.         0.27430356\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.3399922  0.         0.3399922  0.         0.         0.\n",
      "  0.3399922  0.3399922  0.3399922  0.         0.         0.\n",
      "  0.         0.         0.3399922  0.         0.         0.3399922\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.        ]\n",
      " [0.20851441 0.         0.         0.41702883 0.         0.\n",
      "  0.         0.         0.20851441 0.         0.20851441 0.41702883\n",
      "  0.         0.41702883 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.20851441 0.         0.\n",
      "  0.20851441 0.         0.         0.20851441 0.20851441 0.\n",
      "  0.20851441 0.         0.20851441 0.20851441 0.         0.\n",
      "  0.         0.20851441 0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.        ]]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>38</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.261257</td>\n",
       "      <td>0.210780</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.261257</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.261257</td>\n",
       "      <td>0.261257</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.353553</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.353553</td>\n",
       "      <td>0.353553</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.353553</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.353553</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.353553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.339992</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.274304</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.339992</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.208514</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.417029</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.208514</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.208514</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.208514</td>\n",
       "      <td>0.208514</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.208514</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 39 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0         1         2         3         4         5         6   \\\n",
       "0  0.000000  0.000000  0.000000  0.000000  0.261257  0.210780  0.000000   \n",
       "1  0.000000  0.000000  0.353553  0.000000  0.000000  0.000000  0.353553   \n",
       "2  0.000000  0.339992  0.000000  0.000000  0.000000  0.274304  0.000000   \n",
       "3  0.208514  0.000000  0.000000  0.417029  0.000000  0.000000  0.000000   \n",
       "4  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "\n",
       "         7         8         9   ...        29        30        31        32  \\\n",
       "0  0.000000  0.000000  0.261257  ...  0.000000  0.000000  0.000000  0.000000   \n",
       "1  0.353553  0.000000  0.000000  ...  0.000000  0.000000  0.353553  0.000000   \n",
       "2  0.000000  0.000000  0.000000  ...  0.339992  0.000000  0.000000  0.000000   \n",
       "3  0.000000  0.208514  0.000000  ...  0.000000  0.208514  0.000000  0.208514   \n",
       "4  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
       "\n",
       "         33        34        35        36        37        38  \n",
       "0  0.000000  0.261257  0.261257  0.000000  0.000000  0.000000  \n",
       "1  0.000000  0.000000  0.000000  0.353553  0.000000  0.353553  \n",
       "2  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "3  0.208514  0.000000  0.000000  0.000000  0.208514  0.000000  \n",
       "4  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "\n",
       "[5 rows x 39 columns]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#把上面词特征和分类合成DataFrame直观看下\n",
    "import pandas as pd\n",
    "\n",
    "print(tfidfit.toarray())\n",
    "pdDf = pd.DataFrame(tfidfit.toarray())\n",
    "pdDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    0.000000\n",
      "1    0.000000\n",
      "2    0.000000\n",
      "3    0.208514\n",
      "4    0.000000\n",
      "Name: 0, dtype: float64\n",
      "0.5004024235381879\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n#参考\\nfrom sklearn import datasets\\nfrom sklearn import metrics as mr\\n\\niris = datasets.load_iris()\\nx = iris.data\\nlabel = iris.target\\nx0 = x[:, 0]\\nx1 = x[:, 1]\\nx2 = x[:, 2]\\nx3 = x[:, 3]\\n#print(x)\\n#print(x0)\\nprint(label)\\nxDf = pd.DataFrame(x)\\nxDf.head()\\nyDf = pd.DataFrame(label)\\nyDf.head()\\ndf = pd.concat([xDf,yDf],axis = 1)\\ndf.head()\\n# 计算各特征与label的互信息\\n#print(mr.mutual_info_score(x0, label))\\n#print(mr.mutual_info_score(x1, label))\\n#print(mr.mutual_info_score(x2, label))\\n#print(mr.mutual_info_score(x3, label))\\n'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "4. 使用上面生成的特征矩阵，利用互信息进行特征筛选。\n",
    "'''\n",
    "'''\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn import metrics as mr\n",
    "#label=['汽车','电影','美食','计算机']\n",
    "label=[0,2,1,3,4]\n",
    "x0 = pdDf[:][0]\n",
    "print(x0)\n",
    "print(mr.mutual_info_score(x0.values, label))\n",
    "'''\n",
    "\n",
    "'''\n",
    "#参考\n",
    "from sklearn import datasets\n",
    "from sklearn import metrics as mr\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "x = iris.data\n",
    "label = iris.target\n",
    "x0 = x[:, 0]\n",
    "x1 = x[:, 1]\n",
    "x2 = x[:, 2]\n",
    "x3 = x[:, 3]\n",
    "#print(x)\n",
    "#print(x0)\n",
    "print(label)\n",
    "xDf = pd.DataFrame(x)\n",
    "xDf.head()\n",
    "yDf = pd.DataFrame(label)\n",
    "yDf.head()\n",
    "df = pd.concat([xDf,yDf],axis = 1)\n",
    "df.head()\n",
    "# 计算各特征与label的互信息\n",
    "#print(mr.mutual_info_score(x0, label))\n",
    "#print(mr.mutual_info_score(x1, label))\n",
    "#print(mr.mutual_info_score(x2, label))\n",
    "#print(mr.mutual_info_score(x3, label))\n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5.1 3.5 1.4 0.2]\n",
      " [4.9 3.  1.4 0.2]\n",
      " [4.7 3.2 1.3 0.2]\n",
      " [4.6 3.1 1.5 0.2]\n",
      " [5.  3.6 1.4 0.2]\n",
      " [5.4 3.9 1.7 0.4]\n",
      " [4.6 3.4 1.4 0.3]\n",
      " [5.  3.4 1.5 0.2]\n",
      " [4.4 2.9 1.4 0.2]\n",
      " [4.9 3.1 1.5 0.1]\n",
      " [5.4 3.7 1.5 0.2]\n",
      " [4.8 3.4 1.6 0.2]\n",
      " [4.8 3.  1.4 0.1]\n",
      " [4.3 3.  1.1 0.1]\n",
      " [5.8 4.  1.2 0.2]\n",
      " [5.7 4.4 1.5 0.4]\n",
      " [5.4 3.9 1.3 0.4]\n",
      " [5.1 3.5 1.4 0.3]\n",
      " [5.7 3.8 1.7 0.3]\n",
      " [5.1 3.8 1.5 0.3]\n",
      " [5.4 3.4 1.7 0.2]\n",
      " [5.1 3.7 1.5 0.4]\n",
      " [4.6 3.6 1.  0.2]\n",
      " [5.1 3.3 1.7 0.5]\n",
      " [4.8 3.4 1.9 0.2]\n",
      " [5.  3.  1.6 0.2]\n",
      " [5.  3.4 1.6 0.4]\n",
      " [5.2 3.5 1.5 0.2]\n",
      " [5.2 3.4 1.4 0.2]\n",
      " [4.7 3.2 1.6 0.2]\n",
      " [4.8 3.1 1.6 0.2]\n",
      " [5.4 3.4 1.5 0.4]\n",
      " [5.2 4.1 1.5 0.1]\n",
      " [5.5 4.2 1.4 0.2]\n",
      " [4.9 3.1 1.5 0.1]\n",
      " [5.  3.2 1.2 0.2]\n",
      " [5.5 3.5 1.3 0.2]\n",
      " [4.9 3.1 1.5 0.1]\n",
      " [4.4 3.  1.3 0.2]\n",
      " [5.1 3.4 1.5 0.2]\n",
      " [5.  3.5 1.3 0.3]\n",
      " [4.5 2.3 1.3 0.3]\n",
      " [4.4 3.2 1.3 0.2]\n",
      " [5.  3.5 1.6 0.6]\n",
      " [5.1 3.8 1.9 0.4]\n",
      " [4.8 3.  1.4 0.3]\n",
      " [5.1 3.8 1.6 0.2]\n",
      " [4.6 3.2 1.4 0.2]\n",
      " [5.3 3.7 1.5 0.2]\n",
      " [5.  3.3 1.4 0.2]\n",
      " [7.  3.2 4.7 1.4]\n",
      " [6.4 3.2 4.5 1.5]\n",
      " [6.9 3.1 4.9 1.5]\n",
      " [5.5 2.3 4.  1.3]\n",
      " [6.5 2.8 4.6 1.5]\n",
      " [5.7 2.8 4.5 1.3]\n",
      " [6.3 3.3 4.7 1.6]\n",
      " [4.9 2.4 3.3 1. ]\n",
      " [6.6 2.9 4.6 1.3]\n",
      " [5.2 2.7 3.9 1.4]\n",
      " [5.  2.  3.5 1. ]\n",
      " [5.9 3.  4.2 1.5]\n",
      " [6.  2.2 4.  1. ]\n",
      " [6.1 2.9 4.7 1.4]\n",
      " [5.6 2.9 3.6 1.3]\n",
      " [6.7 3.1 4.4 1.4]\n",
      " [5.6 3.  4.5 1.5]\n",
      " [5.8 2.7 4.1 1. ]\n",
      " [6.2 2.2 4.5 1.5]\n",
      " [5.6 2.5 3.9 1.1]\n",
      " [5.9 3.2 4.8 1.8]\n",
      " [6.1 2.8 4.  1.3]\n",
      " [6.3 2.5 4.9 1.5]\n",
      " [6.1 2.8 4.7 1.2]\n",
      " [6.4 2.9 4.3 1.3]\n",
      " [6.6 3.  4.4 1.4]\n",
      " [6.8 2.8 4.8 1.4]\n",
      " [6.7 3.  5.  1.7]\n",
      " [6.  2.9 4.5 1.5]\n",
      " [5.7 2.6 3.5 1. ]\n",
      " [5.5 2.4 3.8 1.1]\n",
      " [5.5 2.4 3.7 1. ]\n",
      " [5.8 2.7 3.9 1.2]\n",
      " [6.  2.7 5.1 1.6]\n",
      " [5.4 3.  4.5 1.5]\n",
      " [6.  3.4 4.5 1.6]\n",
      " [6.7 3.1 4.7 1.5]\n",
      " [6.3 2.3 4.4 1.3]\n",
      " [5.6 3.  4.1 1.3]\n",
      " [5.5 2.5 4.  1.3]\n",
      " [5.5 2.6 4.4 1.2]\n",
      " [6.1 3.  4.6 1.4]\n",
      " [5.8 2.6 4.  1.2]\n",
      " [5.  2.3 3.3 1. ]\n",
      " [5.6 2.7 4.2 1.3]\n",
      " [5.7 3.  4.2 1.2]\n",
      " [5.7 2.9 4.2 1.3]\n",
      " [6.2 2.9 4.3 1.3]\n",
      " [5.1 2.5 3.  1.1]\n",
      " [5.7 2.8 4.1 1.3]\n",
      " [6.3 3.3 6.  2.5]\n",
      " [5.8 2.7 5.1 1.9]\n",
      " [7.1 3.  5.9 2.1]\n",
      " [6.3 2.9 5.6 1.8]\n",
      " [6.5 3.  5.8 2.2]\n",
      " [7.6 3.  6.6 2.1]\n",
      " [4.9 2.5 4.5 1.7]\n",
      " [7.3 2.9 6.3 1.8]\n",
      " [6.7 2.5 5.8 1.8]\n",
      " [7.2 3.6 6.1 2.5]\n",
      " [6.5 3.2 5.1 2. ]\n",
      " [6.4 2.7 5.3 1.9]\n",
      " [6.8 3.  5.5 2.1]\n",
      " [5.7 2.5 5.  2. ]\n",
      " [5.8 2.8 5.1 2.4]\n",
      " [6.4 3.2 5.3 2.3]\n",
      " [6.5 3.  5.5 1.8]\n",
      " [7.7 3.8 6.7 2.2]\n",
      " [7.7 2.6 6.9 2.3]\n",
      " [6.  2.2 5.  1.5]\n",
      " [6.9 3.2 5.7 2.3]\n",
      " [5.6 2.8 4.9 2. ]\n",
      " [7.7 2.8 6.7 2. ]\n",
      " [6.3 2.7 4.9 1.8]\n",
      " [6.7 3.3 5.7 2.1]\n",
      " [7.2 3.2 6.  1.8]\n",
      " [6.2 2.8 4.8 1.8]\n",
      " [6.1 3.  4.9 1.8]\n",
      " [6.4 2.8 5.6 2.1]\n",
      " [7.2 3.  5.8 1.6]\n",
      " [7.4 2.8 6.1 1.9]\n",
      " [7.9 3.8 6.4 2. ]\n",
      " [6.4 2.8 5.6 2.2]\n",
      " [6.3 2.8 5.1 1.5]\n",
      " [6.1 2.6 5.6 1.4]\n",
      " [7.7 3.  6.1 2.3]\n",
      " [6.3 3.4 5.6 2.4]\n",
      " [6.4 3.1 5.5 1.8]\n",
      " [6.  3.  4.8 1.8]\n",
      " [6.9 3.1 5.4 2.1]\n",
      " [6.7 3.1 5.6 2.4]\n",
      " [6.9 3.1 5.1 2.3]\n",
      " [5.8 2.7 5.1 1.9]\n",
      " [6.8 3.2 5.9 2.3]\n",
      " [6.7 3.3 5.7 2.5]\n",
      " [6.7 3.  5.2 2.3]\n",
      " [6.3 2.5 5.  1.9]\n",
      " [6.5 3.  5.2 2. ]\n",
      " [6.2 3.4 5.4 2.3]\n",
      " [5.9 3.  5.1 1.8]]\n",
      "(150, 4)\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2]\n",
      "(150,)\n",
      "[[0.         0.         0.         0.         0.2612568  0.21078034\n",
      "  0.         0.         0.         0.2612568  0.         0.\n",
      "  0.         0.         0.         0.2612568  0.2612568  0.52251361\n",
      "  0.         0.         0.         0.         0.52251361 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.2612568  0.2612568\n",
      "  0.         0.         0.        ]\n",
      " [0.         0.         0.35355339 0.         0.         0.\n",
      "  0.35355339 0.35355339 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.35355339\n",
      "  0.         0.35355339 0.         0.         0.         0.\n",
      "  0.         0.35355339 0.         0.         0.         0.\n",
      "  0.35355339 0.         0.35355339]\n",
      " [0.         0.3399922  0.         0.         0.         0.27430356\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.3399922  0.         0.3399922  0.         0.         0.\n",
      "  0.3399922  0.3399922  0.3399922  0.         0.         0.\n",
      "  0.         0.         0.3399922  0.         0.         0.3399922\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.        ]\n",
      " [0.20851441 0.         0.         0.41702883 0.         0.\n",
      "  0.         0.         0.20851441 0.         0.20851441 0.41702883\n",
      "  0.         0.41702883 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.20851441 0.         0.\n",
      "  0.20851441 0.         0.         0.20851441 0.20851441 0.\n",
      "  0.20851441 0.         0.20851441 0.20851441 0.         0.\n",
      "  0.         0.20851441 0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.        ]]\n",
      "(5, 39)\n",
      "[0 1 2 3 4]\n",
      "(5,)\n",
      "0.5004024235381879\n",
      "0.5004024235381879\n",
      "0.5004024235381879\n",
      "0.5004024235381879\n",
      "0.5004024235381879\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "#参考\n",
    "from sklearn import datasets\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    " \n",
    "iris = datasets.load_iris()\n",
    "x = iris.data\n",
    "label = iris.target\n",
    " \n",
    "mutual_info = mutual_info_classif(x, label, discrete_features= False)\n",
    "print(mutual_info)\n",
    "\n",
    "'''\n",
    "from sklearn import datasets\n",
    "from sklearn import metrics as mr\n",
    "iris = datasets.load_iris()\n",
    "xx = iris.data\n",
    "print(xx)\n",
    "print(xx.shape)\n",
    "print(iris.target)\n",
    "print(iris.target.shape)\n",
    "\n",
    "#用互信息提了文本特征\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "import numpy as np\n",
    "#x = pdDf[:][:]#tfidfit和rs相同，都用了一套数据，两种方法\n",
    "#print(x)\n",
    "x = tfidfit.toarray()\n",
    "print(x)\n",
    "print(x.shape)\n",
    "pdDf['label'] = [0,1,2,3,4]\n",
    "label = pdDf['label'].values#label值需是矩阵，np.matrix生成矩阵\n",
    "print(label)\n",
    "print(label.shape)\n",
    "#【遗留问题：代入自己数据时有误，待解决 】\n",
    "#mutual_info = mutual_info_classif(x, label, discrete_features= False)\n",
    "#print(mutual_info)\n",
    "x0 = x[:,0]\n",
    "x1 = x[:,1]\n",
    "x2 = x[:,2]\n",
    "x3 = x[:,3]\n",
    "x4 = x[:,4]\n",
    "print(mr.mutual_info_score(x0, label))\n",
    "print(mr.mutual_info_score(x1, label))\n",
    "print(mr.mutual_info_score(x2, label))\n",
    "print(mr.mutual_info_score(x3, label))\n",
    "print(mr.mutual_info_score(x4, label))\n",
    "#【遗留问题：\n",
    "#这里特征都分类得分都是一样，原因可能是没有经过拆分训练集，测试集，\n",
    "#这里还是没有搞懂怎么回事，回头在再下吧。\n",
    "#】"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
