{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "ERNIE：\n",
    "\n",
    "知识增强的语义表示模型 ERNIE（Enhanced Representation through kNowledge IntEgration）\n",
    "百度2019/3发布的NLP领域的预训练模型，在中文处理上全面超越BERT\n",
    "\n",
    "与BERT的比较：\n",
    "\n",
    "BERT\n",
    "是基于原始语言信息来学习，\n",
    "通过Transformer的多层自注意力self-attention机制 双向建模 建立句子中上下文各词之间的关系权重，\n",
    "来输出全新的更全局的句子表示，\n",
    "用来做语义理解、相似度、分类、翻译（用全局的表式衡量另一种语言）等任务\n",
    "\n",
    "它的这种机制和英文的特点是有关的，\n",
    "英文的搭配较中文的多，句子重结构，所以它是通过这种搭配的权重来预测与它相近的词。\n",
    "同样的，它可能没有必要再把固定搭配加以学习储存起来备用了。\n",
    "但中文是重语义轻结构的一种语言，约定俗成的语义理解较语法结构更重要，\n",
    "所以ERNIE针对中文，在这一点重点做了处理。\n",
    "\n",
    "ERNIE\n",
    "通过建模海量数据中的实体概念之类的先验知识，学习真实世界中的语义关系。\n",
    "即，它不是用原始语言信息来学习，\n",
    "而是直接对先验语义知识单元建模，增强模型语义表示能力。\n",
    "比如 “哈（）滨是黑龙江的省会“，\n",
    "BERT是通过局部共现（求内积算相似度，即经常出现，判断关联性）判断出中间字是“尔”\n",
    "但没有学到“哈尔滨”的语义，\n",
    "ERNIE是通过学习词、实体与实体的表达，使模型能构建模出”哈尔滨“与”黑龙江“的关系，存起来，\n",
    "学习到了真实语义，如同人学知识一样，下次就可以拿出来用了 。\n",
    "\n",
    "ERNIE是字特征输入的，更灵活通用可扩展组合语义；\n",
    "\n",
    "它的训练语料多源：百科类，新闻类，论坛对话类......\n",
    "\n",
    "对话建模采用了DLM（Dialogue Language Model）\n",
    "把问答对作为输入（因为通常相同的回复问题可能相似），\n",
    "建模Query-Response（问-答）对话结构 \n",
    "\n",
    "github地址\n",
    "https://github.com/PaddlePaddle/LARK/tree/develop/ERNIE\n",
    "百度的BERT\n",
    "https://github.com/PaddlePaddle/LARK/tree/develop/BERT\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
